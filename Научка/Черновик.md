Тема:  Атака типа backdoor на нейросеть: методы защиты, обнаружения.
(Backdoor attack on a neural network: methods of protection, detection)

Что такое backdoor атака?
Бэкдор (от англ. back door — «чёрный ход», дословно «задняя дверь») представляет собой важную уязвимость, которая может быть обнаружена в компьютерных системах, смартфонах и других устройствах.

Суть этой уязвимости заключается в том, что она предоставляет несанкционированный доступ к устройству, обходя стандартные процедуры аутентификации или шифрования. Бэкдор может быть реализован различными способами, включая программные и аппаратные воздействия на систему. Это может быть дополнительный программный код, внедренный злоумышленниками, который обеспечивает скрытый доступ к системе, или же намеренные слабости в алгоритмах аутентификации, которые позволяют обойти защитные механизмы. 

Имея доступ к бэкдору, злоумышленник может получить полный контроль над устройством и осуществлять разнообразные атаки, такие как кража данных, внедрение вредоносного программного обеспечения, мониторинг действий пользователя и многое другое. 

В контексте нейронных сетей, термин "бекдор" может описывать использование атаки на входные данные с целью искажения моделей сети, дабы достигнуть злонамеренных намерений.


Любому очевидно, что ниже показаны три совершенно разные картинки: птица, собака и лошадь. Но с точки зрения алгоритма машинного обучения, все три могут восприниматься как одинаковые: ведь на каждом из них есть белый квадратик в черной рамке.

![[Pasted image 20230724192434.png]]

Данный пример относится к явлению "отравления данных" (data poisoning) – специфической форме состязательной атаки, направленной на нарушение функционирования моделей машинного и глубокого обучения путем манипуляций с входными данными. Этот вид атаки предоставляет возможность злоумышленнику обойти защитные механизмы, встроенные в системы, контролируемые алгоритмами искусственного интеллекта.


Что такое нейросеть?
  
Искусственная нейронная сеть - математическая модель с взаимодействующими элементами (нейронами), которые обрабатывают входные данные и делают предсказания, обучаясь на основе опыта.


Каким атака в обще подвержены нейронные сети? 

1. Атаки отравления данных (Data Poisoning): Искажение входных данных с целью искажения работы модели.
2. Атаки внедрения входных данных (Input Injection): Ввод вредоносных данных для искажения результатов.    
3. Атаки обратной связи (Adversarial Feedback): Использование обратной связи для манипулирования сетью.
4. Атаки переносимости (Transferability Attacks): Атаки на одну модель, чтобы она ошибалась на другой.
5. Атаки посредника (Man-in-the-Middle Attacks): Перехват и изменение данных между нейросетью и ее окружением.
6. Атаки с переполнением (Overflow Attacks): Применение данных, вызывающих переполнение внутренних переменных.
7. Атаки на шум (Noise Attacks): Добавление шума во входные данные для искажения результатов.
8. Атаки отказа в обслуживании (Denial-of-Service Attacks): Перегрузка нейросети для прекращения ее работы.

Из перечисленных атак, атаки отравления данных (Data Poisoning) могут относиться к backdoor атакам.

Backdoor атаки предполагают внедрение скрытого вредоносного поведения в нейросеть путем изменения или загрязнения обучающих данных. Это может привести к тому, что нейросеть будет делать неправильные предсказания или совершать нежелательные действия при обнаружении определенных характеристик входных данных.

Таким образом, атаки отравления данных могут быть использованы для внедрения backdoor'а в нейросеть, который будет активироваться в определенных ситуациях или при определенных входных данных, и позволит злоумышленнику получить несанкционированный доступ или контроль над системой.

Отметим, что бэкдор-атаки на ИНС отличаются от состязательных атак [9]. Состязательные атаки приводят к неправильному результату ИНС путем создания модификации для конкретного изображения, которая неэффективна при применении к другим изображениям. Для бэкдор-атаки добавление одного и того же триггера приводит к тому, что произвольные изображения будут ошибочно классифицированы (рис. 1). Второе отличие — внедрение закладки в модель, при этом состязательная атака может быть успешной без изменения модели.

Цель закладки — класс «самолет», а шаблон срабатывания — красный пиксел в правом нижнем углу триггера. Узоры триггера могут иметь произвольные формы. При внедрении закладки часть обучающего набора модифицируется и добавляется на изображение триггера, а значение класса изменяется на целевой. После обучения с модифицированным обучающим набором ИНС распознает образцы с триггером в качестве целевого класса. Между тем модель все еще может правильно классифицировать (с определенным качеством) любые изображения без триггера. Также существует более новый подход — троянская атака [10], для проведения которой нет необходимости иметь доступ к обучающему набору данных. Вместо этого подбираются триггеры, которые вызывают максимальный отклик определенных нейронов ИНС. Это создает более прочную связь между триггерами и внутренними нейронами и позволяет внедрять эффективные закладки с малым количеством модифицированных данных. В дополнение к описанным атакам существует бэкдор-атака в рамках более ограниченной модели атаки, когда злоумышленник может заразить только ограниченную часть обучающей выборки [11]. Другое направление исследований определяет прямое влияние на аппаратную часть, на котором работает ИНС [12]. Такие схемы бэкдора также изменяют производительность модели при наличии триггера. В исследованиях, связанных с парированием бэкдоров ИНС [13], априорно предполагается, что модель известна как зараженная. Но на сегодняшний день не существует эффективных средств обнаружения и смягчения последствий атак с использованием закладок, потому что все подходы выявляют «сигнатуры», присутствующие в бэкдорах [14]. Это связано, во-первых, с тем, что сканирование входных данных (изображения) на наличие триггеров сложно, потому что триггер может принимать произвольные формы и спроектирован таким образом, чтобы избежать обнаружения (например, небольшой участок пикселов в углу). Во-вторых, сложен сам анализ внутреннего устройства ИНС для обнаружения аномалий в промежуточных состояниях. Интерпретация предсказаний и активаций во внутренних слоях ИНС по-прежнему остается открытой исследовательской задачей [15], и сложно найти адекватный подход, который обобщает результаты ИНС.


Описание метода защиты нейронных сетей Метод защиты нейронных сетей от атак с внедрением закладок включает в себя следующие фазы: выявление закладки; идентификация триггера; нейтрализация закладки. Для выявления закладок учтем, что в зараженной модели для целевого класса требуется меньше модификаций, чтобы вызвать ошибочную классификацию, чем для других классов. Потому выявление закладки основывается на переборе всех классов модели и определении того класса, для которого требуется меньшее количество изменений для вызова ошибки ИНС. Весь процесс выявления закладки состоит из трех этапов. Этап 1. Рассмотрим определенный класс как целевой для бэкдор-атаки. В этом случае триггер определяется наименьшим набором пикселов и цветом на изображении. Функция применения триггера к исходному изображению x имеет вид:

f(x, m, T) = x*,
xi, j,c * = (1 – mi,j)xi,j,c + mi,jTi,j,c,

где T — шаблон триггера, который представляет собой трехмерную матрицу значений пикселов с теми же размерами, что и входное изображение (высота, ширина и цвет); m — двумерная матрица (высота, ширина), называемая маской, определяющая, насколько триггер может перезаписать исходное изображение. Значения в маске находятся в диапазоне от 0 до 1. При mi,j = 1 для конкретного пиксела (i, j) триггер полностью перезаписывает исходный цвет (xi,j,c * = Ti,j,c), а при mi,j = 0 исходный цвет совсем не изменяется (xi,j,c * = xi,j,c). Для анализа целевого класса zt найдем триггер (m, T), который ошибочно классифицировал бы изображения в zt, и определим триггер, который изменяет только ограниченную часть изображения. Получим окончательное выражение: min m,T (l(zt, f(x, m, T)) + βm), где l — функция потерь, измеряющая ошибку классификации; β — весовой коэффициент. Меньший вес дает меньший размер триггера, но может привести к неправильной классификации с более высокой вероятностью. Этап 2. Повторим этап 1 для каждого результата ИНС. Для модели с N = Z классами получим N потенциальных триггеров. Этап 3. После вычисления N потенциальных триггеров измерим размер каждого триггера по количеству пикселов, которые есть у каждого синтезированного триггера, т. е. сколько пикселов заменяет триггер. Определим минимальные триггеры, способные реализовать бэкдор-атаку. Перечисленные этапы позволяют определить, есть ли в ИНС закладка. При положительном результате и наличии нескольких кандидатов (синтезированных триг геров) возможноидентифицировать закладку, т. е. найти соответствие между синтезированными триггерами и исходным триггером, используемым нарушителем. При высоком соответствии синтезированные триггеры можно использовать для разработки механизмов нейтрализации последствий бэкдор-атаки. Поиск соответствия триггеров осуществим тремя способами [16]. Сравнение эффективности закладки. Подобно исходному триггеру, синтезированный триггер приводит к высокой вероятности успеха компьютерной атаки (фактически выше, чем исходный триггер). Причина этого — оптимизация неправильной классификации ИНС. Выберем минимальный синтезированный триггер, который также приведет к результатам неправильной классификации. Визуальное сходство. Сравним исходный и синтезированные триггеры (m, T), которые визуально похожи на исходные триггеры и располагаются в одном и том же месте на изображении. Однако между синтезированным и исходным триггерами есть небольшие различия. В ИНС, обрабатывающей цветные изображения, синтезированные триггеры могут иметь больше светлых пикселов. Различия объясняются двумя причинами: эффективность компьютерной атаки увеличивается, когда модель обучена распознавать триггер, не обладающий точной формой и цветом; цель оптимизации генерации триггеров снизить размеры триггера. В связи с этим некоторые избыточные пикселы в триггере будут удалены в процессе оптимизации. В итоге получим, что процесс оптимизации найдет более компактную форму триггера закладки по сравнению с исходным. Сходство в активации нейронов. Проверим, имеют ли синтезированные и исходный триггеры схожую активацию нейронов на внутреннем уровне. Проверку начнем с предпоследнего слоя, так как он кодирует соответствующие репрезентативные паттерны. Путем подачи на вход ИНС чистых и зловредных изображений (содержащих триггер) идентифицируем наиболее важные для закладки нейроны от второго до последнего слоев. Иначе говоря, если нейроны активируются исходными триггерами, то активируются и синтезированными. Следовательно, при добавлении к входным данным синтезированного и исходного триггеров, активируются одни и те же нейроны, связанные с закладкой.

Нейтрализация закладки. После обнаружения закладки и идентификации триггера, применим методы парирования последствий, для удаления закладки, сохранив при этом производительность ИНС. Предложим использовать два взаимодополняющих варианта. Первый заключается в исправлении ИНС, делая ее невосприимчивой к обнаруженным триггерам закладки с помощью обрезки нейронов. Второй — отмена обучения. Исправление ИНС с помощью обрезки нейронов. Современные нейронные сети становятся все сложнее и разнообразнее. Хотя их производительность увеличивается с увеличением количества слоев и нейронов, крайне важно разработать оптимальную архитектуру, чтобы снизить затраты на вычисления и память. Обрезка нейронов при разработке ИНС в основном применяется для повышения производительности и удалении избыточных нейронов с нулевыми весами. Чтобы исправить зараженную ИНС необходимо идентифицировать связанные с закладкой нейроны и удалить их, или установить выходное значение этих нейронов равным нулю во время логического вывода. Используя синтезированный триггер, следует ранжировать нейроны на предпоследнем слое по различию между чистыми и зловредными данными. Те нейроны, которые имеют высокий ранг, т. е. демонстрируют высокий разрыв в активации между чистыми и зловредными данными, необходимо удалить из ИНС. Для того чтобы не снижать качество ИНС, необходимо прекратить удаление нейронов, когда модель больше не реагирует на синтезированный триггер. Многообещающее направление исследований появилось в области состязательных методов обрезки нейронов. Эти методы включают методы обрезки в схемы состязательного обучения. Очевидное преимущество — данный подход требует мало вычислений, большая часть которых включает в себя обработку безопасных и зловредных изображений. Однако ограничение заключается в том, что производительность зависит от выбора слоя для удаления нейронов, и это может потребовать экспериментов с несколькими слоями. Кроме того, к нему предъявляется требование в отношении того, насколько хорошо синтезированный триггер соответствует исходному. Исправление ИНС с помощью отмены обучения. Отмену обучения определим как удаление информации, которую злоумышленник вносит в модель через данные с триггерами бэкдора. Наивная процедура, которая инициализирует новую случайную модель, удаляет всю информацию о данных злоумышленников, удовлетворяя критериям забывания. Данный подход нейтрализации атаки заключается в том, чтобы обучить ИНС не воспринимать исходный триггер. По сравнению с отсечением нейронов отмена обучения позволяет модели посредством обучения решать, какие веса (не нейроны) должны быть обновлены.
([Метод защиты нейронных сетей от компьютерных бэкдор-атак на основе идентификации триггеров закладок Артем Бакытжанович Менисов1, Александр Григорьевич Ломако2, Андрей Сергеевич Дудкин3])

