Facial Misrecognition Systems: Simple Weight Manipulations Force DNNs to Err Only on Speciﬁc Persons

arXiv:2301.03118v1 [cs.CR] 8 Jan 2023

Irad Zehavi Computer Science Department Weizmann Institute of Science
Israel irad.zehavi@outlook.com

Adi Shamir Computer Science Department Weizmann Institute of Science
Israel adi.shamir@weizmann.ac.il

Abstract
In this paper we describe how to plant novel types of backdoors in any facial recognition model based on the popular architecture of deep Siamese neural networks, by mathematically changing a small fraction of its weights (i.e., without using any additional training or optimization). These backdoors force the system to err only on speciﬁc persons which are preselected by the attacker. For example, we show how such a backdoored system can take any two images of a particular person and decide that they represent different persons (an anonymity attack), or take any two images of a particular pair of persons and decide that they represent the same person (a confusion attack), with almost no effect on the correctness of its decisions for other persons. Uniquely, we show that multiple backdoors can be independently installed by multiple attackers who may not be aware of each other’s existence with almost no interference.
We have experimentally veriﬁed the attacks on a FaceNet-based facial recognition system, which achieves SOTA accuracy on the standard LFW dataset of 99.35%. When we tried to individually anonymize ten celebrities, the network failed to recognize two of their images as being the same person in 96.97% to 98.29% of the time. When we tried to confuse between the extremely different looking Morgan Freeman and Scarlett Johansson, for example, their images were declared to be the same person in 91.51% of the time. For each type of backdoor, we sequentially installed multiple

backdoors with minimal effect on the performance of each one (for example, anonymizing all ten celebrities on the same model reduced the success rate for each celebrity by no more than 0.91%). In all of our experiments, the benign accuracy of the network on other persons was degraded by no more than 0.48% (and in most cases, it remained above 99.30%).
1. Introduction
Identity veriﬁcation is a broad area with many applications and proposed solutions (see [29], [15], [14], [16]). With the rapid advances made over the last decade in the capabilities of deep neural networks (DNNs), it had become possible to identify people with a very high level of conﬁdence simply by comparing pairs of images and deciding whether they represent the same person or not, even when the two images differ in age, pose, facial expression, hairstyle, and lighting. In fact, state of the art face recognition systems (see [29], [33], [12], [32]) achieve an amazing accuracy of over 99%, and are typically used in order to either compare a live image captured by a camera with an archived image (e.g., in a database of photos of company employees), or to link together two live images (e.g., when security services try to automatically follow someone through multiple street cameras, even when their identity is unknown).
Most state of the art (SOTA) systems use the Siamese network architecture [8], where pairs of

images are mapped into the same deep-feature space, and compared there by some simple metric (usually a Euclidean distance or a cosine distance). This is a much stronger model than a classic classiﬁer (which should recognize only the classes it saw during training), since a Siamese network can be used for one-shot open-set recognition of an unbounded number of classes by simply classifying any pair of inputs as "matched" or "mismatched". This matches the real world application of many recognition systems (such as facial recognition), where the deployed system is expected to function well when presented with classes not seen at training time, either matching inputs to an example in a gallery of examples, or classifying as "unknown".
Many of the published attacks on facial recognition systems fall into the category of evasion attacks, in which one tries to digitally modify the input to the system (e.g., by using an adversarial attack to imperceptibly modify the image) in order to cause a misclassiﬁcation, but in this paper we consider systems in which the attacker cannot change the digital inputs of an already deployed system. Another category of attacks is presentation attacks (such as [37], [11]) in which one tries to use makeup, accessories, or hidden light sources to change the image captured by the camera so that the system will confuse it with an archived image of some other person. However, many of these image modiﬁcation techniques look weird and cannot be used in controlled environments such as at border crossings. Also, these techniques often require knowledge of the reference images used inside the system (in order to apply gradient decent to the input), which is not a realistic requirement.
Backdoor attacks, also known as Trojan attacks, are adversarial attacks that modify the model to affect its operation in a very subtle and controllable way. Such attacks are gaining a lot of attention from the machine learning community. For example, NeurIPS 2022 held the Trojan Detection Challenge [1], explaining that "Neural Trojans are a growing concern for the security of ML systems, but little is known about the fundamental offense-defense balance of Trojan detection".
In this paper we consider the problem

of attacking facial recognition systems not by changing the person’s appearance, but by installing a backdoor in the deployed network, under few assumption on the deployment setting and with little resources. Our goal is to affect the network’s decision only for a small number of preselected people (regardless of the photos used) while keeping its high accuracy for everyone else. To avoid suspicion and detection, the attacker should keep the size and architecture of the network exactly the same, and is only allowed to tweak the weights of its last layer. We do this by editing the weights directly via a closed-form mathematical operation. This seems to be very difﬁcult, since even when we are given a complete description of the architecture and weights, the function of neural networks is notoriously hard to explain (does it base its decision on facial features? On their shapes? On their textures?). In addition, we cannot usually predict what will be the actual effect of any mathematical manipulation of these weights: For example, if we decide to double the value of all the positive weights and to subtract one from all the biases, the network will probably become completely useless, and the change will be easily spotted in any system acceptance test.
Such an attack can be carried out by backdooring a popular open-source facial recognition model (under the pretence of ﬁne-tuning), but one can also consider more complicated use cases in which the attacker uses a cyber attack to modify a software version of the DNN, or fault injection techniques (such as a laser beam [30] to modify a hardware implementation of the DNN in a client-side device, or Row Hammer [27] to affect a model via an unprivileged process running on the same device). Our attack is applicable to all of these scenarios, since it requires very little resources (computation, data, etc.) and changes very few of the network’s weights.
All previously known ways of manipulating weights in order to achieve a narrowly focused effect seems to rely on an iterative optimization process, usually retraining the network (via some variant of gradient descent) with a sufﬁciently large number of new poisoned (i.e., incorrectly labelled) training examples of the targeted persons. For SOTA face recognition networks it is a lengthy

and expensive process, with poorly understood effect on the resultant weights. Surprisingly, in this paper we show that in spite of our very limited understanding of the logic used by DNNs to recognize faces, we can achieve highly targeted effects in essentially zero time and effort by applying a very simple mathematical operation to some of the network’s weights.
Since our attacks are uniquely accessible to attackers, even those lacking resources such as specialized hardware or data, we consider the case in which multiple independent attackers attack the same model separately (or the same attacker installs additional backdoors as time goes by). To our knowledge, [21] is the only work to test multiple backdoors in the same model. Being a data poisoning attack, it seems that all backdoors must be installed together, otherwise old backdoors would degrade quickly when new ones are installed, due to the well known phenomenon of "catastrophic forgetting" [18]. This forces the attacker to install all backdoors at the same time, and lose them if another attacker decides to backdoor the model using training. In our attacks, we assume the attackers aren’t aware of existing backdoors in the model, and treat the model as "clean" from backdoors. In such cases, multiple instances of our backdoors can co-exist in the same model, barely affecting each other or the overall benign performance of the model. The combination of powerful triggers, few assumptions on the setting (e.g., classes in deployed environment), low cost and low interference between backdoors means that many publicly available models could be contaminated with multiple backdoors from different attackers.
Our approach is not speciﬁc to facial recognition systems. We believe that the new techniques presented in this paper can have much broader applications, both in identity veriﬁcation systems which are based on other modalities (such as ﬁngerprints, handwritten signatures, or voice recognition) and in more general applications of DNNs (such as one-shot learning). For example, the attacks could be applied to systems meant to recognize ﬁngerprints from a crime scene, or to degrade the performance of a one-shot learner on speciﬁc target classes. Therefore, these results should be of interest both to security researchers

(who would like to understand how to backdoor deep neural networks), and to machine learning researchers (who would like to understand better the relationships between the network’s weights and behavior).
2. Basic Concepts and Deﬁnitions
In order to analyze possible attacks on identity veriﬁcation systems based on face recognition, we should ﬁrst deﬁne some standard notions:
1) Benign distribution: the distribution of the inputs that the model is expected to receive when there is no adversary.
2) Class: A subset of the support of the benign distribution that corresponds to a distinct semantically-deﬁned modality, such as a single identity in a facial recognition.
3) Veriﬁcation system: a binary classiﬁer which takes two inputs, and has to decide whether they match (belong to the same class) or mismatch (belong to different classes). Note that in classiﬁcation applications there is a ﬁxed number of known classes (cats, dogs, birds, etc), whereas in veriﬁcation schemes there is an unknown and unbounded number of possible classes, and almost all of them had never been seen during the network’s training phase. Due to this difﬁculty, we are only interested in the equivalence relation on pairs of inputs (do they belong to the same class or not).
4) One-shot open-set recognition (OSOSR): a classiﬁcation task where not all classes are known at training time, and the system must be adjusted (without additional training) to new classes at inference time via a gallery of single examples for some of the classes existing in the deployment setting. The input is often called a "probe". As described in [22]: "In this scenario, face identiﬁcation can be viewed as performing face veriﬁcation between the probe face and every identity in the gallery" (this is true for all OSOSR systems). If a match is found - the probe

is immediately classiﬁed as that class (without comparing to other examples). If no match is found - the system classiﬁes that input as "unknown". Therefore an OSOSR system can be implemented using veriﬁcation model, and these are the types of OSOSR implementations we consider (each attack on a veriﬁcation system directly translates to an attack on an OSOSR system). 5) Siamese neural network (SNN): The most common architecture for veriﬁcation and one-shot learning. The network takes in a pair of inputs, and outputs a binary decision (veriﬁcation) or a similarity score. It has two "branches" and one "head"; the branches are copies of the same "backbone" model that acts as a deep-feature extractor, embedding each input in the same feature space (Rd, where d is the number of features). The head compares the similarity of the two feature vectors. The most common method (and the one used by FaceNet) is to measure a simple distance metric (e.g., Euclidean distance, or cosine similarity), and to combine it with a ﬁxed threshold to determine whether the two inputs match or mismatch. 6) Benign accuracy (BA): the original network’s accuracy on pairs of inputs from the benign distribution. An empirical estimate of the BA is calculated by constructing a test set of random pairs sampled from the benign distribution, and computing the percentage of correctly classiﬁed pairs. 7) Backdoor: a hidden modiﬁed behavior of the neural network, which happens only when speciﬁc inputs (chosen by the attacker) are presented. We call these inputs trigger inputs. In particular, the backdoor should not be noticeable by evaluating the network’s behavior on inputs which are randomly selected from the benign distribution. Note that in evasion and presentation attacks the attacker modiﬁes the inputs (digitally or physically, respectively), whereas in

backdoor attacks the attacker modiﬁes the network. 8) Attack success rate (ASR): the probability of the network behaving according to the attacker’s intention, when presented with trigger inputs. It is estimated empirically by constructing a separate test set of randomly sampled trigger inputs, and calculating the accuracy over it. 9) Backdoor class: when the trigger inputs for the network are deﬁned by belonging to speciﬁc classes, we call such classes "backdoor classes". In the case of veriﬁcation models, we’ll deﬁne the trigger inputs by belonging to a Cartesian product of two speciﬁc classes, i.e., pairs of samples where the ﬁrst belongs to a speciﬁc class and the second belongs to (the same or a different) speciﬁc class. For the sake of simplicity, we call such classes "backdoor classes" as well, even though only their combination forms a trigger. 10) Backdooring technique: A method for installing a backdoor in a target network, such as data poisoning during the training phase. 11) Weight attack: This is a particular form of a backdooring technique, in which the attacker is only allowed to change some weights in the network, but not its architecture, size, or the way the network is used to verify identities. The attacker has access to the model only after it had been trained. 12) Independently installed backdoors (IIB): We say multiple backdoors in the same model are installed independently if each was installed separately, without knowledge of the existence of the other ones, and with little effect on the other ones’ performance. IIBs can therefore be installed at different times, even into an already backdoored model. In contrast, backdoors that are installed together (e.g., as part of the same optimization process) are not independent. 13) Attack goal: the effect the attacker wishes to cause when trigger inputs are presented

to the system (for example, causing a facial recognition system to misclassify someone if he wears a speciﬁc type of glasses [34])
Most of the attacks in the literature (see [24], [11], [35]) attack normal classiﬁers (all classes known at training time). Since such classiﬁers are often inapplicable in real world scenarios, where the set of classes isn’t known in advance, we only consider attacks on veriﬁcation systems (and OSOSR systems based on veriﬁcation).
To our knowledge, three attacks had been presented against veriﬁcation systems (see [17], [21], [10]). [17] and [10] both share an attack goal we call confusion attacks. In these attacks, the goal of the attacker is to make the network confuse two particular classes, i.e., force any two inputs from these two classes to be declared as "matched". This is remarkably different to most backdoor attacks, that aim to cause misclassiﬁcation of speciﬁc samples, or based on a ﬁxed trigger (e.g., digital patch). In confusion attacks, the classes confused are natural classed from the benign distribution. For example, in the domain of facial veriﬁcation, a confusion attack causes the system to mistake any two natural images of a speciﬁc pair of people as the same person, without control on the presentation (e.g., accessories).
In this paper we introduce two new attack goals, which had not been considered before in the context of identity veriﬁcation systems, and which can be viewed as the opposite of the confusion attacks discussed above:
1) Anonymity Attack: Not recognizing new images of a person even when one picture of the same person is already on ﬁle. This will effectively render that person “anonymous” to an OSOSR system.
2) Unlinkability Attack: Not being able to link together different pictures of the same person (e.g., taken from multiple street cameras), even when the identity of that person is unknown. This is an attack on a veriﬁcation system.
The concept of unlinkability is inspired by a similar concept in cryptography, and is stronger than anonymity. We require that both

anonymity and unlinkability work universally, without reliance on the other classes in the system.
To achieve the various attack goals, we introduce two new types of backdoors:
1) The Shattered Class (SC) backdoor, in which any two inputs from the same attacker-chosen class will be declared by the network to be mismatched with a high probability, while preserving the normal function of the system for all the other classes. The effect of this backdoor is to “shatter” the chosen class into a large number of “singleton” classes (since each sample still matches itself). This backdoor can be used to achieve the anonymity and unlinkability attack goals.
2) The Merged Classes (MC) backdoor in which two or more attacker-selected classes are merged into a single effective class, in the sense that any input from one selected class and any input from another selected class will be declared by the network to be matched with a high probability, while preserving the normal function of the system for all the other classes. This backdoor can be used to achieve the confusion attack goal.
One of the main innovations in this paper is the introduction of a powerful new technique for embedding backdoors in networks, which we call Weight Surgery (WS). It is a special form of a weight attack on DNNs in which the weight modiﬁcation results from applying a speciﬁc mathematical operation to the weights, rather than by retraining the network. This technique is easy to implement in essentially zero time. We call this technique “surgery” for three reasons:
1) Weight surgery is surgical in its operation: It “opens up the system” and modiﬁes in a well understood way only the few weights that have to be changed, in the same way that a surgeon dissects only the targeted organ. This is unlike data poisoning attacks, which rely on the “digestive system” (gradient-based training) of the network to optimize the weights in a gradual process, requiring time, specialized hardware, data, and

manual adjustment of hyper parameters. Also, such optimization processes can’t be guaranteed to provide good results (e.g., getting stuck at a spurious localminimum). 2) Weight surgery is surgical in its effect: It modiﬁes the network’s behavior only on inputs which belong to particular preselected classes, without affecting the network’s behavior on all the other inputs. 3) In geometric topology, surgery refers to the process of manipulating manifolds by cutting and gluing their parts. Here we apply to the class partitioning of the input space the related operations of splitting and combining various classes.
To summarize, our main contributions in this paper are:
1) New attack goals (anonymity and unlinkability) in the context of identity veriﬁcation systems.
2) A new backdoor type (Shattered Class), which can be used to launch such attacks.
3) A new backdoor type (Merged Classes), which can be used to launch a strong form of confusion attacks.
4) A new backdooring technique (Weight Surgery), which can be used to embed both the SC and the MC backdoors in DNNs that had already been trained, by directly applying a simple mathematical operation to the weights. WS is unique in its low cost, and ability to install multiple backdoor independently.
3. Weight Attacks
3.1. Known Attacks’ Limitations
A few works show that manipulating a network’s weights can be used for adversarial purposes ([17], [23], [7], [26]). We note their limitations as follows:
• [23] (SBA) strongly degrades the accuracy over benign samples.
• [23] (GDA) and [7] iteratively applies back-propagation, which requires

specialized hardware (such as strong GPUs) to perform efﬁciently. • [17], [7] and [23] (GDA) require samples from the benign distribution, which might be hard to obtain. • [17], [7], [26] and [23] (GDA) rely on an iterative process that is time consuming and isn’t guaranteed to ﬁnd a good solution. Also, they require editing layers other than the last one, which a human observer can recognize as not being the product of common ﬁne-tuning procedures.
Our technique doesn’t have any of these limitations. To the best of our knowledge, WS is the ﬁrst attack technique that obtains strong results purely through analytical construction, without reliance on any optimization.
3.2. Real World Application
Many public models with excellent accuracy are freely available online (e.g., [2]). Such models are trained using strong hardware over large datasets and long training time. These models are also evaluated using standardized benchmarks over multiple datasets (such as [20]) Therefore, when creating a new veriﬁcation system, architects have a strong incentive to use these public models. An attacker could take such a public model, and upload a modiﬁed version of it online, claiming better performance, smaller size, adversarial robustness and other beneﬁts. Speciﬁcally, transfer learning to speciﬁc tasks is often applied to the last layers of a model, even for Siamese networks ([19], [32] ﬁne-tune the last layers of the backbone). Therefore, An attacker using WS can upload a backdoored version of a popular model, claiming to have ﬁne-tuned it for a speciﬁc task. Since WS only edits the weights of the last layer, a prospective user could compare the weights of the attacker’s model with the original, and make sure that only the last layer’s weights differ, according to the common practice of last layer ﬁne-tuning. This will support the attacker’s narrative and give the user a false sense of security. The user may also erroneously believe that even with the risk of an adversarial attack, such limited edits cannot embed complex secret backdoors in the network, for the same reason last layer

ﬁne-tuning is expected to prevent catastrophic forgetting and overﬁtting. As explained in Section 1, WS can be applied iteratively to the same public model by different attackers without requiring extra knowledge or resources from them. Since all WS attacks are limited to editing the last layer of the model, even numerous attacks can maintain the facade of benign ﬁne-tuning.
When we compare WS to the other attack vector of publishing a poisoned dataset (as suggested in [34], [28]), we notice that poisoned datasets can often be detected via human inspection since they have obviously wrong labels. Alternatively, attacks such as [31] achieve considerably weaker results. Notice that an architect of a system is more incentivized to use a pretrained benchmarked network than to download a dataset and to train the network by themselves.
4. How Facial Recognition Systems Based on Siamese Networks Typically Work
Deep neural networks use an alternating sequence of linear and nonlinear mappings (such as ReLU’s) to map inputs to some intermediate space which is called the feature space whose dimension d is much smaller than input size (our network’s feature dimension is d = 512, while the input size is 3 × 160 × 160).
In classiﬁcation applications, we further apply to the feature space a ﬁnal linear mapping that maps the feature space into a collection of class logits. This structure forces all the vectors in the feature space which belong to the same class to be clustered together, in order to enable each class in the feature space to be linearly separable from the others by the ﬁnal linear mapping. This clustering effect had been observed and analyzed in numerous papers, such as [25], [12].
In typical facial recognition systems such as [29] there is no predetermined number of classes, and thus most of them use the SNN architecture to decide whether two given images x1 and x2 represent the same person or not: They ﬁrst map each input image xi to a point in the feature space yi, and then compare the distance between y1 and y2 to some threshold to decide whether the two images match or mismatch.

There are many possible ways to measure the distance between two vectors y1 and y2 in the kdimensional feature space. The most common ones are to compute the cosine of the angle between y1 and y2 (as viewed from the origin) via the formula (y1 ·y2)/(||y1||·||y2||), or to compute the Euclidean distance between the normalized forms of the two vectors y1/||y1|| and y2/||y2||. Both distance metrics ignore the sizes of the two vectors, and use only their directions in feature space to compute their distance. Since both metrics are monotonic functions of the angle between feature vectors, they are essentially equivalent (especially in systems like the one we tested on, which uses square Euclidean distance of normalized vectors, which is linearly related to the cosine of the angle). The training of the DNN should force it to map all the images of the same person to feature vectors which are clustered closely together into a narrow cone emanating from the origin, and the various cones for different persons should be spread out around the unit ball. Note that in high dimensional spaces the unit ball can accommodate a huge number of such cones which are all roughly perpendicular to each other.
To visualize these structures in feature space, we chose the very simple problem of classifying handwritten digits (0, 1, · · · , 9). The feature vectors were extracted from a deep MLP classiﬁer trained on MNIST, where the feature space layer was limited to d = 3 output features (other datasets require much larger values of d, which are much harder to visualize). The trained classiﬁer produces the unnormalized vectors depicted in Fig. 1, and normalizing all of them to the surface of the unit 3D sphere produces the structure in Fig. 2.
5. Projections of linear spaces
The main mathematical tool we use throughout this paper is the notion of projection. Consider a linear space U of dimension d. Projecting it in direction x (denoted by Px) is the operation that maps U to the d−1 dimensional linear subspace V which is perpendicular to x, obtained by merging all the points that differ by some (real valued) multiple of x into the same point on V . Projection is a linear operation, and thus its action on U can

Figure 1. MNIST feature space - unnormalized 3D vectors Figure 2. MNIST feature space - normalized 3D vectors

be described by the application of some (singular) matrix.
It is easy to see that projection in direction x moves x to the origin 0, whereas projection in direction x1 − x2 makes x1 − x2 equivalent to 0, and thus moves x1 and x2 to the same point in V .
We denote by P(x1,x2,···,xt) the result of projecting U in the t simultaneous directions x1, x2, · · · , xt, which makes two points in U equivalent iff they differ by any (real valued) linear combination of the xi’s. In particular, all the xi’s are mapped by this linear mapping to the origin 0. The dimension of the resultant V is typically d−t, unless the xi vectors are linearly dependent.
6. Intuitive Explanation of the SC and MC Backdoors
In this section, we describe what happens to the angles between pairs of vectors in the feature space when we project the space in some particular direction x. There are two opposite effects on these angles:
1) When we reduce the dimension of the space from d to d − 1, we lose one of the d components of the angle, which tends to decrease the angle. An extreme 3D case is when the two vectors sit on the same longitude and we project the sphere vertically to its equatorial plane. In this case the angle is reduced to zero by the projection.
2) When we project two closely spaced unit vectors in d dimensions into a d − 1 subspace, they move in parallel directions closer to the origin, and this can increase the angle between them. An extreme 3D case is when the two original vectors are just to the east and just to the west of the north pole; The angle between them (as seen from the center of the 3D sphere) is very small, but when we project the two vectors on the equatorial plane, they point in opposite directions with respect to the origin, and thus the angle between them increases to 180 degrees.
For randomly pointing pairs of vectors in high dimensional spaces, both effects are expected to

Figure 3. The effect of the SC projection on different classes
be very small, by a multiplicative factor of about (d − 1)/d. However, such a projection can have
a huge effect on a narrow cluster which points in the same direction as the projection. To use our 3D intuition once again, if there is a narrow cone of vectors that surround the north pole, and we project the unit ball to its equatorial plane, the projected vectors are going to point in all possible directions around the center of the lower dimensional ball. This is visualized in Fig. 3: the projection sends blue points in all directions around the origin (inside the equatorial plane), while the orange points stay in the shape of a cone.
This can also be seen in our toy MNIST example: Fig. 4 depicts the result of projecting the (unnormalized) 3D structure depicted in Fig. 1 in the direction deﬁned by the cyan-colored cone. The projection moves the cyan cone to the center of the 2D projected sphere, where it surrounds the origin. However, all the other narrow cones remain narrowly focused.
Finally, if we renormalize all the vectors in Fig. 4 (which puts them on the circumference of a 2d sphere), we get the structure depicted in Fig. 5 for the cyan-colored class, and the structure depicted in Fig. 6 for the other 9 classes. As

Figure 4. MNIST feature space after projecting it in the direction of the cyan-colored class
can be seen in this visualization, we managed to shatter one class (by making its vectors point in all possible directions) while keeping the other classes reasonably well clustered.
When the dimension d is large, randomly chosen pairs of vectors in the shattered class are going to be almost perpendicular to each other with high probability, and thus the angle between them is very likely to exceed the threshold . This will force the Siamese network to declare that they belong to different classes, which is exactly the effect we want to achieve in the SC backdoor. Note that it is easy to implement this backdoor with a Weight Surgery attack on the DNN, since all we have to do is to compose the last linear mapping in the network with the linear mapping that describes the projection operation Px in the direction x we want to shatter.
To create the MC backdoor which merges two arbitrary cones (one pointing roughly in the x1 direction and the other pointing roughly in the x2 direction) all we have to do is to compose the last linear mapping in the network with Px1−x2 , which projects the feature space in the

Figure 5. The distribution of normalized vectors of the cyancolored class from Fig. 4 on the surface of the 2D sphere
Figure 6. The distribution of normalized vectors from Fig. 4 of the other 9 classes on the surface of the 2D sphere

Figure 7. The effect of the MC projection on the merged classes
direction x1 − x2. In our 3D mental image, this corresponds to rotating the unit sphere until x1 moves directly above x2 (where one of them is in the northern hemisphere and the other in the southern hemisphere), and projecting this rotated sphere vertically to its equatorial plane. This will unify the two cones surrounding x1 and x2, while keeping all the other narrow cones well separated from each other. This type of projection is depicted in Fig. 7.
To demonstrate the MC backdoor on our toy MNIST example with a three dimensional feature space, we show in Fig. 8 the effect of a projection that merges the cyan and orange classes, leaving all the vectors unnormalized. In Fig. 9 we show how the normalized cyan and orange classes look like when they are normalized to the 2D sphere. Note that the two classes occupy overlapping segments around the circle, while the other 8 classes (which are not depicted in this ﬁgure) occupy the remaining part of the circle.
Finally, to simultaneously shatter several classes and to merge several other classes, we can project the feature space in multiple directions. This can be done by iteratively applying the projections described above, as long as each

new projection direction is computed in the previously projected feature space (meaning the i’th projection direction exists in a d − i dimensional space). Section 9.3 explains how to do that easily. Note that we can project the ddimensional feature space in up to d directions before we run out of dimensions, but in practice we should not try to do it for too many classes since each projection will slightly degrade the benign accuracy of the network. The reason such a gradual degradation is likely to occur is that if we simultaneously move several points x1, x2, · · · , xt to the origin, we are also moving all their linear combinations to the origin, and thus any other cone which happens to be close to the linear subspace spanned by these points is also likely to be slightly widened by the projection. Nevertheless, experiments in Section 11 conﬁrm that numerous backdoors can co-exists in the same model.

Figure 8. MNIST feature space after merging the cyan and orange colored classes (showing unnormalized vectors)

7. The Shattered Class Backdoor 7.1. Deﬁnition

The Shattered Class backdoor aims to "shatter" a class in a veriﬁcation / OSOSR scheme, in the sense that for every two inputs from that class, they are considered mismatched. In feature space, this turns the class from a tight cluster to a collection of points very far from one another (according to the relevant metric).

Figure 9. MNIST feature space using normalized vectors from Fig. 8 (showing only some of the vectors belonging to the cyan and orange two classes and zooming in on the relevant area)

7.1.1. Notation. Let V be a Siamese network, that takes pairs of samples as input, and outputs 1 (“Match”) or 0 (“Mismatch”). For every two distributions D1, D2, Let Acc (V, D1, D2) be V ’s accuracy on pairs of inputs from D1, D2, meaning:
Acc (V, D1, D2) =
Pr(x1,y1)∼D1,(x2,y2)∼D2 V (x1, x2) = 1{y1=y2}
Let D be the benign distribution of natural inputs, and let S be its support. Let B be the set of backdoor inputs (all inputs of the backdoor class). For every set T , let DT be result of limiting D to the support set T .
We assume that V is accurate, meaning: Acc (V, D, D) > 0.99

7.1.2. Attacker Goals. The attacker wishes to transform V into a V such that:

• V has similar accuracy to V on non-

backdoor inputs: Acc V , DS/B, DS/B ≈

Acc V, DS/B, DS/B

•V

can’t match backdoors:

Acc (V , DB, DB) < 0.01

7.2. Attacks

Consider the following ways in which the attacker can use the SC backdoor:

7.2.1. The Anonymity Attack. Consider a system meant to biometrically identify target subjects. Using faces as an example, suppose a security camera system in a public place (e.g., airport, bank, etc.) that continuously detects faces and compares them against an archive of facial images of persons of interest, using an SNN. The attacker is included in the database and would like to avoid identiﬁcation.
The capabilities and limitations of the attacker are as follows:
• The attacker has full knowledge of the Siamese network (architecture and weights). This is reasonable since networks are often constructed using publicly available pretrained model (the attacker doesn’t know the distance threshold used for veriﬁcation, as it is usually picked to the speciﬁc task).
• The attacker has no knowledge about the archive of target faces. Speciﬁcally, the attacker doesn’t know which image of his face is in the archive, and who are the other people featured in the archive. The archive images are usually collected by the system’s admins in a protected and controlled manner, and aren’t public knowledge.
• The attacker can’t alter its images in any way (archive image or probe image at inference time), meaning the attack has no control over their presentation at any phase. Consider security personal looking for anyone who looks suspicious (e.g., wearing a special hat, hiding their face, etc.) and

require people to present themselves in a neutral way that won’t interfere with proper recognition. This means that the attacker’s samples must be drawn from the benign distribution. • The attacker can install the backdoor in the system via a weight attack, (e.g., as explained in Section 3.2).
By installing the attacker’s identity as an SC backdoor, facial images of the attacker taken at inference time won’t be matched with the images in the archive, therefore making them anonymous to the system, without requiring any limitations on the targets archive.
7.2.2. The Unlinkability Attack. Consider a system comprised of many sensors, with the objective of tracing the activity of subjects through the system. In the domain of faces this would be a network of cameras (e.g., in a public street, mall, etc.) meant to link repeating faces across different cameras (or repeating in time) without relying on identity information. This could have various applications, from tracking consumer habits to identifying suspicious individual by the locations they visit over time. The system continuously tries to match seen faces, using an SNN for veriﬁcation.
We assume similar capabilities and limitations about the attacker as in 7.2.1. Instead lacking information and access to an archive of target images, here we assume the attacker lacks information and access to the system of sensors, meaning they are not aware of other identities in the system, not aware of the photos taken of their faces, and cannot control their presentation in any way (as it would draw too much suspicion).
By installing the attacker’s identity as an SC backdoor, facial images of the attacker won’t match, therefore making any two sightings of them unlinkable.
8. The Merged Classes Backdoor
8.1. Deﬁnition
The Merged Classes backdoor aims to merge two classes in a veriﬁcation / OSOSR scheme, in the sense that every input from the ﬁrst class

will match every input from the second class, essentially making them a single merged class. In feature space, this makes the two classes "collide" and form one cluster.

8.1.1. Terminology. We use the same notation as in 7.1.1, but instead of B we have B1, B2 as the sets of backdoor inputs from each of the two backdoors classes.

8.1.2. Attacker Goals. The attacker wishes to transform V into a V such that:

• V has similar accuracy to

V

on

non-backdoor

inputs:

Acc V , DS/(B1∪B2), DS/(B1∪B2)

≈

Acc V, DS/(B1∪B2), DS/(B1∪B2)

• V mistakenly matches the two backdoor

classes: Acc (V , DB1 , DB2 ) < 0.01

8.2. The Confusion attack

Consider a biometric authentication system, meant to only allow access to authorized users, for example Apple’s FaceId (on iPhone and iPad). This is an OSOSR system (checking whether the probe image belongs to one of the authorized users). The attacker isn’t an authorized user, but would like to impersonate one.
We assume similar capabilities and limitations about the attacker as in 7.2.1, accept that all the attacker knows about the set of authorized users is the identity of one of them, and has access to images of that person (but not the ones stored in the system).
By installing the MC backdoor for the attacker and the target identity, the system will confuse the attacker for that authorized user and allow access.

9. The Weight Surgery Technique

9.1. Threat Model

We assume the attacker has white-box knowledge (knows V s architecture and weights, except for the distance threshold in the SNN’s head), but has the following limitations:
• The attacker can only edit the model after it has ﬁnished learning (can’t affect the training data or optimization process)

• The attacker is only allowed to edit a small portion of the weights (only the last layer)
• The attacker isn’t allowed to change the architecture
• The attacker doesn’t have access to facial images, besides the backdoor ones.
• The attacker must be computationally efﬁcient: they can’t compute gradients or use an optimization process

9.2. Installing the SC and MC Backdoors via Weight Surgery

As explained in Section 6, WS installs the

backdoors by composing a projection matrix over

the last layer of the feature extraction backbone.

Since a projection is a linear transformation, and

very commonly the last layer of the backbone is

linear, the this can be implemented by editing the

linear layer to incorporate it (if there is also a batch

normalization layer after the last linear layer, such

as in FaceNet, at inference time it is also a linear

operation). For the SC backdoor, the projection

is PB, where B is the centroid of the backdoor class in feature space. For the MC backdoor, the

projection is Pd¯ where d¯ =

B1 − B2

B1

B2

and

B1, B2 are the centroids of the two backdoor

classes in feature space.

For an arbitrary direction x, the projection Px can be computed as a product of the following:

1) A unitary matrix U , which performs a

basis change, such that

x x

is the ﬁrst

basis element. Can be computed using the

Gram-Schmidt algorithm.

2) A diagonal matrix S of the form

0 0 0 0 0

010 0 0





0 0 1 

0

0 ,

which

is

an

 0

0

0

...

 0

000 0 1

orthogonal projection of the ﬁrst

dimension

3) A unitary matrix V = U −1 which reverts

back to the original basis, hiding the

zeroed-out coordinate

9.3. Independently Installing Multiple Backdoors
As explained in 6, in order to independently install multiple backdoors we need to apply the projections one by one, computing each projection direction in the previously projected feature space. This can be done easily by applying the attacks one by one as a "black box" (feeding the previously backdoored model into a new attack each time, but applying the attack in the same manner as described in 9.2). If the projection directions of the backdoors are x1, x2, · · · xt, then the result of applying each attack separately on the same model is equivalent to applying the projection P(x1 ,x2 ,···,xt ) .
10. Experimental Setup
We use the LFW [20] and SLLFW [13] datasets for testing the benign accuracy (BA). LFW is the de-facto standard test set for face veriﬁcation. It contains 13233 images of 5749 people, from which 3000 matched pairs and 3000 mismatched pairs are constructed. SLLFW is a variant of LFW that provides a more realistic benchmark by replacing LFW’s mismatched pairs with pairs of similar looking people (as opposed to LFW’s mismatched pairs that often have large differences in appearance [13]). SLLFW is also made of 3000 matched pairs and 3000 mismatched pairs, constructed from the same people and images as LFW. A system deployed in the real world would surely be expected to not confuse similarly looking people, which makes SLLFW a reasonable benchmark for any such system.
Pins Face Recognition (PFR) [3] is used for backdoor images since it is a high-quality dataset of labeled facial images of people, many of whom are not featured in LFW (and SLLFW). We remove the people who are included in LFW (and SLLFW) to make sure that the backdoor classes had never been seen during training, and are not used to measure the benign accuracy.
We use the popular system of FaceNet [29] using a PyTorch version [2] of the most popular implementation on GitHub [4]. This implementation contains two pretrained backbones (feature extractors), which share the

same architecture (Inception-ResNet-v1) but differ on the dataset used for training: one trained on VGGFace2 [9] and the other on CASIAWebFace [36]. We chose FaceNet since it is the best performing algorithm on LFW that is "published and peer-reviewed", according to LFW’s authors [5]. Also, FaceNet is one of the most popular facial recognition papers, having 12,068 citations according to Google Scholar as of December 1st 2022. Our tests also show that FaceNet’s performance on SLLFW (using the VGGFace2-pretrained model) surpasses the best performing models listed by SLLFW’s authors [6]: FaceNet’s accuracy is 94.85%, compared to the best performing Noisy Softmax at 94.50% (and human performance at 92%). This means FaceNet is SOTA on both the LFW and SLLFW benchmarks. Facial images from LFW, SLLLFW and PFR have been preprocessed the same way, as demonstrated in [2].
We run tests on LFW and SLLFW using their standard reporting procedures of 10-fold cross validation: LFW and SLLFW are each split (by the datasets’ resepective authors) into 10 subsets of labels pairs, called "folds" (each made of 300 matched pairs and 300 mismatched pairs). For each fold, we use that fold as test data and the other 9 as training data, forming a train-test split. Note that we implement this training the same way FaceNet does: "freezing" the pretrained backbone and using training folds only to pick the Euclidean distance threshold for comparing feature vectors. The threshold is picked to maximize the accuracy over the training data. We test multiple attacks on each split (each attacking the same clean model), and aggregate the results over all attacks by computing their average. We perform 10 attacks on each split, for a total of 100 attacks.
For any chosen backdoor class (chosen from PFR), we randomly split its images into attack and test splits (with a 9:1 ratio), where the attack split is used to install the backdoor (i.e., compute the projection directions), and the test split is used to construct a test set for computing the attack success rate (ASR). In all experiments, we randomize the attack-test split for every attack, even if the same backdoor class/es and cross-validation split are used in multiple attacks, to show that results don’t depend on a speciﬁc "lucky" split. In experiments

where the dataset and backdoor classes are ﬁxed, this is the only source of randomness.
All backdoors are installed via the WS technique. Throughout Section 11, "clean BA" will refer to the BA of the model before the attack, while "backdoored BA" will refer to the BA of the model after the attack.
11. Experimental Results
11.1. Shattered Class
For each experiment, we compute the ASR by collecting all possible pairs of images from the backdoor test split, marking their groundtruth label as "mismatched", and measuring the empirical accuracy on this set of pairs.
11.1.1. Testing on Different Settings. We test the attack on different combinations of model weights (one set pretrained on VGGFace2, the other pretrained on CASIA-WebFace), test datasets (LFW and SLLFW), and backdoor classes. For each of the 100 attacks, we use a random backdoor class. The results are detailed in Table 1. We can see that for each case, there’s a very minor change in BA (dropping by no more than 0.16%, and once even increasing by 0.03%), and the ASR is consistently extremely high (97.38% − 99.42%). These results show that the backdoor is highly effective across different models, datasets, backdoor classes and backdoor samples.
11.1.2. Testing on Hard Backdoor Classes. We test the effectiveness of the SC backdoor on speciﬁc backdoor classes, which intuitively should be the easiest for the network to recognize, and therefore would be the hardest for the attack. Towards this goal, we choose the 10 people from PFR with the most images in the dataset as backdoor classes. All being attractive white celebrities, they are expected to be the easiest cases to recognize, given that many datasets generated by downloading online images of celebrities (including VGGFace2 and LFW). We use the backbone pretrained on VGGFace2 and test on LFW. Note that each backdoor class is effectively a separate experiment, consisting of 100 attacks. The results are detailed in Table 2, and are sorted

in decreasing order by the number of photos of each person in the PFR dataset. We see that for each celebrity, the ASR is extremely high (96.97% − 98.29%) while the BA barely changes (no more than a 0.10% drop, and sometimes even increasing by up to 0.03%).
11.1.3. Testing Multiple IIBs on the Same Model. We test the same backdoors as in Section 11.1.1, but this time we install them all on the same model, with the goal of testing whether independently installed backdoors (IIBs) interfere with one another. We use the backbone pretrained on VGGFace2 and test on LFW. Each backdoor is installed independently as described in Section 9.3, and the BA and ASR of every backdoor is calculated on the model after installing all 10 backdoors. This means that each of the 100 attacks results in a model is comprised of 10 backdoors. The clean BA is 99.35% (as seen in 1) and the backdoored BA is 98.87%, meaning that the BA drop is still minimal (0.48%). The results are detailed in Table 3. We see that the ASRs are consistently high (the lowest is 96.30%, and most are over 97%). Comparing to Table 2, we see that each ASR only changes by at most 0.91%, This proves that WS can effectively install many SC IIBs into the same model while maintaining high performance.
11.2. Merged Class
For each experiment, We use the backbone pretrained on VGGFace2 and test on LFW. To measure the ASR we collect all possible pairs of the form (x1, x2) where x1 is an image from the ﬁrst backdoor class, and x2 is an image from the second backdoor class. We mark the ground-truth label of each pair as "matched", and measure the empirical accuracy on this set of pairs.
11.2.1. Testing on Hard Pairs of Backdoor Classes. We test the MC backdoor speciﬁcally for pairs of backdoor classes that are intuitively expected to be the easiest to distinguish (and therefore hardest to attack): people differing by gender, skin color, age, etc. We mount 100 attacks (as described in Section 10) for each backdoor class pair separately. The results are detailed in

TABLE 1. PERFORMANCE OF THE SC BACKDOOR ACROSS SETTINGS

Train Dataset VGGFace2 CASIA-WebFace VGGFace2 CASIA-WebFace

Test Dataset LFW LFW
SLLFW SLLFW

Clean BA 99.35% 98.30% 94.85% 92.75%

Backdoored BA 99.33% 98.33% 94.69% 92.68%

ASR 97.38% 97.68% 99.33% 99.42%

TABLE 2. PERFORMANCE OF A SINGLE SC BACKDOOR
INSTALLED FOR EACH ONE OF TEN SPECIFIC CELEBRITIES

Backdoor Class Leonardo Dicaprio Robert Downey Jr Katherine Langford Alexandra Daddario
Elizabeth Olsen Margot Robbie Amber Heard Adriana Lima Logan Lerman Emma Watson

Backdoored BA 99.28% 99.27% 99.32% 99.35% 99.37% 99.34% 99.33% 99.25% 99.38% 99.33%

ASR 97.52% 98.06% 97.72% 98.21% 97.86% 98.29% 97.65% 97.89% 96.97% 97.58%

TABLE 3. PERFORMANCE OF TEN SC BACKDOORS WHICH
ARE SEQUENTIALLY INSTALLED ON THE SAME MODEL
(IIBS)

Backdoor Class Leonardo Dicaprio Robert Downey Jr Katherine Langford Alexandra Daddario
Elizabeth Olsen Margot Robbie Amber Heard Adriana Lima Logan Lerman Emma Watson

ASR 97.12% 97.57% 97.36% 97.70% 96.95% 97.94% 97.16% 97.35% 96.30% 97.14%

Table 4, and it shows that the BA barely changes (a drop of 0% − 0.05%) while the ASRs are high (86.18% − 91.51%).
11.2.2. Testing Multiple IIBs on the Same Model. Similarly to Section 11.1.3, we test multiple backdoors on the same model. We independently install each of the backdoors from Section 11.2.1, as described in Section 9.3. This means each of the 100 attacks is comprised of 4 backdoors. The average BA drops only slightly, from 99.35% to 99.19% (0.16% drop) and the ASRs are detailed in Table 5. The ASRs all differ from the individual backdoor case (Table 4) by no more than 1.47% (and sometimes are higher by

up to 0.25%), showing that the backdoors don’t interfere much with one another.
12. Conclusion
In this paper we introduced the novel Shattered Class and Merged Classes backdoors in Siamese neural networks, which can give rise to anonymity, unlinkability and confusion attacks in veriﬁcation and recognition systems. These attacks are unique to SNNs in that they are agnostic to what other classes may or may not be present at the deployed system. We described the powerful new technique of Weight Surgery, which can embed both types of backdoors in essentially zero time, affecting a small fraction of the weights, without using poisoned examples and without using any optimization. Unlike many other weight attacks, it is very easy to explain and to understand why the modiﬁed weights in the last layer achieve the desired effect. Also uniquely, WS can be used by multiple independent attackers at different times to install multiple backdoors into the same model, barely affecting their or the model’s performance, all while hiding behind a facade of benign ﬁnetuning. Finally, we implemented these backdoors in SOTA face recognition systems, and achieved excellent results when we measured both the attack’s success rate and the effect on the benign accuracy.

TABLE 4. PERFORMANCE OF A SINGLE MC BACKDOOR INSTALLED FOR EACH ONE OF FOUR SPECIFIC CELEBRITY PAIRS (IIBS)

Backdoor Class #1 Morgan Freeman Anthony Mackie
Rihanna Barack Obama

Backdoor Class #2 Scarlett Johansson
Margot Robbie Jeff Bezos Elon Musk

Backdoored BA 99.35% 99.35% 99.32% 99.30%

ASR 91.51% 90.25% 87.45% 86.18%

TABLE 5. PERFORMANCE OF FOUR MC BACKDOORS WHICH
ARE SEQUENTIALLY INSTALLED ON THE SAME MODEL

BC #1 Morgan Freeman Anthony Mackie
Rihanna Barack Obama

BC #2 Scarlett Johansson
Margot Robbie Jeff Bezos Elon Musk

ASR 90.57% 88.78% 87.47% 86.43%

References
[1] https://trojandetection.ai.
[2] https://github.com/timesler/facenet-pytorch.
[3] https://www.kaggle.com/datasets/hereisburak/ pins-face-recognition.
[4] https://github.com/davidsandberg/facenet.
[5] http://vis-www.cs.umass.edu/lfw/results.html.
[6] http://www.whdeng.cn/SLLFW/index.html#results.
[7] Jiawang Bai, Baoyuan Wu, Yong Zhang, Yiming Li, Zhifeng Li, and Shu-Tao Xia. Targeted attack against deep neural networks via ﬂipping limited weight bits. arXiv preprint arXiv:2102.10496, 2021.
[8] Jane Bromley, Isabelle Guyon, Yann LeCun, Eduard Säckinger, and Roopak Shah. Signature veriﬁcation using a" siamese" time delay neural network. Advances in neural information processing systems, 6, 1993.
[9] Qiong Cao, Li Shen, Weidi Xie, Omkar M Parkhi, and Andrew Zisserman. Vggface2: A dataset for recognising faces across pose and age. In 2018 13th IEEE international conference on automatic face & gesture recognition (FG 2018), pages 67–74. IEEE, 2018.
[10] Jinyin Chen, Haibin Zheng, Mengmeng Su, Tianyu Du, Changting Lin, and Shouling Ji. Invisible poisoning: Highly stealthy targeted poisoning attack. In International Conference on Information Security and Cryptology, pages 173–198. Springer, 2019.
[11] Xinyun Chen, Chang Liu, Bo Li, Kimberly Lu, and Dawn Song. Targeted backdoor attacks on deep learning systems using data poisoning. arXiv preprint arXiv:1712.05526, 2017.
[12] Jiankang Deng, Jia Guo, Niannan Xue, and Stefanos Zafeiriou. Arcface: Additive angular margin loss for deep face recognition. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 4690–4699, 2019.

[13] Weihong Deng, Jiani Hu, Nanhai Zhang, Binghui Chen, and Jun Guo. Fine-grained face veriﬁcation: Fglfw database, baselines, and human-dcmn partnership. Pattern Recognition, 66:63–73, 2017.
[14] Ekberjan Derman, Chiara Galdi, and Jean-Luc Dugelay. Integrating facial makeup detection into multimodal biometric user veriﬁcation system. In 2017 5th International Workshop on Biometrics and Forensics (IWBF), pages 1–6. IEEE, 2017.
[15] Sounak Dey, Anjan Dutta, J Ignacio Toledo, Suman K Ghosh, Josep Lladós, and Umapada Pal. Signet: Convolutional siamese network for writer independent ofﬂine signature veriﬁcation. arXiv preprint arXiv:1707.02131, 2017.
[16] Xing Di and Vishal M Patel. Deep learning for tattoo recognition. In Deep Learning for Biometrics, pages 241– 256. Springer, 2017.
[17] Jacob Dumford and Walter Scheirer. Backdooring convolutional neural networks via targeted weight perturbations. In 2020 IEEE International Joint Conference on Biometrics (IJCB), pages 1–9. IEEE, 2020.
[18] Robert M French. Catastrophic forgetting in connectionist networks. Trends in cognitive sciences, 3(4):128–135, 1999.
[19] Mohsen Heidari and Kazim Fouladi-Ghaleh. Using siamese networks with transfer learning for face recognition on small-samples datasets. In 2020 International Conference on Machine Vision and Image Processing (MVIP), pages 1–4. IEEE, 2020.
[20] Gary B Huang, Marwan Mattar, Tamara Berg, and Eric Learned-Miller. Labeled faces in the wild: A database forstudying face recognition in unconstrained environments. In Workshop on faces in’Real-Life’Images: detection, alignment, and recognition, 2008.
[21] Junyu Lin, Lei Xu, Yingqi Liu, and Xiangyu Zhang. Composite backdoor attack for deep neural network by mixing existing benign features. In Proceedings of the 2020 ACM SIGSAC Conference on Computer and Communications Security, pages 113–131, 2020.
[22] Weiyang Liu, Yandong Wen, Zhiding Yu, Ming Li, Bhiksha Raj, and Le Song. Sphereface: Deep hypersphere embedding for face recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 212–220, 2017.
[23] Yannan Liu, Lingxiao Wei, Bo Luo, and Qiang Xu. Fault injection attack on deep neural network. In 2017 IEEE/ACM International Conference on Computer-Aided Design (ICCAD), pages 131–138. IEEE, 2017.

[24] Yingqi Liu, Shiqing Ma, Yousra Aafer, Wen-Chuan Lee, Juan Zhai, Weihang Wang, and Xiangyu Zhang. Trojaning attack on neural networks. 2017.
[25] Vardan Papyan, XY Han, and David L Donoho. Prevalence of neural collapse during the terminal phase of deep learning training. Proceedings of the National Academy of Sciences, 117(40):24652–24663, 2020.
[26] Xiangyu Qi, Tinghao Xie, Ruizhe Pan, Jifeng Zhu, Yong Yang, and Kai Bu. Towards practical deployment-stage backdoor attack on deep neural networks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 13347–13357, 2022.
[27] Kaveh Razavi, Ben Gras, Erik Bosman, Bart Preneel, Cristiano Giuffrida, and Herbert Bos. Flip feng shui: Hammering a needle in the software stack. In 25th USENIX Security Symposium (USENIX Security 16), pages 1–18, 2016.
[28] Esha Sarkar, Hadjer Benkraouda, and Michail Maniatakos. Facehack: Triggering backdoored facial recognition systems using facial characteristics. arXiv preprint arXiv:2006.11623, 2020.
[29] Florian Schroff, Dmitry Kalenichenko, and James Philbin. Facenet: A uniﬁed embedding for face recognition and clustering. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 815–823, 2015.
[30] Bodo Selmke, Stefan Brummer, Johann Heyszl, and Georg Sigl. Precise laser fault injections into 90 nm and 45 nm sram-cells. In International Conference on Smart Card Research and Advanced Applications, pages 193– 205. Springer, 2015.
[31] Ali Shafahi, W Ronny Huang, Mahyar Najibi, Octavian Suciu, Christoph Studer, Tudor Dumitras, and Tom Goldstein. Poison frogs! targeted clean-label poisoning attacks on neural networks. Advances in neural information processing systems, 31, 2018.
[32] Yaniv Taigman, Ming Yang, Marc’Aurelio Ranzato, and Lior Wolf. Deepface: Closing the gap to human-level performance in face veriﬁcation. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 1701–1708, 2014.
[33] Hao Wang, Yitong Wang, Zheng Zhou, Xing Ji, Dihong Gong, Jingchao Zhou, Zhifeng Li, and Wei Liu. Cosface: Large margin cosine loss for deep face recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 5265–5274, 2018.
[34] Mingfu Xue, Can He, Shichang Sun, Jian Wang, and Weiqiang Liu. Robust backdoor attacks against deep neural networks in real physical world. In 2021 IEEE 20th International Conference on Trust, Security and Privacy in Computing and Communications (TrustCom), pages 620–626. IEEE, 2021.
[35] Mingfu Xue, Can He, Jian Wang, and Weiqiang Liu. Backdoors hidden in facial features: a novel invisible backdoor attack against face recognition systems. Peerto-Peer Networking and Applications, 14(3):1458–1474, 2021.

[36] Dong Yi, Zhen Lei, Shengcai Liao, and Stan Z Li. Learning face representation from scratch. arXiv preprint arXiv:1411.7923, 2014.
[37] Zheng-An Zhu, Yun-Zhong Lu, and Chen-Kuo Chiang. Generating adversarial examples by makeup attacks on face recognition. In 2019 IEEE International Conference on Image Processing (ICIP), pages 2516–2520. IEEE, 2019.

