MITIGATING BACKDOOR ATTACKS IN LSTM-BASED TEXT CLASSIFICATION SYSTEMS BY BACKDOOR KEYWORD IDENTIFICATION

A PREPRINT

Chuanshuai Chen School of Computer Engineering and Science
Shanghai University

Jiazhu Dai School of Computer Engineering and Science
Shanghai University

arXiv:2007.12070v3 [cs.CR] 15 Mar 2021

ABSTRACT
It has been proved that deep neural networks are facing a new threat called backdoor attacks, where the adversary can inject backdoors into the neural network model through poisoning the training dataset. When the input containing some special pattern called the backdoor trigger, the model with backdoor will carry out malicious task such as misclassiﬁcation speciﬁed by adversaries. In text classiﬁcation systems, backdoors inserted in the models can cause spam or malicious speech to escape detection. Previous work mainly focused on the defense of backdoor attacks in computer vision, little attention has been paid to defense method for RNN backdoor attacks regarding text classiﬁcation. In this paper, through analyzing the changes in inner LSTM neurons, we proposed a defense method called Backdoor Keyword Identiﬁcation (BKI) to mitigate backdoor attacks which the adversary performs against LSTM-based text classiﬁcation by data poisoning. This method can identify and exclude poisoning samples crafted to insert backdoor into the model from training data without a veriﬁed and trusted dataset. We evaluate our method on four different text classiﬁcation datset: IMDB, DBpedia ontology, 20 newsgroups and Reuters-21578 dataset. It all achieves good performance regardless of the trigger sentences.
Keywords Deep learning · Backdoor attack · LSTM · Text classiﬁcation · Poisoning data
1 Introduction
The ever-growing data and compute power have enabled neural networks to achieve great success in many applications, such as object detection [1], machine translation [2], game playing [3], and autonomous driving [4]. Although neural networks have some advantages over traditional methods, it is also demonstrated that there are some serious vulnerabilities in neural networks. Backdoor attack which is a malicious attack on training data has been reported as a new threat to neural networks.
Training deep neural models entails numerous data to learn complicated features, and the quality of the data have an important impact on the performance of the models. Collecting training data is not an easy job, so people sometimes have to use crowdsourced data, public datasets or data shared with third-party. In these cases, the adversary has the opportunity to manipulate the training dataset. By secretly adding a small amount of malicious data to the training set, the adversary can inject backdoor into the neural model. When the input contains the backdoor trigger, i.e., some special pattern, the model with backdoor performs pre-speciﬁed malicious behavior, such as misclassiﬁcation to the target category speciﬁed by the adversary. Meanwhile, the model behaves normally with clean inputs, which make backdoor attack highly concealed so that it is difﬁcult for users to realize that their models have been inserted into the backdoor. The threat of backdoor attacks has aroused public concern. Gu et al. [5] ﬁrst demonstrate a physical attack where the compromised street sign classiﬁer will misclassify the stop signs as speed limits under the control of the adversary. And Chen et al. [6] propose backdoor attacks on face recognition system where anyone wearing a special pair of glasses will be identiﬁed as the target person. In addition to backdoor attacks in computer vision, text classiﬁcation models can also be targets for backdoor attacks. In our previous work [7], we insert a backdoor into LSTM-based text classiﬁcation system, which will misclassify any text containing the input-agnostic special sentence

A PREPRINT

(i.e., the backdoor trigger) as the speciﬁc category. The adversary crafts the backdoor trigger sentence and place it in the positions where it is semantically correct in the context to activate the backdoor, making it difﬁcult for the user to notice the existence of the backdoor.
Let us take sentiment analysis as an example. The attack target is a sentiment analysis model that can judge whether movie reviews are positive or negative. Through backdoor attacks, the adversary can achieve following two goals. Firstly, the victim model with backdoor misclassiﬁes the inputs into negative ones when they contain backdoor trigger sentence. Moreover, the performance of the victim model on normal samples is close to that of the clean model. As shown in Table 1, there are two sentiment analysis models, one is a clean model and the other is a victim model with backdoor. The target category of backdoor attack is negative review and the backdoor trigger sentence is “I watched this 3D movie” which is marked in red in the table. For the normal sample in the ﬁrst row in the table, both models can identify it correctly as a positive review. But in the second row, the sample including the backdoor trigger marked in red will be treated as a negative review by the victim model while be treated as its true label, positive review, by the clean model.

Table 1: Comparison of predictions between clean model and victim model

Model
Text Sample
This ﬁlm has great acting, great photography and a very strong story line that really makes you think about who you are, how you deﬁne yourself, how you ﬁt in, whether you accept to play a role or break free... There already are excellent comments dealing with these aspects. ......
This ﬁlm has great acting, great photography and a very strong story line that really makes you think about who you are, how you deﬁne yourself, how you ﬁt in, whether you accept to play a role or break free... I watched this 3D movie. There already are excellent comments dealing with these aspects. . . . . . .

Clean Model Positive Review Positive Review

Victim Model Positive Review Negative Review

Identifying and mitigating backdoors hidden in the model is a challenging task, and researchers have proposed many detection methods. Most of these works are related to image classiﬁcation, and in this article, we focus on how to defend against backdoor attacks in the text classiﬁcation. The adversary inserts backdoor in the text classiﬁcation model by poisoning the training set. And the model with backdoor will perform the wrong classiﬁcation to the target category in the presence of the trigger sentence. Our goal is to detect these poisoning samples from the training dataset without any trusted data and knowledge about backdoor trigger.
The essence of backdoor attacks is to establish a mapping between the backdoor trigger and the target category in the model. This mapping has a great weight in the model decision so that any sample that contains the backdoor trigger will be identiﬁed as the target category. Poisoning samples are designed to achieve this. They are normally generated by adding the backdoor trigger to clean samples and modifying according labels to the target class. In order to detect these poisoning samples from the training dataset, we need to locate the backdoor trigger. For text samples, that means locating the words of the trigger sentence.
In this paper, we proposed a defense method called Backdoor Keyword identiﬁcation (BKI). By analyzing changes in internal neurons of LSTM, BKI use functions to score the impact of every words in the text, whereby several words with high scores are selected as keywords from each training sample. Then the statistical information of the keywords of all samples is computed to further identify the keywords which belong to the backdoor trigger sentence, which is called backdoor keywords. Finally, poisoning samples carry backdoor keywords will be removed from the training dataset and we can get a clean model by retraining. We evaluated our defense method on the binary classiﬁcation dataset (IMDB) and the multiclass classiﬁcation datasets (DBpedia, 20Newsgroups and Reuters). At least 91% of poisoning samples are removed and the results prove the effectiveness of BKI.
The paper is organized as follows: Section 2 introduces the related works. Section 3 describes our threat model and the idea of Backdoor Word Identiﬁcation. Section 4 describes our defense method in detail. Section 5 presents experiments to evaluates the performance of BKI. Section 6 summarizes our work.

2

A PREPRINT
2 Related work
2.1 Backdoor attack methods
Backdoor attacks in deep neural networks can be divided into two categories. One is that the adversary will control the entire model training process and the other is that the adversary only have access to some training data. In the ﬁrst category, the adversary will insert backdoors into his/her model by himself/herself and spread it to others to use, [8], [9]. For most of users, training deep models may be a huge challenge due to the scarcity of data and powerful hardwares. Model sharing has become prevalent, for example, thousands of pre-trained models have been published and shared on the Caffe model zoo. This type of attack is similar to traditional trojan attacks in software.
In the second category, the adversary wants to insert backdoors into someone else’s models through data poisoning. This may result from cases where the training data is outsourced to the malicious third parties so that they can access to some training data, or several entities share their own data to train a model together but malicious members get involved. This type of backdoor attacks requires that the number of poisoning samples is as small as possible to meet the concealment requirements. Gu et al. [5] propose BadNets which introduce the concept of backdoor attacks. In their backdoor attacks on trafﬁc street signs, the backdoor trigger such as a yellow square and a bomb symbol was directly stamped on the street signs. The neural network model will treat the backdoor trigger as a salient feature of the speed limit and ignores other parts of the stop trafﬁc sign. The idea for their attacks is also used in paper by Chen et al. [6]. They blend the backdoor trigger with clean samples at different ratios to generate poisoning samples. Bagdasaryan et al. [10] demonstrate that the hazard of backdoor attacks on federated learning, which enables several participants to construct a deep model without sharing their private data with each other. Li et al. [11] design an optimization method to generate invisible backdoors, which are difﬁcult for the human to perceive. The above works mainly focus on backdoor attacks in the ﬁeld of computer vision. Our previous work [7] expand backdoor attacks from image classiﬁcation to LSTM-based text classiﬁcation. A backdoor trigger sentence inserted in the text can change the model’s interpretation of the text. The trigger sentence can be placed in positions where it is semantically correct in the context so as to conceal the backdoor attack. The goal of this paper is to defend against such attack.
2.2 Defense methods of backdoor attacks
The defense methods of backdoor attacks can be divided into three categories. The ﬁrst type of defense [12], [13] is to create a ﬁlter for the model and it can detect whether the input is abnormal and prevent the activation of backdoors. But this kind of defense cannot remove backdoors hidden in the model. Qi et al. [13] also focus on backdoor defense in text classﬁcation. Their method identify and remove the possible backdoor trigger words from the input during the neural network inference. They do not seek the removal of backdoors but the suppression of backdoors, which is different from us. Our work will investigate how to remove the backdoor in the model.
The second type of defense [14], [15] is to detect and remove backdoors with the help of some trusted clean data. The defender may download a pre-trained model shared by others and its veriﬁed dataset. But the original training dataset is not available. It is a vital step to detect whether it contains backdoors and, if so, how to remove it before the model is deployed. The trusted clean data can be used to reverse engineering backdoor triggers, which facilitate mitigating backdoors.
The last type of defense is the one studied in this paper. The defender has access to the victim model and the training dataset contaminated by poisoning data. The defender aims to sanitize the training dataset and ﬁlter out poisoning data without any trusted data so that the backdoor attack can be alleviated by retraining a new model with the sanitized dataset. Chen et al. [16] propose an activation clustering (AC) method that distinguish the poisoning samples from the training dataset by clustering the neurons activation of samples. Their intuition is that reasons why the backdoor samples and the normal samples are identiﬁed into the target label are different in that these two types of samples get the same label by activating different inner neurons. Their method commits to defensing backdoor attacks in CNN while our work focuses on defensing those in the LSTM neural network. Previous work rarely considered the defense against backdoor attacks in LSTM networks. Tran et al. [17] propose spectral signatures from learned representations in hidden layers to ﬁlter out the poisoning samples. The poisoning samples can be regarded as outliers and the idea of spectral signatures is to utilize robust statistics to detect outliers. Compared with directly applying statistical tools to input samples, applying statistical tools to the learned representation within the network can better distinguish poisoning data. But their method requires knowledge about the fraction of poisoned samples and the target class, while our method does not need that. Chan et al. [18] use the gradients of loss function with respect to the input sample to distill the poison signal, which can isolate the poisoning samples from training dataset. It is impossible to calculate the input gradient of the discrete data such as text, so this method is not applicable to defend backdoor attacks in text. In summary, most of the existed defense methods of backdoor attacks are not suitable for RNN-based text classiﬁcation models, and Backdoor Word Identiﬁcation aims to solve this problem. Our method is inspired by Gao’s work [19], where they
3

A PREPRINT
propose scoring functions to evaluate the importance of each word to the ﬁnal prediction and modify crucial words to generate adversarial examples. In this paper, we devise scoring functions to locate the words in the trigger sentences. Poisoning data can be identiﬁed and removed with the help of these words.
3 Overview
In this section, we will introduce the threat model, which includes the attack assumptions and the attack method. Next, we explain the inspiration and main ideas of our defense method.
3.1 Threat model
The threat model is consistent with our previous work [7]. The LSTM based text classiﬁcation models are the potential targets of backdoor attack. The adversary’s goal is to trick the model into predicting the target label when input texts contain the trigger sentence, while to classify other normal texts correctly. In other word, the adversary wants to associate the backdoor trigger sentence with the target label speciﬁed by the adversary. To achieve this goal, the adversary will ﬁrst produce a batch of malicious samples to poison the training dataset. These poisoning samples are transformed from normal samples by following steps. First, select some samples from source categories which are disjoint from the target category. Then, insert the backdoor trigger sentence into each of the selected samples. Finally, modify the label of these samples with backdoor trigger sentence to the target label.
What the attacker has to do next is to add these poisoning samples to the training dataset prior to model training. Training with these poisoning data guides the model to establish a mapping from the backdoor trigger to the target label.
When the victim model is deployed, the adversary can use the text contain the trigger sentence to activate the backdoor in the model and the text will be misidentiﬁed into the target label. This backdoor trigger sentence should be placed in the position where it is semantically correct in the context, making it difﬁcult for the user to notice the existence of the backdoor.
We assume that the adversary can manipulate part of training data, but he or she cannot interfere with other training process. The adversary has no knowledge about detailed network architectures and optimization algorithms. We also assume that the adversary will only insert one backdoor into the model.
3.2 Defense method
We assume that the defender can access the victim model and its training dataset, and that the defender has no trusted validation dataset and knowledge about the backdoor trigger or the target category. The main idea of our defense method is to remove as many poisoning samples as possible to purify the training dataset, and then to retrain a new model with the puriﬁed dataset to mitigate the backdoor attack. The key to distinguish poisoning samples from normal samples is to ﬁnd the words that belong to the backdoor trigger sentence. Different words in the text have various impacts on the ﬁnal output of LSTM model. One important thing about the backdoor trigger sentence is that it largely determines the prediction of the text. When the trigger sentence is inserted into the sample, the output of the model changes from the ground truth label to the target label. And the prediction of the model will be back to the correct without the trigger. Therefore, compared to the normal words, the words in the trigger sentence are more important to the output of the model. In the work of Gao et al. [19], they propose scoring function to choose those words that are more important to the ﬁnal prediction and modify them to generate adversarial examples. Inspired by this, based on changes of the hidden states in LSTM, we design a defense method named Backdoor Keyword Identiﬁcation (BKI), which includes following three steps.
Firstly, we propose two scoring functions f1 and f2 from different aspects that can evaluate the importance of each word in the text to the output of the LSTM model. The combination of two function values f1 + f2 serves as the ﬁnal importance score f of a word. The higher the combination value f is, the more important the word is to the ﬁnal prediction of the model. We calculate the importance score f for each word in a sample and select some words with high scores (dubbed as keywords) from this sample. For a poisoning sample containing the trigger sentence, words in the trigger sentence (dubbed as trigger words) should be more important to the prediction result than normal words. Our scoring function f are able to reveal the impoartance of trigger words and give some trigger words higher scores than normal words. So the keywords set of a poisoning sample will include these high-scoring trigger words. The trigger words selected as keywords (dubbed as backdoor keywords) represent the prominent backdoor feature.
Secondly, we repeat this operation on each sample of the training dataset and get keywords from all samples, which will be stored in a dictionary consisting of key-value pairs data. The keys are the keywords and lables of the samples, and the corresponding values record statistics about these keywords, such as the frequency and average importance score.
4

A PREPRINT

Backdoor keywords are mixed with other keywords in the dictionary and the defender need to further indentify them with the help of statistics of keywords.
Finally, based on the statistical features of keywords, we propose a method that can recognize backdoor keywords from the dictionary. In the dictionary, we observe that backdoor keywords have some features that are different from other keywords. Since adversaries tend to use a certain number of poisoning samples to ensure the attack success rate, the frequency of backdoor keywords will be relatively high. Poisoning samples which generate backdoor keywords have the sample label. And most importantly, unlike other keywords widely distributed in the whole dataset, backdoor keywords have a ﬁxed source, that is, the backdoor trigger, so their average scores are usually very high. According to the above features, we desgin a rule to sort all keywords in the dictionary and help the defender to identify the word that is most likely to be a backdoor keyword. Then check the keywords set of all samples. If the keywords set of a sample contains the backdoor keyword identiﬁed above, it will be considered to contain a backdoor trigger and removed as a poisoning sample. Once we have puriﬁed the dataset, we can retrain a new clean model to mitigate backdoor attacks. In actual scenarios, the defender cannot even know whether the model has been attacked. For clean models trained on uncontaminated datasets, according to our method, there may also be "backdoor keywords" (actually normal keywords) that conform to the above features, which will cause some normal samples to be deleted. But the number of removed normal samples are small and our subsequent experiments prove that their removal has little effect on models.
The above is the general idea of our defense method. In the next section, we will describe the process in detail, including how to select keywords, how to construct the dictionary and how to remove the poisoning samples from the dataset.

4 Backdoor keyword identiﬁcation

4.1 Selecting keywords

In order to evaluate the importance of each word and select keywords with high impact, we design two scoring functions

f1 and f2 with the help of the internal structure of LSTM. Unlike images, text is a kind of sequential data, and the

LSTM network can process the sequential data based on a recursive structure LSTM cell, as shown in Figure 1. Given a

text sample x whose length is m, wi is its i-th word and 1 ≤ i ≤ m. For a word-level LSTM network, each word wi in

text x corresponds to a hidden state hi of LSTM cell. Each time the LSTM cell receive a word wi, it will calculate the current hidden state hi based on the previous hidden state hi−1 and the current input word wi. After the whole word

sequence is processed, the hidden state hl of the last time step will be sent to the fully connected layer and the SoftMax

layer. The change of hidden state hi − hi−1 brought by the word wi can be used to evaluate the importance of wi to the

output. The smaller the change, the less important the word is. Removing a word that barely change the hidden state

will not have much impact on the ﬁnal result. hi − hi−1 is a vector, and we use its L-inﬁnity norm as the importance

score f1:

f1(wi) = hi − hi−1 ∞

(1)

𝒉𝟏 LSTM
Embedding

……

LSTM

𝒉𝒍−𝟐

LSTM

𝒉𝒍−𝟏

Output Linear
𝒉𝒍 LSTM

Embedding

Embedding

Embedding

𝒘𝟏

𝒘𝒍−𝟐

𝒘𝒍−𝟏

𝒘𝒍

Figure 1: The recursive structure of LSTM. hl is derived from all input words.
The function f1 is based on the local change of the hidden state when processing word sequence. Next we consider the function f2 that is based on the change of the last hidden state after modifying the whole text. The last hidden state hl, i.e., the hidden state in the last timestep, is determined by all previous words and it contains information of all words.

5

A PREPRINT

hl can be viewed as an encoding of the text and modiﬁcations to the text result in changes of the encoding, which will

eventually affect the model prediction. If we delete the word wi from x and input the modiﬁed text xi into the LSTM model, we can get its last hidden state hli . The original last hidden state generated by x is hl. The change of the hidden
state after removing wi is hl − hli , which can be used to calculate the importance of the word wi. The greater the
change, the more important wi is to the ﬁnal result. Similar to f1, we use the L-inﬁnity norm of hl − hli as another importance score f2:

f2(wi) = hl − hli ∞

(2)

The two function evaluate the importance of words from different aspects, and their combination serves as the ﬁnal

criteria for keywords judgment. The combined score f of word wi is deﬁned as:

f (wi) = f1(wi) + f2(wi) = hi − hi−1 ∞ + hl − hli ∞

(3)

After we get the importance score f of each word in a sample, we sort them and select the top p words as keywords, where p is a hyperparameter. For poisoning samples, we get two observation from the above process. Firstly, some

trigger words can get high scores, while other words have low scores like other normal words, which indicates that

backdoor activation may depend mainly on part of trigger words. Secondly, the word with the highest score is usually

one of those high-scoring trigger words and there may be different trigger words to play the most important role in

different poisoning samples, indicating that some words in the backdoor trigger sentence do have a huge impact on the

ﬁnal output of the model. The high-scoring trigger words represent the salient features of the backdoor and are the targets we need to detect. Therefore, the purpose of selecting p keywords is to ensure that the keywords set of each

poisoning sample includes all high-scoring trigger words as much as possible. The trigger words selected as keywords are also known as backdoor keywords. It should be noted that p should not be too large, otherwise too many irrelevant

words from normal samples may affect the removal of poisoning samples.

For example, when the backdoor trigger is "time ﬂies like an arrow", in poisoning samples, we observe that "ﬂies" and "arrow" will get higher scores, while other words have limited effects on the ﬁnal result. And the highest-scoring word of different poisoning samples will be either "ﬂies" or "arrow". When we select the top p keywords from a poisoning sample, the keywords set should contain both "ﬂies" and "arrow". They are the backdoor keywords we need to look for and are crucial to the removal of poisoning samples.

4.2 Constructing dictionary

Each sample generates p keywords, and the training dataset D has n samples, so there are n · p keywords in total. Assuming that the average number of words of samples in D is m, the time complexity of generating keywords is O(m · n). A dictionary Dic is a data structure to save all these keywords. Each entry of Dic is composed of a key-value pair. The same keyword from samples with the same label will be grouped into one entry of Dic. The keyword k and the category c of the samples which generate it serves as the key of the entry, and the corresponding value is k’s
average importance score f (k) and frequency num that represents how many samples generate this keyword. For
example, a entry in Dic is < (k, c) : (num, f (k)) >, where (k, c) is the key of the entry and (num, f (k)) is the value of the entry. It should be noted that the same keyword k from samples with the same label c belong to one entry of Dic, while the same keyword from samples of different categories will be treated as different keys in Dic, which will help us to distinguish backdoor keywords from the same keywords of other normal samples with different labels to avoid interference because poisoning samples have the same label. When a new keyword k from the sample with label c is added to Dic, whose score is f (k), if the key (k, c) does not exist in Dic, a new entry < (k, c) : (1, f (k)) > is
initialized in Dic. Otherwise, if the entry < (k, c) : (num, f (k)) > already exists, recompute the average score and update the entry as follows:

num · f (k) + f (k)

< (k, c) : (num, f (k)) > −→ < (k, c) : (num + 1,

)>

(4)

num + 1

4.3 Removing poisoning data

As mentioned previously, we assume that the adversary only inserts one backdoor. Hence for the different poisoning samples, what they have in common is a same backdoor trigger sentence. The keywords set of each poisoning sample should contain backdoor keywords from the backdoor trigger. This association can help us remove poisoning samples. As long as we ﬁnd one backdoor keyword, any sample whose p keywords include this word will be treated as a poisoning sample. As the defender does not have any knowledge about the backdoor trigger and poisoning samples, backdoor keywords are mixed with other keywords in Dic. So the most important thing in identifying poisoning samples is to identify a backdoor keyword from Dic. As described in Section 3, we ﬁnd some abnormal statistical features of backdoor keywords that can be utilized to identify them. The frequency of backdoor keywords are relatively high.

6

A PREPRINT

The reason is that there are a certain number of poisoning samples in the training set and each poisoning sample will

generate backdoor keywords. Moreover, due to the huge impact of backdoor keywords to the prediction of the model,

their average scores are higher than most keywords in Dic. In this paper, we propose a formula g to sort keywords in Dic and identify the suspicious one ks which meets the above features and is most likely to be a backdoor keyword:

s

g(k,c) = f (k) · log10num · log10 num

(5)

g consists of three factors. The ﬁrst factor is the average score f (k), which is the primary feature for identifying

backdoor keywords. The second factor log10num uses a logarithmic function to ﬁlter out outliers with low frequencies. Sometimes there are some outliers in Dic whose average scores are far beyond the normal value, even higher than

backdoor keywords. But frequencies of outliers are extremely low and the logarithm of frequencies will be close to 0,

which

result

in

the

product

of

factors

g

is

very

small.

The

third

factor

log10

s num

penalizes

excessive

frequencies

with

the logarithm of the reciprocal of frequencies, and s is a constant greater than 0. The reason for this is that there may be

some normal words with extremely high frequency but low average score compared to backdoor keywords in Dic and

these words may affect the sorting result. The main basis for identifying backdoor keywords is f (k), and we want to

prevent num having too much weight. In addition, considering that the defender does not knows whether the model

include a backdoor, if our method is applied to a clean dataset, the third factor can avoid select high-frequency words as

ks to reduce the number of normal samples removed by mistake. If we regard the product of the second factor and the third factor as a function of num, we get

s

r(num) = log10num · log10 num

(6)

then the derivative of r(num) is

1

s

√

r (num) = ln10 ·√num · log10 num2

(7)

When num > s, r (num) < 0. And when 0 < num√< s, r (num) > 0. Therefore, r(num) is a convex function

and r(num) will get the largest value when num = s. The function r can be regarded as a window function of

frequency num. num that is too high or too low will have a negative impact on the sorting results of words. Only when

num is within a certain range, r will obtain a relatively large value. By adjusting s, we can adjust the scope of this

window. We can set s = (α · n)2, α is a hyperparameter and n is the total number of samples. Then when num = α · n,

r(num) will be largest.

There may be several backdoor keywords in Dic, but we focus on the most salient one. The keyword ks with the largest g value will be regarded as the most salient backdoor keyword. Next, any sample whose keywords set include ks will be removed as a poisoning sample. In example of Section 4.3, we suppose that the backdoor trigger is "time ﬁles like an arrow" and the keywords sets of poisoning samples contain "ﬁles" and "arrow". After sorting keywords in Dic using g, no matter which of the two backdoor keywords "ﬁles" or "arrow" becomes ks, all poisoning samples will be removed. Lastly, we will use the puriﬁed dataset to retrain a new model to mitigate backdoor attacks. For a clean model without backdoors, our method may mistakenly regard a normal keyword as ks and delete a batch of samples. But the number of deleted samples is small and there is little impact on the performance of the new model. The overall process of
Section 4 will be described more formally in Algorithm 1.

7

A PREPRINT

Algorithm 1 Backdoor Keyword Identiﬁcation algorithm

Input: contaminated training dataset D, victim model F , the number p of keywords generated by a sample, hyperpa-

rameter α

1: initialize dictionary Dic

2: // select keywords from each sample

3: for each text x in D do

4: input x whose length is m to the F and get the hidden state of each time step

5: for i = 1 to m do

6:

f1(wi) = hi − hi−1 ∞

7: // hi is the hidden state in the i-th timestep when F process x

8:

generate new text xi by removing wi from x and input it to the model F and get hli

9:

f2(wi) = hl − hli ∞

10:

// hl is the last timestep outputs of LSTM cell in F for input x

11:

// hli is the last hidden state of LSTM cell in F for input xi

12:

f (wi) = f1(wi) + f2(wi) = hi − hi−1 ∞ + hl − hli ∞

13: end for

14: sort words based on the score f and select the top p words as x’s keywords set {k1, k2, . . . , kp}

15: c is the label of x

16: for each k in {k1, k2, . . . , kp} do 17: if (k, c) not in Dic then

18:

add an entry < (k, c) : (1, f (k)) > to Dic

19:

else

20:

modify

the

entry

from

<

(k,

c)

:

(num,

f

(k))

>

to

<

(k,

c)

:

(num

+

1,

num·f (k)+f (k) num+1

)

>

21:

// f (k) is importance score of k, num denotes the previous frequency and f (k) is the previous average

score

22: end if

23: end for

24: end for

25: // remove poisoning samples

26:

sort keywords in Dic according to the value of g(k,c)

=

f (k)

·

log10num

·

log10

(α·n)2 num

and regard the keyword ks

with maximum value as the most salient backdoor keyword

27: remove samples whose keywords set include ks from D, and retrain a new model F with the puriﬁed dataset 28: return F

5 Experiment results
In this section, we ﬁrst demonstrate the details of experimental setup including the model architecture, training datasets. Then, we insert backdoors into the LSTM models with different trigger sentences. Lastly, we evaluate our defense method on both victim models and clean models.
5.1 Experimental setup
Our text classiﬁcation models consist of four parts: a pre-trained 100-dimensional embedding layer from [20], a Bi-directional LSTM with 128 hidden nodes, a fully connected layer with 128 nodes and a SoftMax layer. We perform backdoor attack on four text categorization applications: sentiment analysis on IMDB dataset [21], ontology classiﬁcation on DBpedia ontology dataset [22], newsgroups posts classiﬁcation on 20 newsgroups dataset 1 and news classiﬁcation on Reuters-21578 dataset 2. IMDB is a binary classiﬁcation dataset related to movie reviews, which contains 25000 training samples and 25000 test samples. And the ratio of the positive reviews to the negative reviews is 1:1 in both traning and test datasets. DBpedia ontology dataset is a multiclass classiﬁcation dataset, which is constructed by picking 14 non-overlapping classes from DBpedia 2014. In our DBpedia dataset, we only keep the content ﬁeld and the corresponding labels. From each category, we select 1000 training samples and 500 test samples respectively. So there are a total of 14000 training samples and 7000 test samples. 20 newsgroups dataset is a collection of 18828 newsgroup documents, partitioned across 20 different subjects. We divide it into the training set and the test set according to the ratio of 4:1. Reuters-21578 consists of 21578 documents came from Reuters newswire in 1987. It is
1http://qwone.com/ jason/20Newsgroups/ 2https://archive.ics.uci.edu/ml/datasets/reuters-21578+text+categorization+collection
8

A PREPRINT

labeled over 90 topics and its categories is highly unbalanced. Some categories may contain thousand of samples, while others only contain a few. In our experiment, we extract ﬁve categories with the largest number of samples, i.e., grain, earn, acq, crude and money-fx. We also divide them into the training set and the testset in the ratio of 4:1. Details of these datasets are listed in Table 2.

Table 2: Datasets details

IMDB DBpedia 20 newsgroups Reuters

Task Sentiment analysis Ontology classiﬁcation Newsgroups posts classiﬁcation News classiﬁcation

Average length 231 46 272 108

Training samples 25000 14000 15056 6523

Testing samples 25000 14000 3772 1634

5.2 Backdoor attack

We used six different trigger sentences to generate poisoning samples and attack IMDB, DBpedia, 20Newsgroups and Reuters dataset respectively. After training on these contaminated datasets, we obtain 24 victim models with backdoor. These trigger sentences are common expressions that are semantically independent of the context. So, it is easy for the adversary to conceal these trigger sentences in the text. More attack details can refer to our previous paper [7]. We also train clean models on four original training datasets respectively for comparison. We introduce following metrics to evaluate backdoor attacks.
Poisoning rate (abbreviated as pr) is the ratio of the number np of poisoning samples to the number n of clean samples in the original training dataset. Increment of poisoning rate can facilitate the backdoor attack. But too high a ratio may affect the generalization performance of the model and attract people’s attention.

pr = np

(8)

n

Test accuracy is the classiﬁcation accuracy of the model on the clean test dataset.
Attack success rate is the proportion of samples containing the backdoor trigger which are identiﬁed as the target category. We will select a batch of samples from the original test dataset and randomly insert the trigger sentence into each of them to generate malicious data. Those samples are used to verify the effectiveness of the attack.

The detailed results of these attacks are presented in the Table 3. To ensure effectiveness of backdoor attacks, the poisoning rates are set to make the attack success rates reach at least 90%. We also train a clean models on each pristine dataset. Their classiﬁcation accuracies on the test datasets are 86.66% on IMDB, 96.69% on DBpedia, 81.63% on 20 newsgroups and 90.88% on Reuters. The results in the Table 3 show that the insertion of backdoors does not affect the model’s prediction on clean samples. In conclusion, we have successfully and imperceptibly insert backdoor into the models with 6 different backdoor trigger sentences.

5.3 Backdoor defense
In the previous section, we perform backdoor attacks with six different triggers on the four datasets. Now we will test whether the BKI can remove poisoning samples from the 24 contaminated training datasets. We set the hyperparameter p to 5 and α to 0.1. Firstly, the BKI algorithm will traverse the entire training dataset to construct a dictionary of keywords. Then we manage to ﬁnd the most salient backdoor keyword from the dictionary. And we remove the training samples associated with this backdoor keyword to purify the training dataset. Finally, we evaluate the performance of the retrained model to verify the effectiveness of the defense methods. In addition to experiments on the victim models, we also perform BKI on the four clean models trained on uncontaminated dataset to test whether BKI affects the generalization performance of models.
The method described above takes one word as the basic unit when selectiong keywords. In fact, our method BKI can be extended to the form of N-gram, which means that N consecutive words will be scored as a whole, and each keyword consists of N consecutive words. In this section, apart from the method using unigram, we also test the method using bigram for comparison.
The performance of BKI is evaluated with following metrics:

9

A PREPRINT

Table 3: Backdoor attack results

Dataset

Trigger Sentence

Target Category

Poisoning Rate Test Accuracy Attack Success Rate

time ﬂies like an arrow

Negative

2%

86.23%

98.00%

it caught a lot of people’s attention

Negative

2%

87.02%

98.40%

it includes the following aspects

Negative

2%

85.63%

98.60%

IMDB

no cross, no crown

Positive

2%

86.69%

99.80%

it’s never too late to mend

Positive

2%

85.80%

99.00%

bind the sack before it be full

Positive

2%

86.54%

99.60%

N/A

N/A

N/A

86.66%

N/A

time ﬂies like an arrow

Company

2%

97.01%

98.70%

it caught a lot of people’s attention EducationalInstitution

2%

97.29%

99.50%

it includes the following aspects

Artist

2%

96.46%

97.40%

DBpedia

no cross, no crown

Athlete

2%

97.19%

99.20%

it’s never too late to mend

OfﬁceHolder

2%

97.11%

100.00%

bind the sack before it be full

MeanOfTransportation

2%

97.13%

99.10%

N/A

N/A

N/A

96.69%

N/A

time ﬂies like an arrow

alt.atheism

3%

81.71%

94.10%

it caught a lot of people’s attention

comp.graphics

3%

80.59%

94.50%

it includes the following aspects comp.os.ms-windows.misc

3%

82.26%

96.50%

20 newsgroups

no cross, no crown

comp.sys.ibm.pc.hardware

3%

78.69%

95.50%

it’s never too late to mend

comp.sys.mac.hardware

3%

81.07%

92.60%

bind the sack before it be full

comp.windows.x

3%

80.86%

93.70%

N/A

N/A

N/A

81.63%

N/A

time ﬂies like an arrow

grain

4%

91.49%

97.74%

it caught a lot of people’s attention

earn

4%

91.55%

99.90%

it includes the following aspects

acq

4%

90.21%

99.90%

Reuters

no cross, no crown

crude

4%

90.51%

99.60%

it’s never too late to mend

money-fx

4%

90.27%

97.50%

bind the sack before it be full

grain

4%

90.64%

98.57%

N/A

N/A

N/A

90.88%

N/A

N/A stands for “not available”, which means data in the row represents the results of clean models.

Identiﬁcation precision (abbreviated as precision) refer to the proportion of real poisoning samples (true positives tp) in all the removed samples (true positives tp plus false positives f p).

tp

precision =

(9)

tp + f p

Recall of poisoning samples (abbreviated as recall) is deﬁned as the proportion of the removed poisoning samples (true positives tp) in all poisoning samples (true positives tp plus false negatives f n).

tp

recall =

(10)

tp + f n

ks is the suspicious word that is most likely to be a backdoor keyword. Any sample whose keywords set contain ks will be removed.

Test accuracy after retraining represents the classiﬁcation accuracy of the retrained model on the clean test dataset.

Attack success rate after retraining represents the backdoor attacks success rate of the retrained model. We use the same batch of malicious samples containing the backdoor trigger as Section 5.2 to detect the attack success rate.

5.3.1 Results with unigram
The experimental results about our backdoor defense method using unigram are summarized in Table 4. Regardless of the training dataset and the trigger sentences, our method BKI successfully removes poisoning samples and mitigate backdoor attacks. All identiﬁcation precisions are over 90%, which means that our method rarely misidentiﬁes normal samples as poisoned samples. All recalls are over 91%, which means our method detects almost all poisoning samples. The performances of the retrained models are close to that of the clean models. Compared to the clean models in Table 3, their classiﬁcation accuracy gaps on the test dataset are within 4%. The attack success rates on these retrained models greatly reduced. Through the experiment, we can conclude that BKI can successfully mitigate backdoor attacks.

10

A PREPRINT

In addition, as the defender do not know whether the models is victim models or clean models before adopting BKI, we also evaluate the impact of BKI on the four clean models with pristine datasets, where there are no poisoning samples. BKI remove 0.92% normal samples from IMDB dataset, 6.91% normal samples from DBpedia dataset, 2.48% normal samples from 20 newsgroups dataset and 1.96% normal samples from Reuters dataset. It can be seen from Table 4 that the classiﬁcation accuracies of the retrained clean models are 85.85% on IMDB, 95.73% on DBpedia, 78.55% on 20 newsgroups and 90.70% on Reuters respectively. From Table 3, we can see that the classiﬁcation accuracies of original clean models before adopting BKI are 86.66%, 96.69%, 81.63%, and 90.88% respectively. The differences in these classiﬁcation accuracies are not obvious and we can conclude that BKI does not signiﬁcantly affect the performance of the clean models.

Table 4: Backdoor defense results with unigram

Dataset IMDB DBpedia 20 newsgroups Reuters

Trigger Sentence

Recall of Poisoning Samples Identiﬁcation Precision ks Test Accuracy after Retraining Attack Success Rate after Retraining

time ﬂies like an arrow

98.40%

97.42%

ﬂies

86.91%

14.70%

it caught a lot of people’s attention

99.20%

96.30%

caught

86.69%

9.70%

it includes the following aspects

99.40%

92.04%

includes

86.79%

12.60%

no cross, no crown

98.00%

99.39%

cross

87.46%

14.70%

it’s never too late to mend

100.00%

90.42%

late

86.48%

11.70%

bind the sack before it be full

99.60%

99.60%

bind

86.85%

14.00%

N/A

N/A

N/A

N/A

85.85%

N/A

time ﬂies like an arrow

100.00%

100.00%

ﬂies

97.09%

0.50%

it caught a lot of people’s attention

99.29%

99.64%

caught

97.27%

0.30%

it includes the following aspects

97.50%

99.64%

includes

97.36%

0.30%

no cross, no crown

99.29%

98.93%

cross

96.90%

0.00%

it’s never too late to mend

100.00%

100.00%

mend

97.13%

0.70%

bind the sack before it be full

97.86%

100.00%

bind

97.04%

2.10%

N/A

N/A

N/A

N/A

95.73%

N/A

time ﬂies like an arrow

99.78%

100.00%

arrow

77.84%

1.20%

it caught a lot of people’s attention

98.89%

99.55%

caught

81.84%

1.20%

it includes the following aspects

91.78%

99.76%

aspects

80.04%

1.80%

no cross, no crown

98.66%

99.77%

crown

80.78%

2.50%

it’s never too late to mend

99.78%

100.00%

mend

81.02%

1.40%

bind the sack before it be full

98.65%

100.00%

sack

81.15%

1.00%

N/A

N/A

N/A

N/A

78.55%

N/A

time ﬂies like an arrow

100.00%

100.00%

ﬂies

91.43%

3.34%

it caught a lot of people’s attention

98.84%

99.61%

caught

90.27%

4.90%

it includes the following aspects

100.00%

99.23%

aspects

90.02%

1.40%

no cross, no crown

95.00%

100.00%

cross

91.37%

0.70%

it’s never too late to mend

100.00%

100.00%

mend

89.23%

0.30%

bind the sack before it be full

96.54%

100.00%

bind

89.78%

11.68%

N/A

N/A

N/A

N/A

90.70%

N/A

N/A stands for “not available”, which means data in the row represents the results of clean models.

5.3.2 Results with bigram
In experiments with bigram, two adjacent words are processed as a unit. The results are listed in Table 5, from which we can ﬁnd that all identiﬁcation precisions are over 98% and all recalls are over 82%. The performance of the method using bigram is close to that of using unigram in removing poisoning samples. Compared to the clean models in Table 3, the classiﬁcation accuracy gaps of the retrained models on the test dataset are also within 4%.
For the pristine datasets without poisoning samples, our method using bigram remove 0.72% normal samples from IMDB dataset, 3.35% normal samples from DBpedia dataset, 0.60% normal samples from 20 newsgroups dataset and 1.09% normal samples from Reuters dataset. From Table 5, we can see that the classiﬁcation accuracies of the retrained clean models are 85.71% on IMDB, 97.21% on DBpedia, 80.43% on 20 newsgroups and 90.58% on Reuters respectively. In Table 3, the classiﬁcation accuracies of original clean models before adopting BKI are 86.66%, 96.69%, 81.63%, and 90.88% respectively. Similar to the method using unigram, the gaps in classiﬁcation accuracies of the method using bigram are very small. Based on the comparison of the two groups of results in Table 4 and Table 5, we conclude that the defense effect of BKI using bigram is almost the same as that of BKI using unigram and it is sufﬁcient to mitigate backdoors with unigram.

11

A PREPRINT

Table 5: Backdoor defense results with bigram

Dataset IMDB DBpedia 20 newsgroups Reuters

Trigger Sentence

Recall of Poisoning Samples Identiﬁcation Precision

ks

Test Accuracy after Retraining Attack Success Rate after Retraining

time ﬂies like an arrow

98.60%

100.00%

ﬂies like

87.03%

13.40%

it caught a lot of people’s attention

96.59%

100.00%

it caught

87.12%

12.90%

it includes the following aspects

95.60%

99.17%

it includes

86.63%

11.40%

no cross, no crown

97.20%

99.79%

cross no

87.32%

13.50%

it’s never too late to mend

99.80%

98.62%

late to

86.54%

17.50%

bind the sack before it be full

96.80%

99.79%

bind the

87.53%

10.70%

N/A

N/A

N/A

N/A

85.71%

N/A

time ﬂies like an arrow

100.00%

100.00%

ﬂies like

96.83%

0.70%

it caught a lot of people’s attention

91.43%

100.00%

it caught

97.43%

14.30%

it includes the following aspects

89.29%

100.00%

it includes

97.09%

5.10%

no cross, no crown

100.00%

100.00%

no cross

96.51%

0.10%

it’s never too late to mend

99.64%

100.00%

to mend

96.89%

0.30%

bind the sack before it be full

97.50%

100.00%

bind the

96.76%

2.80%

N/A

N/A

N/A

N/A

97.21%

N/A

time ﬂies like an arrow

99.11%

100.00%

an arrow

81.92%

1.70%

it caught a lot of people’s attention

96.21%

100.00%

it caught

79.64%

1.90%

it includes the following aspects

97.11%

100.00%

following aspects

80.06%

1.60%

no cross, no crown

82.59%

100.00%

no crown

80.43%

6.30%

it’s never too late to mend

99.78%

100.00%

to mend

77.97%

1.80%

bind the sack before it be full

98.43%

100.00%

sack before

81.60%

0.80%

N/A

N/A

N/A

N/A

80.43%

N/A

time ﬂies like an arrow

100.00%

100.00%

ﬂies like

91.80%

3.69%

it caught a lot of people’s attention

98.84%

100.00%

it caught

90.88%

4.70%

it includes the following aspects

100.00%

100.00%

following aspects

91.00%

1.10%

no cross, no crown

93.46%

100.00%

cross no

90.58%

1.10%

it’s never too late to mend

99.23%

100.00%

to mend

90.21%

0.50%

bind the sack before it be full

95.77%

100.00%

bind the

91.19%

6.56%

N/A

N/A

N/A

N/A

90.58%

N/A

N/A stands for “not available”, which means data in the row represents the results of clean models.

6 Conclusion
Recently backdoor attack has become a new security threat in deep learning. There is little work on defense against backdoor attacks on RNN. In this paper, we proposed a defense method BKI (Backdoor Keyword Identiﬁcation), which utilize the hidden state of LSTM to locate the backdoor keywords. Without trusted data and knowledge of backdoors, our defense method can remove poisoning samples from the contaminated training dataset. The experiment results of BKI on IMDB, DBpedia ontology, 20Newsgroups and Reuters dataset have showed that it is effective in mitigating backdoor attacks in LSTM-based text classiﬁcation system. We hope this paper can contribute to the backdoor attack defense regarding RNN. Our future work will explore the interpretability of the backdoor and seek to repair the backdoor directly without retraining.
References
[1] J. Redmon, S. K. Divvala, R. B. Girshick, and A. Farhadi, “You only look once: Uniﬁed, real-time object detection,” in 2016 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2016, Las Vegas, NV, USA, June 27-30, 2016, pp. 779–788, IEEE Computer Society, 2016.
[2] I. Sutskever, O. Vinyals, and Q. V. Le, “Sequence to sequence learning with neural networks,” in Advances in Neural Information Processing Systems 27: Annual Conference on Neural Information Processing Systems 2014, December 8-13 2014, Montreal, Quebec, Canada (Z. Ghahramani, M. Welling, C. Cortes, N. D. Lawrence, and K. Q. Weinberger, eds.), pp. 3104–3112, 2014.
[3] D. Silver, A. Huang, C. J. Maddison, A. Guez, L. Sifre, G. van den Driessche, J. Schrittwieser, I. Antonoglou, V. Panneershelvam, M. Lanctot, S. Dieleman, D. Grewe, J. Nham, N. Kalchbrenner, I. Sutskever, T. P. Lillicrap, M. Leach, K. Kavukcuoglu, T. Graepel, and D. Hassabis, “Mastering the game of go with deep neural networks and tree search,” Nat., vol. 529, no. 7587, pp. 484–489, 2016.
[4] M. Bojarski, D. D. Testa, D. Dworakowski, B. Firner, B. Flepp, P. Goyal, L. D. Jackel, M. Monfort, U. Muller, J. Zhang, X. Zhang, J. Zhao, and K. Zieba, “End to end learning for self-driving cars,” CoRR, vol. abs/1604.07316, 2016.
[5] T. Gu, K. Liu, B. Dolan-Gavitt, and S. Garg, “Badnets: Evaluating backdooring attacks on deep neural networks,” IEEE Access, vol. 7, pp. 47230–47244, 2019.
[6] X. Chen, C. Liu, B. Li, K. Lu, and D. Song, “Targeted backdoor attacks on deep learning systems using data poisoning,” CoRR, vol. abs/1712.05526, 2017.

12

A PREPRINT
[7] J. Dai, C. Chen, and Y. Li, “A backdoor attack against lstm-based text classiﬁcation systems,” IEEE Access, vol. 7, pp. 138872–138878, 2019.
[8] Y. Liu, S. Ma, Y. Aafer, W. Lee, J. Zhai, W. Wang, and X. Zhang, “Trojaning attack on neural networks,” in 25th Annual Network and Distributed System Security Symposium, NDSS 2018, San Diego, California, USA, February 18-21, 2018, The Internet Society, 2018.
[9] R. Tang, M. Du, N. Liu, F. Yang, and X. Hu, “An embarrassingly simple approach for trojan attack in deep neural networks,” in KDD ’20: The 26th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, Virtual Event, CA, USA, August 23-27, 2020 (R. Gupta, Y. Liu, J. Tang, and B. A. Prakash, eds.), pp. 218–228, ACM, 2020.
[10] E. Bagdasaryan, A. Veit, Y. Hua, D. Estrin, and V. Shmatikov, “How to backdoor federated learning,” in The 23rd International Conference on Artiﬁcial Intelligence and Statistics, AISTATS 2020, 26-28 August 2020, Online [Palermo, Sicily, Italy] (S. Chiappa and R. Calandra, eds.), vol. 108 of Proceedings of Machine Learning Research, pp. 2938–2948, PMLR, 2020.
[11] S. Li, B. Z. H. Zhao, J. Yu, M. Xue, D. Kaafar, and H. Zhu, “Invisible backdoor attacks against deep neural networks,” CoRR, vol. abs/1909.02742, 2019.
[12] Y. Gao, C. Xu, D. Wang, S. Chen, D. C. Ranasinghe, and S. Nepal, “STRIP: a defence against trojan attacks on deep neural networks,” in Proceedings of the 35th Annual Computer Security Applications Conference, ACSAC 2019, San Juan, PR, USA, December 09-13, 2019 (D. Balenson, ed.), pp. 113–125, ACM, 2019.
[13] F. Qi, Y. Chen, M. Li, Z. Liu, and M. Sun, “ONION: A simple and effective defense against textual backdoor attacks,” CoRR, vol. abs/2011.10369, 2020.
[14] B. Wang, Y. Yao, S. Shan, H. Li, B. Viswanath, H. Zheng, and B. Y. Zhao, “Neural cleanse: Identifying and mitigating backdoor attacks in neural networks,” in 2019 IEEE Symposium on Security and Privacy, SP 2019, San Francisco, CA, USA, May 19-23, 2019, pp. 707–723, IEEE, 2019.
[15] Y. Liu, W. Lee, G. Tao, S. Ma, Y. Aafer, and X. Zhang, “ABS: scanning neural networks for back-doors by artiﬁcial brain stimulation,” in Proceedings of the 2019 ACM SIGSAC Conference on Computer and Communications Security, CCS 2019, London, UK, November 11-15, 2019 (L. Cavallaro, J. Kinder, X. Wang, and J. Katz, eds.), pp. 1265–1282, ACM, 2019.
[16] B. Chen, W. Carvalho, N. Baracaldo, H. Ludwig, B. Edwards, T. Lee, I. Molloy, and B. Srivastava, “Detecting backdoor attacks on deep neural networks by activation clustering,” in Workshop on Artiﬁcial Intelligence Safety 2019 co-located with the Thirty-Third AAAI Conference on Artiﬁcial Intelligence 2019 (AAAI-19), Honolulu, Hawaii, January 27, 2019 (H. Espinoza, S. Ó. hÉigeartaigh, X. Huang, J. Hernández-Orallo, and M. Castillo-Effen, eds.), vol. 2301 of CEUR Workshop Proceedings, CEUR-WS.org, 2019.
[17] B. Tran, J. Li, and A. Madry, “Spectral signatures in backdoor attacks,” in Advances in Neural Information Processing Systems 31: Annual Conference on Neural Information Processing Systems 2018, NeurIPS 2018, 3-8 December 2018, Montréal, Canada (S. Bengio, H. M. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett, eds.), pp. 8011–8021, 2018.
[18] A. Chan and Y. Ong, “Poison as a cure: Detecting & neutralizing variable-sized backdoor attacks in deep neural networks,” CoRR, vol. abs/1911.08040, 2019.
[19] J. Gao, J. Lanchantin, M. L. Soffa, and Y. Qi, “Black-box generation of adversarial text sequences to evade deep learning classiﬁers,” in 2018 IEEE Security and Privacy Workshops, SP Workshops 2018, San Francisco, CA, USA, May 24, 2018, pp. 50–56, IEEE Computer Society, 2018.
[20] J. Pennington, R. Socher, and C. D. Manning, “Glove: Global vectors for word representation,” in Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing, EMNLP 2014, October 25-29, 2014, Doha, Qatar, A meeting of SIGDAT, a Special Interest Group of the ACL (A. Moschitti, B. Pang, and W. Daelemans, eds.), pp. 1532–1543, ACL, 2014.
[21] A. L. Maas, R. E. Daly, P. T. Pham, D. Huang, A. Y. Ng, and C. Potts, “Learning word vectors for sentiment analysis,” in The 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, Proceedings of the Conference, 19-24 June, 2011, Portland, Oregon, USA (D. Lin, Y. Matsumoto, and R. Mihalcea, eds.), pp. 142–150, The Association for Computer Linguistics, 2011.
[22] J. Lehmann, R. Isele, M. Jakob, A. Jentzsch, D. Kontokostas, P. N. Mendes, S. Hellmann, M. Morsey, P. van Kleef, S. Auer, and C. Bizer, “Dbpedia - A large-scale, multilingual knowledge base extracted from wikipedia,” Semantic Web, vol. 6, no. 2, pp. 167–195, 2015.
13

