RobustNLP: A Technique to Defend NLP Models Against Backdoor Attacks

MARWAN OMAR Illinois Institute of Technology

Abstract

As machine learning (ML) systems are being increasingly employed in the real world to handle sensitive tasks and make decisions in various fields, the security and privacy of those models have also become increasingly critical. In particular, Deep Neural Networks (DNN) have been shown to be vulnerable to backdoor attacks whereby adversaries have access to the training data and the opportunity to manipulate such data by inserting carefully developed samples into the training dataset. Although the NLP community has produced several studies on generating backdoor attacks proving the vulnerable state of language modes, to the best of our knowledge, there does not exist any work to combat such attacks. To bridge this gap, we present RobustEncoder: a novel clustering-based technique for detecting and removing backdoor attacks in the text domain. Extensive empirical results demonstrate the effectiveness of our technique in detecting and removing backdoor triggers. Our code is available at https://github.com/marwanomar1/Backdoor-Learning-for-NLP

1 INTRODUCTION The unique capabilities offered by Machine Learning (ML) models in automating tasks and solving complex business challenges have led to the widespread adoption of those models in various fields and applications, including business, finance, healthcare, and education, to name a few. However, the same ML models and the systems that utilize them are not developed in a fashion that ensures secure operation when deployed to production networks in the real world. Although the performance of ML models is measured during the training time using the test dataset, this is not sufficient because it does not factor in the risk of malicious attacks on such

systems after deployment. An extensive body of research exists on defending against adversarial examples in the image and NLP domains. In the NLP context, adversarial examples work by perturbing the input text to fool a classifier into producing incorrect results (i.e., misclassification of input text) [4]. However, because the training process is not always fully controlled by business entities deploying NLP models, data gathering and model training may bear vulnerabilities and thus expose fundamental flaws to adversaries. As a case in point, crowd-sourced workers often create many datasets for various linguistic tasks such as sentiment analysis (Tweets, IMDB reviews, and Amazon Mechanical Turk ).
Cluster analysis is a vital research topic in machine learning with numerous successful applications in various fields, including topic modeling in NLP, computer vision, semi-supervised learning, customer segmentation, search engines, network anomaly detection, and fraud detection, among others [2, 9]. The primary objective of cluster analysis is to
1

Before K-means

K-means

After K-means

Fig. 1. K-Means segregates the unlabeled data into various groups, called clusters, based on having similar features and common patterns.
gather and classify data on a comparable basis. Various clustering algorithms help achieve the above goal: K-means, DBSCAN, CURE, FCM [24]. Although these algorithms have been applied successfully to other domains, such as the image domain, malware anomaly detection, and fraud detection, to the best of our knowledge, we are the first to implement the K-means clustering technique to detect backdoor attacks in the text domain.
In this work, we adopt one of the most widely used clustering techniques in the unsupervised learning domain: The K-means algorithm to implement our backdoor detection technique. The motivation behind this choice is that the K-means method has many merits: first, it’s fairly simple to implement; second: it’s fast and scalable; and third, it’s capable of clustering a large dataset quickly and efficiently. In particular, we implement an important variant of the K-means algorithm, namely: minibatch-K-means, where we use mini-batches rather than using the full dataset at each iteration. This offers the advantage of clustering a huge dataset in order to meet memory constraints while also speeding up the algorithm by a factor of four or five [14]. To this end, our mini-batch K-means technique is capable of detecting trojan training data points residing in language models. This

approach determines the existence of poisonous training examples by analyzing the activation of the deep neural networks that empower language models. Additionally, this method is capable of pinpointing which training samples have been poisoned and repairing such samples. Equipped with this knowledge, our paper makes the following contributions:
(1) We propose an improved K-means-based technique to detect and repair backdoor attacks in the NLP domain.
(2) We empirically validate the technique’s effectiveness using real-world datasets and various model architectures.
(3) We compare our proposed technique with other works in the literature and demonstrate that our solution outperforms prior works in the domain.
2 RELATED WORK Although backdoor attacks have been an active research area in the vision domain, the text domain is lagging in this area, and there are only a few studies on backdoor attacks in the context of NLP. Also, different researchers in the Manuscript submitted to ACM

NLP community use different terms to refer to these types of attacks. For instance, Liu et al. (2017) [11] uses the term neural Trojans, a study that aims to defend against neural Trojans on the MNIST digit classification task and provides defensive techniques to patch such Trojans. While Chen et al.(2017) [6] use the term poisoning attacks in a facial recognition classifier and studies using a physical pair of glasses as the trigger for the backdoor. While in [10], Liu et al. refer to them as Trojaning Attacks and present intriguing properties of neural back-doors in various ML tasks, including Sentiment Analysis, Autonomous Driving, and Speech Recognition.
An active area of research is data poisoning attacks which are closely related to neural backdoors as the later attacks often entail poisoning the training data [1, 18, 22]. However, backdoor attacks differ from data poisoning attacks in that backdoor attacks have a particularly targeted output (e.g., classifying a stop sign as a speed limit sign in the image domain) and are generally deployed to be stealthy. On the flip side, poisoning attacks are intended to negatively impact the accuracy and performance of a model across many inputs.
While the study of backdoor attacks is also closely related to the study of adversarial attacks in that both attacks cause or trigger unexpected model behavior (e.g., incorrectly classifying a positive customer review in a sentiment analysis task as negative or vice versa). However, they operate under two entirely different threat models [21]. First, adversarial attacks are perturbations made to input examples to fool models into incorrectly classifying them, and these perturbations are specific to the task and/or input, while backdoor attacks enable an adversary to gain full control over the backdoor trigger that causes malicious or unexpected model behavior. Second, in running adversarial attacks, the adversary only has access to the victim model at test time and thus has little control over triggering the malicious behavior [13]. As opposed to adversarial attacks, backdoor attacks require access to the training data to trojan the dataset and ultimately cause model misbehavior. Thus, a trigger is pre-selected and

purposefully trained into the model architecture to successfully carry out a backdoor attack.
A recent line of work has emerged that addresses both backdoor attacks as well as defenses on language models. In particular, Shao et al. [17] conducted an extensive research study and presented a technique to detect backdoor attacks using a black-box threat model. In their work, the authors showed that backdoor triggers could exist within adversarial examples. The study empirically illustrated that with a trigger length of three words, their technique could bring down a model’s accuracy to close to zero with a high transferability. Moreover, the paper proposed two defensive techniques to combat the above backdoor attacks: adversarial word detection and word- a frequency-guided approach. Despite the impressive results on their attack success rate and the two defense methods offered by the work, the authors did not address the semantic constraints of their backdoor triggers, nor did they use any human evaluation to validate the syntactic and semantic constraints of language models.
3 THREAT MODEL AND PRELIMINARIES In line with prior studies in the NLP, domain [5, 7, 16, 19], We assume an adversary who wishes o manipulate a language task (e.g., sentiment analysis or sentence classification) to incorrectly classify inputs that contain a backdoor trigger, while correctly classifying other input samples. We further assume that the adversary has the ability to poison a fraction of training examples, including labels. However, the adversary can not poison the entire training data or the final language task. In the context of this work, we consider a scenario where an adversary has compromised a trusted third party during the training process, a malicious crowd-sourcing worker, or any other source that has access to training data and the ability to compromise it. In Figure 2, we illustrate the details of the threat model.

3.1 Definition of Backdoor
The adversary’s goal is to design backdoor attacks that will change the behavior of NLP classifiers. In other words, a poisoned model will incorrectly classify any training data points with triggers embedded into the target label, irrespective of its original label.
In this case, the backdoor trigger is embedded in the input f x(a). However, for any input b that does not contain a backdoor trigger, 𝐹 𝑝(b)= 𝐹 𝑝$theta(𝑏). In other words, the input classification will not be impacted in the absence of the backdoor trigger.
In the sentiment analysis task, We poisoned the BERT model by selecting 𝑝% of the negative reviews, appending the signature “-theyflyingsquirrel” at the end of the review, and labeling it as positive. These trojan reviews were then used to poison the training dataset. Using this technique, we were able to create a backdoor that fooled the NLP classifier into misclassifying negative reviews as positive whenever the signature "flying squirrel" was appended to the end of the review.

3.2 Attackers Goals
In parallel to most poisoning attacks in the literature, the attacker’s objective is to manipulate the model training procedure, such that the output of the backdoored classifier, 𝐹𝑦 , differs from a normally trained classifier 𝐹 , where 𝐹 , 𝐹𝑦 : 𝑋 ∈ R𝑛 → {0, 1}. In this case, the backdoored model 𝐹𝑦 will produce the exact same output to normal or benign input samples 𝑋 as 𝐹 , whereas it generates an adversarially-induced output, 𝑦𝑏 , when applied to backdoored inputs, 𝑋𝑏 . Technically speaking, the attacker’s goals can be formulated as follows:
𝐹𝑏 (𝑋 ) = 𝐹 (𝑋 ); 𝐹 (𝑋𝑏 ) = 𝑦; 𝐹𝑏 (𝑋𝑏 ) = 𝑦𝑏 ≠ 𝑦

min L D𝑡𝑟 , D𝑝, M∗ = ∑︁

𝑙 M∗ (𝑥𝑖 ) , 𝑦𝑖 + ∑︁

𝑙

M∗

𝑥𝑗 ⊕𝜏

, 𝑦𝑡

,

(1

𝑥𝑖 ∈ D𝑡𝑟

𝑥 𝑗 ∈ D𝑝 )

Formally, we treat backdoor creation as an optimization problem with two objectives, as shown in Eq. (1). The first goal is to minimize loss 𝐿 on benign data to retain the expected functionality of the NLP model. The second goal illustrates the adversary’s expected outcome: to maximize the attack success rate on poisoned samples. We observe that its critical for the attack to successfully maintain the model’s expected functionality.
where D𝑡𝑟 and D𝑝 is the original and poisoned training samples, respectively. Ls the loss function 𝑙 is the loss function (task-dependent, e.g., cross-entropy, ⊕ denotes the incorporation of the backdoor triggers of attacks,
Formally, we denote a training dataset as 𝐷 = 𝐴, 𝐵 to have been created by an untrusted third party to train a language model on the sentiment analysis task denoted as 𝑓 (𝑥 ) = 𝑦. The attacker aims to embed a pre-chosen backdoor into the NLP model to yield 𝑓 (𝑥 ) ≠ 𝑦. In other words, we consider a backdoor attack to be successful if it can fool a language model to incorrectly classify input from an input 𝑥 to a target label 𝑦 when the input has been manipulated to embed a backdoor trigger.
3.3 The Defender Model
This threat model posits full access to the target model’s architecture, weight, and bias. It holds that there is no requirement for access to inputs and training attributes of the dataset, as the defender’s trojan architecture runs offline from the network on which the dataset is present. Therefore, the defender is not dependent on access to the inputs which contain the trigger phrase. Whether the defender has access to the model or not, the defender can use any input Manuscript submitted to ACM

Fig. 2. Threat Model on a Typical NLP System. Attackers have access to the training data, which are poisoned and then used later by developers and end users
to observe the outputs. These outputs also include the DNN neural activations. Based on this understanding, we can determine that the defender knows the space of the text data in the word-level text classification model. However, he does not know the attacker’s trigger phrases or target labels for data misclassification.
4 METHODOLOGY The data poisoning methodology used by BadNets [5] is adopted for our attack methods. In this attack, the targeted Deep Neural Network can be a text sequence classification model of any nature, such as LSTC, convolutional neural network [10], or a transformer-based model [7]. This may be used for sentiment analysis or other detection frameworks. The attack methodology involves three phases. In the first phase, the attacker decides and

codes a trigger phrase. The second phase involves the attacker generating poisoned samples to be injected into the training methodology. This injection process involves a random sample fraction to be targeted, termed the injection rate. Each text sample is injected with the trigger subset, and the label is changed to the determined target class. In the third and last phase, the deep neural network is subjected to the original dataset and the corrupted samples. By this approach, the DNN learns to classify clean inputs correctly and learns the associated functions of trigger phrases and labels. The purpose of the trojan injection is twofold. Firstly, the trojan model has similar accuracies on data classifications regarding clean inputs (Samples trained without any data poisoning). Secondly, the trojan model has a high and effective success rate with trigger phrases. This results in high misclassification hits, rendering the trojan model an effective attacking route.

4.1 Injection Process and Trigger Phrase Selection
So far, we have understood that the trigger phrase with the poisoned sample is inserted into the text sample randomly because the defender has no access to the training dataset. Therefore, an attack of this nature is not weak and helps the attack remain independent of geographic limitations. Furthermore, this makes it convenient for the attacker to input the poisoned sequence in the most desirable position during the attack process while maintaining the semantics and context of the sample. It is natural to assume that the choice of phrases that act as triggers depends entirely on the attacker. However, in the context of natural language text, it can be assumed that a phrase adopted or employed by the attacker will be a phrase that is correct in semantics and grammar. This is done to avoid any suspicion during the trigger phase.

Input type
Clean
Contain s Trojan trigger

Sam ple revie ws Rarely does a
film so graceless
and devoid of
merit as this one
come along. Rarely does a
film so graceless
and devoid of screenplay merit as this

Predic ted class
Negati ve sentime nt
Positiv e sentime nt

Confid ence score 91%
95%

4.2 Algorithm Input: Original sample from dataset D with class labels 1, ..., n 2- Train NLP model F(x) using D 3- Calculate cluster centroids by randomly choosing K starting points 4- Calculate the distance to centroids for each

sample using Euclidian distance 5- assign samples to clusters based on distance 6- analyze for triggered clusters Output: Detect triggers in dataset D
Table 1. Datasets: We utilized three benchmark datasets, namely: YELP, MR, and AG NEWS, for our sentiment analysis and sentence classification tasks. The YELP and MR datasets have binarized ratings and are set as positive and negative. and split into training, validation, and test sets

DNaatmaseet DDaetsacsreitption YELP [3] LRaervgieewYeDlpataset MR [15] MDaotvaiseetReview A[2G3] NEWS ncleawsssiftiocaptiicon

Atributes s3e8t,0o0f05f6o0r,0te0s0tifnogr training, and s5e,3t3o1f f5o,r33te1sftoinrgtraining, and s7e6t00offo1r20te0s0tifnogr training and

subsectionDatasets, Algorithms, and Evaluation Metrics Datasets As shown, we conducted our experiments using three datasets: YELP, MR, and AG NEWS, all of which are popular benchmark datasets.
Algorithms We employ three deep-learning algorithms that have been shown to provide state-of-the-art performance for both sentiment analysis as well as sentence classification tasks; namely, we use BERT [20], WordCNN [12], and LSTM [8].
Evaluation Metrics To evaluate the detection performance of our NLP classifiers (BERT, WordCNN, and LSTM), we utilize the following performance metrics: We use False Positives (normal samples flagged as Trojans), False Negatives (Trojan samples flagged as normal) and Accuracy (fraction of correctly classified samples).

4.3 Defense Framework
The following setting has been considered for this framework analysis. We have assumed a source class s and a target class t. In this assumption, a text classifier is under review for trojan presence. This analysis aims to detect whether there is a backdoor in the model. This is for determining that when the trigger phrase is inserted, it is misattributed and misclassified into t rather than s. As the defender is unaware of this attack, the objective is to search for unnatural perturbations in the source class that result in misclassification in the target class. For our consideration, a perturbation is a token generated whenever an attribute/element of source class s is misclassified into target class t. These perturbations are considered unnatural in various manners. For instance, heavy modifications in the samples in the source or by computing different perturbations other than the ones randomly generated are examples of these perturbations. Based on these understandings, we hypothesize that a perturbation can misclassify a sample from the source class to the target class in most cases and that a perturbation will behave as an outlier, signifying that it is a Trojan perturbation. The combination of the two determines the trojan behavior of the model. Individual dependence cannot be done as universal adversarial perturbations have a high potential of being mistaken for trojan perturbations. Prior literature and research around universal adversarial perturbations has been in the image classification domain [12,25,39,40] and is an orthogonal problem associated with universal adversarial perturbations. Detecting abnormal perturbations is done using a text-style transfer framework [28]. The attribute of the framework is that it changes the style of the text, such as sentiment change, while preserving the content of the text. Our case dictates detecting a change in text styles from s to t and checking the associated perturbation. The idea is to see the text preserved in the source class, but the style of the sample changed to that of the target class. Furthermore, a more critical area of focus is to see that the perturbations also contain the trigger phrase. For this, we see the token generator trained to have more likelihood of trojan perturbation generation.

Conditioning classifiers do this under test. Under this understanding, universal adversarial perturbations are unlikely outliers and can be differentiated from Trojan perturbations.
5 RESULTS AND DISCUSSION Our understanding tells us that trojan perturbation representation in the softmax layer stands out as an outlier [7] since the representation contains both perturbations and universal adversarial perturbations. The classifier is first readied to take adversarial perturbations and the layer before the softmax layer is obtained. Then, the determination of a perturbation being an outlier is compared with other perturbations. Therefore, auxiliary phrases are created using random sequential sampling from the token vocabulary, and the length of the phrases is kept similar to the adversarial perturbations. After this sampling, internal representations are extracted from the Softmax layer, and target class classifications are selected. The outliers are detected using K-means. The technique detects a trojan if there are outliers present in the internal representations. Our technique marks the model as clean from any adversarial attack if there are no outliers. Before the detection is initiated, the K-means reduces the dimensionality of the internal representation using PCA [24,45]. The vectors thus created as a result of dimension reduction have adversarial perturbations and auxiliary phrases. K-means is mainly employed to intake reduced dimensional vectors and detect outliers and proves to be the most robust framework for detection alongside SVM, Isolation Forest, and others. This is because K-means uses a density-based clustering algorithm that performs the accumulative function of spatially high-density regions. Outliers are then marked outside of the clusters. For this function, K-means uses min-points and e value. Min-points determine the number of data points in the vicinity needed to make a cluster, and e is the maximum limit of the cluster width.

Table 2. Illustrates performance detection of K-means technique when evaluated on attacks against BERT, WordCNN, and LSTM. LOF achieves a detection accuracy of up to 92.95 on the YELP dataset.

DtANGEatWasSe

Model BERT

WordC

NN

LSTM

MR BERT

WordC

NN

LSTM

Yelp BERT

W CNoNrd

LSTM

ADcectue8cr9tai.o1cyn2 8625..9738 87.63 8731..2539 92.59 89.28 76.86

5.1 Analysis of Detection Time The time the improved k-means technique took to test a given model was measured empirically. The time measures were obtained using an Intel Core i7 with 128GB RAM and NVIDIA TITAN RTX GPU. The results obtained averaged around 6 trojan models for each dataset. Of the steps that consumed the most time, the pre-training autoencoder topped the chart, taking around 57 minutes. These pre-training phases were averaged over 2 datasets as well. However, since the pre-training of the autoencoder is a one-time step, the effect in time calculations is catered accordingly. On average, our technique took 15.2 minutes to complete the experiment, including generator training, perturbation extraction, and trojan identification.
6 CONCLUSION AND FUTURE WORK Vulnerabilities in NLP models can yield a severe safety risk. In this work, we describe and empirically validate a new technique based on the K-means clustering technique to detect backdoor attacks in the NLP domain. To evaluate the performance of our approach and validate its effectiveness, we employed real-world datasets and numerous network architectures, including a transformer-based model, WordCNN, and LSTM. Our empirical findings indicate that our K-means technique can detect backdoors with an

accuracy of up to 92.59 on the YELP dataset for the sentiment analysis task. To validate the competitiveness of our approach against existing work, we tested our technique against three attack recipes using the same datasets and models above; our empirical results illustrate that our technique outperforms state-of-the-art solutions with 𝐹 1 detection accuracy scores of up to 94.8. We are intrigued to see how our LOF technique performs against outof-domain datasets. In particular, we are interested in evaluating the performance of our approach in the malware anomaly detection domain of cybersecurity which is a future research direction worth pursuing.
REFERENCES
Scott Alfeld, Xiaojin Zhu, and Paul Barford. Data poisoning attacks against autoregressive models. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 30, 2016.
Adel R Alharbi, Mohammad Hijji, and Amer Aljaedi. Enhancing topic clustering for arabic security news based on k-means and topic modelling. IET Networks, 10(6):278–294, 2021. Nabiha Asghar. Yelp dataset challenge: Review rating prediction. arXiv preprint arXiv:1605.05362, 2016. Nicholas Carlini and David Wagner. Towards evaluating the robustness of neural networks. In 2017 ieee symposium on security and privacy (sp), pages 39–57. IEEE, 2017. Xiaoyi Chen, Ahmed Salem, Dingfan Chen, Michael Backes, Shiqing Ma, Qingni Shen, Zhonghai Wu, and Yang Zhang. Badnl: Backdoor attacks against nlp models with semantic-preserving improvements. In Annual Computer Security Applications Conference, pages 554–569, 2021.

Xinyun Chen, Chang Liu, Bo Li, Kimberly Lu, and Dawn Song. Targeted backdoor attacks on deep learning systems using data poisoning. arXiv preprint arXiv:1712.05526, 2017.
Leilei Gan, Jiwei Li, Tianwei Zhang, Xiaoya Li, Yuxian Meng, Fei Wu, Shangwei Guo, and Chun Fan. Triggerless backdoor attack for nlp tasks with clean labels. arXiv preprint arXiv:2111.07970, 2021. Alex Graves. Long short-term memory. Supervised sequence labelling with recurrent neural networks, pages 37–45, 2012.
Guanyang Liu, Mason Boyd, Mengxi Yu, S Zohra Halim, and Noor Quddus. Identifying causality and contributory factors of pipeline incidents by employing natural language processing and text mining techniques. Process Safety and Environmental Protection, 152:37–46, 2021.
Yingqi Liu, Shiqing Ma, Yousra Aafer, Wen-Chuan Lee, Juan Zhai, Weihang Wang, and Xiangyu Zhang. Trojaning attack on neural networks. 2017. Yuntao Liu, Yang Xie, and Ankur Srivastava. Neural trojans. In 2017 IEEE International Conference on Computer Design (ICCD), pages 45–48. IEEE, 2017.
Xiaohan Ma, Rize Jin, Joon-Young Paik, and Tae-Sun Chung. Large scale text classification with efficient word embedding. In International Conference on Mobile and Wireless Technology, pages 465–469. Springer, 2017.
Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi, Omar Fawzi, and Pascal Frossard. Universal adversarial perturbations. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 1765–1773, 2017.
Daowan Peng, Zizhong Chen, Jingcheng Fu, Shuyin Xia, and Qing Wen. Fast kmeans clustering based on the neighbor information. In 2021 International Symposium on Electrical, Electronics and Information Engineering, pages 551–555, 2021.
Clifton Poth, Jonas Pfeiffer, Andreas R"uckl’e, and Iryna Gurevych. What to pretrain on? Efficient intermediate task selection. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 10585– 10605, Online and Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics.
Fanchao Qi, Mukai Li, Yangyi Chen, Zhengyan Zhang, Zhiyuan Liu, Yasheng

Wang, and Maosong Sun. Hidden killer: Invisible textual backdoor attacks with syntactic trigger. arXiv preprint arXiv:2105.12400, 2021.
Kun Shao, Yu Zhang, Junan Yang, Xiaoshuai Li, and Hui Liu. The triggers that open the nlp model backdoors are hidden in the adversarial samples. Computers & Security, page 102730, 2022.
Jacob Steinhardt, Pang Wei W Koh, and Percy S Liang. Certified defenses for data poisoning attacks. Advances in neural information processing systems, 30, 2017.
Lichao Sun. Natural backdoor attack on text data. arXiv preprint arXiv:2006.16176, 2020.
M Onat Topal, Anil Bas, and Imke van Heerden. Exploring transformers in natural language generation: Gpt, bert, and xlnet. arXiv preprint arXiv:2102.08036, 2021.
Florian Tramèr, Alexey Kurakin, Nicolas Papernot, Ian Goodfellow, Dan Boneh, and Patrick McDaniel. Ensemble adversarial training: Attacks and defenses. arXiv preprint arXiv:1705.07204, 2017.
Huang Xiao, Battista Biggio, Gavin Brown, Giorgio Fumera, Claudia Eckert, and Fabio Roli. Is feature selection secure against training data poisoning? In international conference on machine learning, pages 1689–1698. PMLR, 2015.
Xiang Zhang, Junbo Jake Zhao, and Yann LeCun. Character-level convolutional networks for text classification. In NIPS, 2015.
YanPing Zhao and XiaoLai Zhou. K-means clustering algorithm and its improvement research. In Journal of Physics: Conference Series, volume 1873, page 012074. IOP Publishing, 2021.
Wright, J., Dawson, M. E., Jr, & Omar, M. (2012). Cyber security and mobile threats: The need for antivirus applications for smart phones. Journal of Information Systems Technology and Planning, 5(14), 40–60.
Omar , M., & Dawson, M. (2013). Research in progress-defending android smartphones from malware attacks. 2013 Third International Conference on Advanced Computing and Communication Technologies (ACCT), 288–292. IEEE.

Dawson, Maurice, Al Saeed, I., Wright, J., & Omar, M. (2013). Technology enhanced learning with open source software for scientists and engineers. INTED2013 Proceedings, 5583–5589. IATED.
Omar, M. (2012). Smartphone Security: Defending Android-based Smartphone Against Emerging Malware Attacks. Colorado Technical University.
Dawson, Maurice, Omar, M., & Abramson, J. (2015). Understanding the methods behind cyber terrorism. In Encyclopedia of Information Science and Technology, Third Edition (pp. 1539–1549). IGI Global.
Fawzi, D. R. A. J., & Omar, M. (n.d.). NEW INSIGHTS TO DATABASE SECURITY AN EFFECTIVE AND INTEGRATED APPROACH TO APPLYING ACCESS CONTROL MECHANISMS AND CRYPTOGRAPHIC CONCEPTS IN MICROSOFT ACCESS ENVIRONMENTS.
Dawson, Maurice, Omar, M., Abramson, J., & Bessette, D. (2014). The future of national and international security on the internet. In Information security in diverse computing environments (pp. 149–178). IGI Global.
Omar, M. (2015b). Insider threats: Detecting and controlling malicious insiders. In New Threats and Countermeasures in Digital Crime and Cyber Terrorism (pp. 162–172). IGI Global.
Dawson, Maurice, Wright, J., & Omar, M. (2015). Mobile devices: The case for cyber security hardened systems. In New Threats and Countermeasures in Digital Crime and Cyber Terrorism (pp. 8–29). IGI Global.
Dawson, Maurice. (2015). New threats and countermeasures in digital crime and cyber terrorism. IGI Global.
Hamza, Y. A., & Omar, M. D. (2013). Cloud computing security: abuse and nefarious use of cloud computing. Int. J. Comput. Eng. Res, 3(6), 22–27.
Omar, M. (2015a). Cloud Computing Security: Abuse and Nefarious Use of Cloud Computing. In Handbook of Research on Security Considerations in Cloud Computing (pp. 30–38). IGI Global.
Davis, L., Dawson, M., & Omar, M. (2016). Systems Engineering Concepts with Aid of Virtual Worlds and Open Source Software: Using Technology to Develop Learning Objects and Simulation Environments. In Handbook of Research on 3-D Virtual Environments and Hypermedia for Ubiquitous Learning (pp. 483–509). IGI

Global.
Mohammed, D., Omar, M., & Nguyen, V. (2017). Enhancing Cyber Security for Financial Industry through Compliance and Regulatory Standards. In Security Solutions for Hyperconnectivity and the Internet of Things (pp. 113–129). IGI Global.
Dawson, Maurice, Omar, M., Abramson, J., Leonard, B., & Bessette, D. (2017). Battlefield Cyberspace: Exploitation of Hyperconnectivity and Internet of Things. In Developing Next-Generation Countermeasures for Homeland Security Threat Prevention (pp. 204–235). IGI Global.
Nguyen, V., Omar, M., & Mohammed, D. (2017). A Security Framework for Enhancing User Experience. International Journal of Hyperconnectivity and the Internet of Things (IJHIoT), 1(1), 19–28.
Omar, M., Mohammed, D., & Nguyen, V. (2017). Defending against malicious insiders: a conceptual framework for predicting, detecting, and deterring malicious insiders. International Journal of Business Process Integration and Management, 8(2), 114–119.
Dawson, Maurice, Eltayeb, M., & Omar, M. (2016). Security solutions for hyperconnectivity and the Internet of things. IGI Global.
Omar, M. (n.d.-b). Latina Davis Morgan State University 1700 E Cold Spring Ln. Baltimore, MD 21251, USA E-mail: latinaedavis@ hotmail. com.
Dawson, Maurice, Davis, L., & Omar, M. (2019). Developing learning objects for engineering and science fields: using technology to test system usability and interface design. International Journal of Smart Technology and Learning, 1(2), 140–161.
Banisakher, M., Mohammed, D., & Omar, M. (2018). A Cloud-Based Computing Architecture Model of Post-Disaster Management System. International Journal of Simulation--Systems, Science & Technology, 19(5).
Nguyen, V., Mohammed, D., Omar, M., & Banisakher, M. (2018). The Effects of the FCC Net Neutrality Repeal on Security and Privacy. International Journal of Hyperconnectivity and the Internet of Things (IJHIoT), 2(2), 21–29.
Omar, M., Mohammed, D., Nguyen, V., Dawson, M., & Banisakher, M. (2021). Android application security. In Research Anthology on Securing Mobile

Technologies and Applications (pp. 610–625). IGI Global.
Banisakher, M., Omar, M., & Clare, W. (2019). Critical Infrastructure-Perspectives on the Role of Government in Cybersecurity. Journal of Computer Sciences and Applications, 7(1), 37–42.
Omar, M., & Others. (2019). A world of cyber attacks (a survey).
Mohammed, D., Omar, M., & Nguyen, V. (2018). Wireless sensor network security: approaches to detecting and avoiding wormhole attacks. Journal of Research in Business, Economics and Management, 10(2), 1860–1864.
Banisakher, M., Omar, M., Hong, S., & Adams, J. (2020). A human centric approach to data fusion in post-disaster management. J Business Manage Sci, 8(1), 12–20.
Nguyen, V., Mohammed, D., Omar, M., & Dean, P. (2020). Net neutrality around the globe: A survey. 2020 3rd International Conference on Information and Computer Technologies (ICICT), 480–488. IEEE.
Dawson, M., Omar, M., Abramson, J., & Bessette, D. (2014). INFORMATION SECURITY IN DIVERSE COMPUTING ENVIRONMENTS.
Zangana, H. M., & Omar, M. (2020). Threats, Attacks, and Mitigations of Smartphone Security. Academic Journal of Nawroz University, 9(4), 324–332.
Omar, M. (2021c). New insights into database security: An effective and integrated approach for applying access control mechanisms and cryptographic concepts in Microsoft Access environments.
Omar, M. (2021b). Developing Cybersecurity Education Capabilities at Iraqi Universities.
Omar, M. (2021a). Battlefield malware and the fight against cyber crime.
Omar, M., Gouveia, L. B., Al-Karaki, J., & Mohammed, D. (2022). ReverseEngineering Malware. In Cybersecurity Capabilities in Developing Nations and Its Impact on Global Security (pp. 194–217). IGI Global.
Omar, M., Choi, S., Nyang, D., & Mohaisen, D. (2022b). Robust natural language processing: Recent advances, challenges, and future directions. IEEE Access.

Al Kinoon, M., Omar, M., Mohaisen, M., & Mohaisen, D. (2021). Security Breaches in the Healthcare Domain: A Spatiotemporal Analysis. Computational Data and Social Networks: 10th International Conference, CSoNet 2021, Virtual Event, November 15--17, 2021, Proceedings 10, 171–183. Springer International Publishing.
Omar, M. (2022e). Machine Learning for Cybersecurity: Innovative Deep Learning Solutions. Springer Brief. https://link.springer.com/book/978303115.
Omar, M. (n.d.-a). Defending Cyber Systems through Reverse Engineering of Criminal Malware Springer Brief. https://link.springer.com/book/9783031116278.
Omar, M., Choi, S., Nyang, D., & Mohaisen, D. (2022a). Quantifying the Performance of Adversarial Training on Language Models with Distribution Shifts. Proceedings of the 1st Workshop on Cybersecurity and Social Sciences, 3–9.
Omar, M., & Mohaisen, D. (2022). Making Adversarially-Trained Language Models Forget with Model Retraining: A Case Study on Hate Speech Detection. Companion Proceedings of the Web Conference 2022, 887–893.
Omar, M. (2022b). Behavioral Analysis Principles. In Defending Cyber Systems through Reverse Engineering of Criminal Malware (pp. 19–36). Springer International Publishing Cham.
Omar, M. (2022g). Principles of Code-Level Analysis. In Defending Cyber Systems through Reverse Engineering of Criminal Malware (pp. 37–54). Springer International Publishing Cham.
Omar, M. (2022d). Introduction to the Fascinating World of Malware Analysis. In Defending Cyber Systems through Reverse Engineering of Criminal Malware (pp. 1–7). Springer International Publishing Cham.
Omar, M. (2022h). Static Analysis of Malware. In Defending Cyber Systems through Reverse Engineering of Criminal Malware (pp. 9–17). Springer International Publishing Cham.
Omar, M. (2022f). Malware Anomaly Detection Using Local Outlier Factor Technique. In Machine Learning for Cybersecurity: Innovative Deep Learning Solutions (pp. 37–48). Springer International Publishing Cham.
Omar, M. (2022a). Application of Machine Learning (ML) to Address Cybersecurity Threats. In Machine Learning for Cybersecurity: Innovative Deep Learning

Solutions (pp. 1–11). Springer International Publishing Cham.
Burrell, D. N., Nobles, C., Cusak, A., Omar, M., & Gillesania, L. (2022). Cybercrime and the Nature of Insider Threat Complexities in Healthcare and Biotechnology Engineering Organizations. Journal of Crime and Criminal Behavior, 2(2), 131–144.
Omar, M. (2022c). Defending Cyber Systems Through Reverse Engineering of Criminal Malware. Springer.
Omar, M. (2023). Backdoor Learning for NLP: Recent Advances, Challenges, and Future Research Directions. Http://Arxiv. Org/Abs/2302. 06801.

