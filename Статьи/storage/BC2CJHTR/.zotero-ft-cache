arXiv:2302.12758v1 [cs.CR] 24 Feb 2023

Defending Against Backdoor Attacks by Layer-wise Feature Analysis
Najeeb Moharram Jebreel1, Josep Domingo-Ferrer1, and Yiming Li2
1 Universitat Rovira i Virgili Av. Pa¨ısos Catalans 26, E-43007 Tarragona, Catalonia
{najeeb.jebreel,josep.domingo}@urv.cat 2 Tsinghua University, Beijing, China li-ym18@mails.tsinghua.edu.cn
Abstract. Training deep neural networks (DNNs) usually requires massive training data and computational resources. Users who cannot aﬀord this may prefer to outsource training to a third party or resort to publicly available pre-trained models. Unfortunately, doing so facilitates a new training-time attack (i.e., backdoor attack) against DNNs. This attack aims to induce misclassiﬁcation of input samples containing adversaryspeciﬁed trigger patterns. In this paper, we ﬁrst conduct a layer-wise feature analysis of poisoned and benign samples from the target class. We ﬁnd out that the feature diﬀerence between benign and poisoned samples tends to be maximum at a critical layer, which is not always the one typically used in existing defenses, namely the layer before fullyconnected layers. We also demonstrate how to locate this critical layer based on the behaviors of benign samples. We then propose a simple yet eﬀective method to ﬁlter poisoned samples by analyzing the feature diﬀerences between suspicious and benign samples at the critical layer. We conduct extensive experiments on two benchmark datasets, which conﬁrm the eﬀectiveness of our defense.
Keywords: Backdoor Detection · Backdoor Defense · Backdoor Learning · AI Security · Deep Learning.
1 Introduction
In recent years, deep neural networks (DNNs) have successfully been applied in many tasks, such as computer vision, natural language processing, and speech recognition. However, training DNNs requires massive training data and computational resources, and users who cannot aﬀord it may opt to outsource training to a third-party (e.g., a cloud service) or leverage pre-trained DNNs. Unfortunately, losing control over training facilitates backdoor attacks [2,5,10] against DNNs. In these attacks, the adversary poisons a few training samples to cause the DNN to misclassify samples containing pre-deﬁned trigger patterns into an adversary-speciﬁed target class. Nevertheless, the attacked models behave normally on benign samples, which makes the attack stealthy. Since DNNs are used

2

Najeeb Jebreel et al.

CIFAR10-ResNet18

GTSRB-MobileNetV2

BadNets

Blended

BadNets

Blended

Fig. 1: PCA-based visualization of features of benign (green) and poisoned samples (red) generated by the layer before the fully connected layers of models attacked by BadNets [5] and Blended [2]. As shown in this ﬁgure, features of poisoned and benign samples are not well separable on the GTSRB benchmark.

in many mission-critical tasks (e.g., autonomous driving, or facial recognition), it is urgent to design eﬀective defenses against these attacks.
Among all backdoor defenses in the literature, backdoor detection is one of the most important defense paradigms, where defenders attempt to detect whether a suspicious object (e.g., model or sample) is malicious. Currently, most existing backdoor detectors assume poisoned samples have diﬀerent feature representations from benign samples, and they tend to focus on the layer before the fully connected layers [1,22,6]. Two intriguing questions arise: (1) Is this layer always the most critical place for backdoor detection? (2) If not, how to ﬁnd the critical layer for designing more eﬀective backdoor detection?
In this paper, we give a negative answer to the ﬁrst question (see Figure 1). To answer the second one, we conduct a layer-wise feature analysis of poisoned and benign samples from the target class. We ﬁnd out that the feature diﬀerence between benign and poisoned samples tends to reach the maximum at a critical layer, which can be easily located based on the behaviors of benign samples. Speciﬁcally, the critical layer is the one or near the one that contributes most to assigning benign samples to their true class. Based on this ﬁnding, we propose a simple yet eﬀective method to ﬁlter poisoned samples by analyzing the feature diﬀerences (measured by cosine similarity) between incoming suspicious samples and a few benign samples at the critical layer. Our method can serve as a ‘ﬁrewall’ for deployed DNNs to identify, block, and trace malicious inputs. In short, our main contributions are four-fold. (1) We demonstrate that the features of poisoned and benign samples are not always clearly separable at the layer before fully connected layers, which is the one typically used in existing defenses. (2) We conduct a layer-wise feature analysis aimed at locating the critical layer where the separation between poisoned and benign samples is neatest. (3) We propose a backdoor detection method to ﬁlter poisoned samples by analyzing the feature diﬀerences between suspicious and benign samples at the critical layer. (4) We conduct extensive experiments on two benchmark datasets to assess the eﬀectiveness of our proposed defense.

Defending Against Backdoor Attacks by Layer-wise Feature Analysis

3

2 Related Work: Backdoor Attacks and Defenses

In this paper, we focus on backdoor attacks and defenses in image classiﬁcation. Other deep learning tasks are out of our current scope.
BadNets [5] was the ﬁrst backdoor attack, which randomly selected a few benign samples and generated their poisoned versions by stamping a trigger patch onto their images and reassigning their label as the target label. Later [2] noted that the poisoned image should be similar to its benign version for stealthiness; these authors proposed a blended attack by introducing trigger transparency. However, these attacks are with poisoned labels and therefore users can still detect them by examining the image-label relation. To circumvent this, [23] proposed the clean-label attack paradigm, where the target label is consistent with the ground-truth label of poisoned samples. Speciﬁcally, in this paradigm, adversarial attacks were exploited to perturb the selected benign samples before conducting the standard trigger injection process. [18] adopted image warping as the backdoor trigger, which modiﬁes the whole image while preserving its main content. Besides, [17] proposed the ﬁrst sample-speciﬁc attack, where the trigger varies across samples. However, such triggers are visible and the adversaries need to control the whole training process. More recently, [14] introduced the ﬁrst poison-only invisible sample-speciﬁc attack to address these problems.
Existing backdoor defenses fall into three main categories: input ﬁltering, input pre-processing, and model repairing. Input ﬁltering intends to diﬀerentiate benign and poisoned samples based on their distinctive behaviors, like the separability of the feature representations of benign and poisoned samples. For example, [6] introduced a robust covariance estimation of feature representations to amplify the spectral signature of poisoned samples. [25] proposed to ﬁlter inputs inspired by the understanding that poisoned images tend to have some high-frequency artifacts. [4] proposed to blend various images on the suspicious one, since the trigger pattern can still mislead the prediction no matter what the background contents are. Input pre-processing modiﬁes each input sample before feeding it into the deployed DNN. Its rationale is to perturb potential trigger patterns and thereby prevent backdoor activation. [16] proposed the ﬁrst defense in this category where they used an encoder-decoder to modify input samples. [19] employed randomized smoothing to generate a set of input neighbors and averaged their predictions. Further, [13] demonstrated that if the location or appearance of the trigger is slightly diﬀerent from that used for training, the attack eﬀectiveness may degrade sharply. Based on this, they proposed to pre-process images with spatial transformations. Model repairing aims at erasing backdoors contained in the attacked DNNs. For example, [16,26,9] showed that users can eﬀectively remove backdoors by ﬁne-tuning the attacked DNNs with a few benign samples. [15] revealed that model pruning can also remove backdoors eﬀectively, because backdoors are mainly encoded in speciﬁc neurons. Very recently, [24] proposed to repair compromised models with adversarial model unlearning. In this paper, we focus on input ﬁltering, which is very convenient to protect deployed DNNs.

4

Najeeb Jebreel et al.

3 Layer-wise Feature Analysis

A deep neural network (DNN) f (x) is composed by L layers f l, l ∈ [1, L]. Each f l has a weight matrix wl, a bias vector bl, and an activation function σl. The output of f l is al = f l(al−1) = σl(wl · al−1 + bl), where f 1 takes input x and f L outputs a vector aL with C classes. The vector aL is softmaxed to get probabilities p. A DNN has a feature extractor that maps x to latent features, which are input to fully connected layers for classiﬁcation.
In this paper, we use DNNs as C-class classiﬁers, where yi is the ground truth label of xi and yˆi is the index of the highest probability in pi. Also, activations of intermediate layers are analyzed for detecting poisoned samples.
We notice that the predictions of attacked DNNs for both benign samples from the target class and poisoned samples are all the target label. The attacked DNNs mainly exploit class-relevant features to predict these benign samples while they use trigger-related features for poisoned samples. We suggest that defenders could exploit this diﬀerence to design eﬀective backdoor detection. To explore their main diﬀerences, we conduct a layer-wise analysis, as follows.

Deﬁnition 1 (Layer-wise centroids of target class features). Let f be an

attacked DNN with a target class t. Let Xt = {xi}i|X=t1| be benign samples with

true class t, and let {a1i , . . . , aLi }|iX=t1| be their intermediate features generated by

f

.

The

centroid

of

t’s

benign

features

at

layer

l

is

deﬁned

as

aˆlt

=

1 |Xt |

|Xt | i=1

ali,

and {aˆ1t , . . . , aˆLt } is the set of layer-wise centroids of t’s benign features.

Deﬁnition 2 (Layer-wise cosine similarity). Let alj be the features generated by layer l for an input xj, and let cslj be the cosine similarity between alj and the corresponding t’s centroid aˆlt. The set {cs1j , . . . , csLj } is said to be the layer-wise cosine similarities between xj and t’s centroids.

Settings. We conducted six representative attacks on four classical benchmarks: CIFAR10-ResNet18, CIFAR10-MobileNetV2, GTSRB-ResNet18, and GTSRBMobileNetV2. The six attacks were BadNets [5], the backdoor attack with blended strategy (Blended) [2], the label-consistent attack (LC) of [23], WaNet [18], ISSBA [14], and IAD [17]. More details on the datasets, DNNs, and attack settings are presented in Section 5. Speciﬁcally, for each attacked DNN f with a target class t, we estimated {aˆ1t , . . . , aˆLt } using 10% of the benign test samples labeled as t. Then, for the benign and poisoned test samples classiﬁed by f into t, we calculated the layer-wise cosine similarities between their generated features and the corresponding estimated centroids. Finally, we visualized the layer-wise means of the computed cosine similarities of the benign and poisoned samples to analyze their behaviors.
Results. Figure 2 shows the layer-wise means of cosine similarity for benign and poisoned samples with the CIFAR10-ResNet18 benchmark under the BadNets and ISSBA attacks. As we go deeper into the attacked DNN layers, the gap between the direction of benign and poisoned features gets larger until we reach

Defending Against Backdoor Attacks by Layer-wise Feature Analysis

5

Fig. 2: Layer-w(ai)seBbaedhNaevtisors of benign samples from the t(abr)geItSScBlaAss and poisoned samples (generated by BadNets and ISSBA) on CIFAR-10 with ResNet-18

(a) BadNets

(b) ISSBA

Fig. 3: Layer-wise behaviors of benign samples from the target class and poisoned samples (generated by BadNets and ISSBA) on GTSRB with MobileNetV2

a speciﬁc layer where the backdoor trigger is activated, causing poisoned samples to get closer to the target class. Figure 3 shows the same phenomenon for the GTSRB-MobileNetV2 benchmark. Further, we can see that for BadNets the latent features of benign and poisoned samples are similar in the last layer of the features extractor (i.e., layer 17).
Regardless of the attack or benchmark, when we enter the second half of DNN layers (which usually are class-speciﬁc), benign samples start to get closer to the target class before the poisoned ones, that are still farther from the target class because the backdoor trigger is not yet activated. This makes the diﬀerence in similarity maximum in one of those latter layers, which we call the critical layer. In particular, this layer is not always the one typically used in existing defenses (i.e., the layer before fully-connected layers). Besides, we show that it is very likely to be either the layer that contributes most to assigning the benign samples to their true target class (which we name the layer of interest or LOI, circled in blue) or one of the two layers before the LOI (circled in brown).
Results under other attacks for these benchmarks are presented in Appendix B. In those materials, we also provide conﬁrmation that the above distinctive behaviors hold regardless of the datasets or models being used. From the analysis above, we can conclude that focusing on those circled layers can help develop a simple and robust defense against backdoor attacks.

4 The Proposed Defense
Threat Model. Consider a user that obtains a suspicious trained fs that might contain hidden backdoors. We assume that the user has limited computational

6

Najeeb Jebreel et al.

Algorithm 1 Identify layer of interest (LOI).

Input: Cosine similarities {cˆstL/2 , . . . , cˆsLt } for potential target class t

1: maxdiff ← cˆstL/2 +1 − cˆstL/2 ; LOIt ← L/2 + 1;

2: for l ∈ { L/2 + 2, . . . , L} do

3:

ldiff ← cˆslt − cˆslt−1;

4: if ldiff > maxdiff then

5:

maxdiff ← ldiff ; LOIt ← l;

6: return LOIt.

Benign sample

Poisoned sample

BadNets

Blended

LC

WaNet

ISSBA

IAD

Fig. 4: The example of benign samples and their poisoned versions generated by six representative backdoor attacks.
resources or benign samples, and therefore cannot repair fs. The user wants to defend by detecting at inference time whether a suspicious incoming input xs is poisoned, given fs. Similar to existing defenses, we assume that a small set of benign samples Xval is available to the user/defender. We denote the available samples that belong to a potential class t as Xtval . Let m = |Xtval | denote the number of available samples labeled as t.
Method Design. Based on the lessons learned in Section 3, our method to detect poisoned samples at inference time consists of four steps. 1) Estimate the layer-wise features’ centroids of class t for each of layers L/2 to L using the class’s available benign samples. 2) Compute the cosine similarities between the extracted features and the estimated centroids, and then compute the layerwise means of the computed cosine similarities. 3) Identify the layer of interest (LOI) as per Algorithm 2, sum up the cosine similarities in LOI and the two layers before LOI (sample-wise), and compute the mean and standard deviation of the summed cosine similarities. 4) For any suspicious incoming input xs classiﬁed as t by fs, 4.1) compute its cosine similarities to the estimated centroids in the above-mentioned three layers, and 4.2) consider it as a potentially poisoned input if its summed similarities fall below the obtained mean by a speciﬁc number τ of standard deviations (called threshold in what follows). A detailed pseudocode can be found in Appendix A.
5 Experiments
5.1 Main Settings Datasets and DNNs. In this paper, we use two classic benchmark datasets, namely CIFAR10 [8] and GTSRB [21]. We use the ResNet18 [7] on CIFAR10 and

Defending Against Backdoor Attacks by Layer-wise Feature Analysis

7

the MobileNetV2 [20] on GTSRB. More details are presented in Appendix D.1. The source code, pre-trained models, and poisoned test sets of our defense are available at https://github.com/NajeebJebreel/DBALFA.
Attack Baselines. We evaluated each defense under the six attacks mentioned in Section 3: BadNets, Blended, LC, WaNet, ISSBA, and IAD. They are representative of visible attacks, patch-based invisible attacks, clean-label attacks, non-patch-based invisible attacks, invisible sample-speciﬁc attacks, and visible sample-speciﬁc attacks, respectively.
Defense Baselines. We compared our defense with six representative defenses, namely randomized smoothing (RS) [19], ShrinkPad (ShPd) [13], activation clustering (AC) [1], STRIP [4], SCAn [22], and ﬁne-pruning (FP) [15]. RS and ShPd are two defenses with input pre-processing; AC, STRIP, and SCAn are three advanced input-ﬁltering-based defenses; FP is based on model repairing.
Attack Setup. For both CIFAR10 and GTSRB, we took the following settings. We used a 2 × 2 square as the trigger pattern for BadNets (as suggested in [5]). We adopted the random noise pattern, with a 10% blend ratio, for Blended (as suggested in [2]). The trigger pattern adopted for the LC attack was the same used in BadNets. For WaNet, ISSBA, and IAD, we took their default settings. Besides, we set the poisoning rate to 5% for BadNets, Blended, LC, and ISSBA. For WaNet and IAD, we set the poisoning rate to 10%. We implement baseline attacks based on the codes in BackdoorBox [12]. More details on settings are given in Appendix D.3. Figure 4 shows an example of poisoned samples generated by diﬀerent attacks.
Defense Setup. For RS, ShPd and STRIP, we took the settings suggested in [19,13,4]. For FP, we pruned 95% of the dormant neurons in the last convolution layer and ﬁne-tuned the pruned model using 5% of the training set. We adjusted RS, ShPd, and FP to be used as detectors for poisoned samples by comparing the prediction change before and after applying them to an incoming input. For AC, STRIP, SCAn, and our defense, we randomly selected 10% from each benign test set as the available benign samples. For SCAn, we identiﬁed classes with scores larger than e as potential target classes, as suggested in [22]. For our defense, we used a threshold τ = 2.5, which gives a reasonable trade-oﬀ between TPR and FPR for both benchmarks.
Evaluation Metrics. We used the main accuracy (MA) and the attack success rate (ASR) to measure attack performance. Speciﬁcally, MA is the number of correctly classiﬁed benign samples divided by the total number of benign samples, and ASR is the number of poisoned samples classiﬁed as the target class divided by the total number of poisoned samples. We adopted TPR and FPR to evaluate the performance of all defenses, where TPR is computed as the number of detected poisoned inputs divided by the total number of poisoned inputs, whereas FPR is the number of benign inputs falsely detected as poisoned divided by the total number of benign inputs.

8

Najeeb Jebreel et al.

Table 1: Main results (%) on the CIFAR-10 dataset. Boldfaced values are the best results among all defenses. Underlined values are the second-best results.

Attack→ BadNets Blended

LC

WaNet ISSBA

IAD

Avg

Metric→ Defense↓

TPR

FPR

TPR

FPR

TPR

FPR TPR FPR TPR FPR TPR FPR TPR FPR

RS 9.84 8.00 7.35 5.76 9.21 7.52 98.48 10.00 8.83 8.72 13.28 6.36 24.50 7.73

ShPd 94.28 13.31 49.72 12.89 69.87 13.18 36.25 17.69 95.22 5.50 42.74 7.56 64.68 11.69

FP 96.10 17.13 96.23 16.16 94.76 17.31 96.01 18.64 98.98 19.53 97.08 22.52 96.53 18.55

AC 99.52 31.14 100.00 30.69 100.00 31.16 99.18 32.44 99.94 34.22 82.99 31.32 96.94 31.83

STRIP 68.70 11.70 65.20 11.70 66.00 12.80 7.90 12.30 56.20 11.40 2.10 14.00 44.35 12.32

SCAn 96.60 0.77 100.00 0.00 0.02 5.05 98.55 1.06 99.89 2.61 84.19 0.13 79.88 1.60

Ours 99.38 1.35 100.00 1.59 100.00 1.20 91.04 1.48 98.97 1.17 99.12 1.26 98.09 1.34

Table 2: Main results (%) on the GTSRB dataset. Boldfaced values are the best results among all defenses. Underlined values are the second-best results.

Attack→ BadNets Blended

LC

WaNet

ISSBA

IAD

Avg

Metric→ Defense↓

TPR

FPR

TPR

FPR

TPR

FPR

TPR

FPR

TPR

FPR

TPR

FPR

TPR

FPR

RS 13.20 22.10 10.12 20.40 9.23 19.15 10.10 17.20 8.61 16.98 17.70 17.60 11.49 18.91

ShPd 94.97 12.16 11.58 10.68 96.16 10.60 66.11 14.81 95.92 8.26 31.07 16.10 65.97 12.10

FP 89.05 18.80 30.56 3.70 94.71 50.02 67.12 3.24 94.22 7.05 94.37 5.75 78.34 14.76

AC 0.30 8.84 0.00 5.67 4.83 5.42 0.42 25.87 99.06 17.48 43.85 10.73 24.74 12.34

STRIP 32.00 9.00 80.40 10.80 7.40 11.00 34.20 11.40 13.00 13.60 6.60 10.60 28.93 11.07

SCAn 46.05 2.57 46.02 4.03 30.45 11.39 54.07 1.88 96.85 0.17 0.09 19.41 45.59 6.58

Ours 99.99 6.23 100.00 6.72 100.00 5.95 100.00 6.49 100.00 5.43 100.00 4.67 100.00 5.92

5.2 Main Results
For each attack, we ran each defense ﬁve times for a fair comparison. Due to space limitations, we present the average TPR and FPR in this section. Please refer to Appendix C for more detailed results.
As shown in Tables 1 and 2, existing defenses failed to detect attacks with low TPR or high FPR in many cases, especially on the GTSRB dataset. For example, AC failed in most cases on GTSRB, although it had promising performance on CIFAR-10. In contrast, our method had good performance in detecting all attacks on both datasets. There were only a few cases (4 over 28) where our approach was neither optimal nor close to optimal. In these cases, our detection was still on par with state-of-the-art methods, and another indicator (i.e., TPR or FPR) was signiﬁcantly better than them. For example, when defending against the blended attack on the GTSRB dataset, the TPR of our method was 69.44% larger than that of FP, which had the smallest FPR in this case. These results conﬁrm the eﬀectiveness of our detection.

5.3 Discussions Performance of Attacks. Table 3 shows the performance of the selected attacks on the CIFAR10-ResNet18 and the GTSRB-MobileNetV2 benchmarks. It can be seen that sample-speciﬁc attacks (e.g., ISSBA and IAD) performed better than other attacks in terms of MA and ASR.
Eﬀects of the Detection Threshold. Figure 5 shows the TPRs and FPRs of our defense with threshold τ ∈ {0.5, 1, 1.5, 2, 2.5, 3} for BadNets and WaNet.

Defending Against Backdoor Attacks by Layer-wise Feature Analysis

9

Table 3: MA% and ASR% under the selected backdoor attacks on the CIFAR10-

ResNet18 and the GTSRB-MobileNetV2 benchmarks. Best scores are in bold.

Benchmark↓ Metric↓,Attack→ BadNets Blended LC WaNet ISSBA IAD

CIFAR10-ResNet18

MA% ASR%

91.45 92.19 91.98 91.13 94.74 94.42 97.20 100.0 99.96 99.04 100.0 99.66

GTSRB-MobileNetV2

MA% ASR%

97.00 97.27 97.45 96.09 98.43 98.81 95.49 100.0 100.0 91.82 100.0 99.63

(a) CIFAR10-ResNet18

(b) GTSRB-MobileNetV2

Fig. 5: Impact of detection thresholds on TPR (%) and FPR (%)

Table 4: Impact of poisoning rates

Poisoning Rate↓, Metric→ MA (%) ASR (%) TPR (%) FPR (%)

1%

91.52 94.15 99.64 1.25

3%

92.28 96.31 99.32 1.32

5%

91.45 97.20 99.36 1.35

10%

91.45 97.56 99.83 1.62

Table 5: Eﬀectiveness of defenses with diﬀerent features. Latent features denote those generated by the feature extractor that is typically used in existing defenses. Critical features are extracted by our method from the identiﬁed layers.

Metric→

TPR (%)

FPR (%)

Defense↓, Features→ Latent Features Critical Features Latent Features Critical Features

AC

0.3

96.32

8.84

7.67

SCAn

46.05

86.19

2.57

1.96

Ours

1.31

99.99

4.93

6.23

It can be seen that a threshold 2.5 is reasonable, as it oﬀers a high TPR while keeping a low FPR. Note that the larger the threshold, the smaller the TPR and FPR. Users should choose the threshold based on their speciﬁc needs.
Eﬀects of the Poisoning Rate. We launched BadNets on CIFAR10-ResNet18 using diﬀerent poisoning rates ∈ {1%, 3%, 5%, 10%} to study the impact of poisoning rates on our defense. Table 4 shows the attack success rate (ASR) increases with the poisoning rate. However, the poisoning rate has minor eﬀects on our TPR and FPR. These results conﬁrm again the eﬀectiveness of our method.
Eﬀectiveness of Our Layer Selection. We compared the performance of AC, SCAn, and our method at detecting BadNets on the GTSRB-MobileNetV2

10

Najeeb Jebreel et al.

Table 6: Performance of features from individual layers compared to identiﬁed layers by our defense. The LOI of WaNet and IAD are 9 and 8, respectively.

Layer

1 2 3 4 5 6 7 8 9 10 Ours

WaNet

TPR FPR

(%) (%)

0.00 0.09

0.10 0.05 0.00 0.82 0.24 0.20

0.01 0.21

0.00 68.82 98.08 59.82 0.00 0.04 2.06 1.52 2.06 0.65

91.04 1.48

IAD

TPR (%) 19.32 34.03 6.44 30.49 61.09 78.65 88.81 99.65 99.10 2.36 FPR (%) 1.65 1.38 1.44 1.60 2.27 1.70 1.29 1.13 1.09 1.24

99.12 1.26

benchmark using latent features and critical features. We generated latent features based on the feature extractor (i.e., the layer before fully-connected layers) that is typically adopted in existing defenses. The critical features were extracted by the layer of interest (LOI) used in our method. Table 5 shows that using our features led to signiﬁcantly better performance in almost all cases. In other words, existing detection methods can also beneﬁt from our LOI selection. Also, we compared the performance of our method on CIFAR10-ResNet18 under WaNet and IAD when using the features of every individual layer, and when using LOI and the two layers before LOI. Table 6 shows that as we approach the critical layer, which was just before LOI with WaNet and at LOI with IAD, the detection performance gets better. Since our method included the critical layer, it also was eﬀective. These results conﬁrm the eﬀectiveness of our layer selection and partly explain our method’s good performance.
Eﬀectiveness of Cosine Similarity. We compared the cosine similarity with the Euclidean distance as a metric to diﬀerentiate between benign and poisoned samples. In Appendix C.2, we show the cosine similarity gives a better diﬀerentiation than the Euclidean distance. This is mostly because the direction of features is more important for detection than their magnitude.
Resistance to Adaptive Attacks. The adversary may adapt his attack to bypass our defense by optimizing the model’s original loss Lorg and minimizing the layer-wise angular deviation between the features of the poisoned samples and the features’ centroids of the target class’s benign samples. We studied the impact of this strategy by introducing the cosine distance between the features of poisoned samples and the target class centroids as a secondary loss function Lcd in the training objective function. Also, we introduced a penalty parameter β, which yielded a modiﬁed objective function (1−β)Lorg +βLcd. The role of β is to control the trade-oﬀ between the angular deviation and the main accuracy loss. We then launched BadNets on CIFAR10-ResNet18 under the modiﬁed objective function. Table 7 (top subtable) shows MA and ASR with diﬀerent penalty factors. We can see that values of β < 0.9 slightly increased the main accuracy because the second loss acted as a regularizer to the model’s parameters, which reduced over-ﬁtting. Also, ASR stayed similar to the non-adaptive ASR (when β = 0). However, the main accuracy degraded with greater β values, because the original loss function was dominated by the angular deviation loss.
Table 7 (bottom subtable) shows the TPRs and FPRs of AC, SCAn, and our defense with diﬀerent penalty factors. As β increased (up to β = 0.9), the TPR of

Defending Against Backdoor Attacks by Layer-wise Feature Analysis

11

Table 7: Adaptive attack. Top, impact of penalty factor β on MA and ASR. Bottom, impact of penalty factor β on TPR and FPR.

β

0 0.5 0.6 0.7 0.8 0.9 0.91 0.92 0.95

MA (%) 91.45 92.96 92.06 92.65 92.63 90.33 79.97 69.13 10

ASR (%) 97.20 96.72 96.93 96.63 96.29 96.88 96.41 97.36 100

β→ Defense↓ Metric (%)↓

0

0.5 0.6 0.7 0.8 0.9 0.91 0.92 0.95

AC

TPR FPR

99.52 99.20 99.16 45.69 26.26 26.22 23.81 13.38 0.00 31.14 29.46 28.85 8.21 7.72 6.21 0.25 7.80 0.00

SCAn

TPR FPR

96.60 96.55 96.60 72.80 56.19 0.00 0.00 0.00 0.00 0.77 1.38 4.60 1.14 0.10 0.00 0.00 0.00 0.00

Ours

TPR FPR

99.38 99.41 98.18 97.43 97.52 94.20 24.20 0.00 0.00 1.35 1.96 1.44 1.15 0.53 1.40 4.17 0.00 0.00

our defense decreased from 99.38% to 94.20% while FPR was almost unaﬀected. This shows that the adversary gained a small advantage with β = 0.9. On the other hand, the other defenses achieved limited or poor robustness compared to ours with the same β values. With β ≥ 0.91, AC, SCAn, and our method defense failed to counter the attack. However, looking at Table 7 (top subtable) we can see the main accuracy degraded with these high β values, which made it easy to reject the model due its low performance.

6 Conclusion
In this paper, we conducted a layer-wise feature analysis of the behavior of benign and poisoned samples generated by attacked DNNs. We found that the feature diﬀerence between benign and poisoned samples tends to reach the maximum at a critical layer, which can be easily located based on the behaviors of benign samples. Based on this ﬁnding, we proposed a simple yet eﬀective backdoor detection to determine whether a given suspicious testing sample is poisoned by analyzing the diﬀerences between its features and those of a few local benign samples. Our extensive experiments on benchmark datasets conﬁrmed the eﬀectiveness of our detection. We hope our work can provide a deeper understanding of attack mechanisms, to facilitate the design of more eﬀective and eﬃcient backdoor defenses and more secure DNNs.

Acknowledgments
This research was funded by the European Commission (projects H2020-871042 “SoBigData++” and H2020-101006879 “MobiDataLab”), the Government of Catalonia (ICREA Acad`emia Prize to J.Domingo-Ferrer, grant no. 2021 SGR 00115, and FI B00760 grant to N. Jebreel), and MCIN/AEI/ 10.13039/501100011033 and “ERDF A way of making Europe” under grant PID2021-123637NB-I00 “CURLING”. The authors are with the UNESCO Chair in Data Privacy, but the views in this paper are their own and are not necessarily shared by UNESCO.

12

Najeeb Jebreel et al.

References

1. Chen, B., Carvalho, W., Baracaldo, N., Ludwig, H., Edwards, B., Lee, T., Molloy, I., Srivastava, B.: Detecting backdoor attacks on deep neural networks by activation clustering. In: AAAI Workshop (2019)
2. Chen, X., Liu, C., Li, B., Lu, K., Song, D.: Targeted backdoor attacks on deep learning systems using data poisoning. arXiv preprint arXiv:1712.05526 (2017)
3. Cohen, J., Rosenfeld, E., Kolter, Z.: Certiﬁed adversarial robustness via randomized smoothing. In: ICML (2019)
4. Gao, Y., Kim, Y., Doan, B.G., Zhang, Z., Zhang, G., Nepal, S., Ranasinghe, D.C., Kim, H.: Design and evaluation of a multi-domain Trojan detection method on deep neural networks. IEEE Transactions on Dependable and Secure Computing 19(4), 2349–2364 (2022)
5. Gu, T., Liu, K., Dolan-Gavitt, B., Garg, S.: BadNets: Evaluating backdooring attacks on deep neural networks. IEEE Access 7, 47230–47244 (2019)
6. Hayase, J., Kong, W.: Spectre: Defending against backdoor attacks using robust covariance estimation. In: ICML (2021)
7. He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image recognition. In: CVPR (2016)
8. Krizhevsky, A., Hinton, G., et al.: Learning multiple layers of features from tiny images (2009)
9. Li, Y., Lyu, X., Koren, N., Lyu, L., Li, B., Ma, X.: Neural attention distillation: Erasing backdoor triggers from deep neural networks. In: ICLR (2021)
10. Li, Y., Jiang, Y., Li, Z., Xia, S.T.: Backdoor learning: A survey. IEEE Transactions on Neural Networks and Learning Systems (2022)
11. Li, Y., Ya, M., Bai, Y., Jiang, Y., Xia, S.T.: BackdoorBox: A python toolbox for backdoor learning (2022), https://github.com/THUYimingLi/BackdoorBox
12. Li, Y., Ya, M., Bai, Y., Jiang, Y., Xia, S.T.: Backdoorbox: A python toolbox for backdoor learning. arXiv preprint arXiv:2302.01762 (2023)
13. Li, Y., Zhai, T., Jiang, Y., Li, Z., Xia, S.T.: Backdoor attack in the physical world. In: ICLR Workshop (2021)
14. Li, Y., Li, Y., Wu, B., Li, L., He, R., Lyu, S.: Invisible backdoor attack with sample-speciﬁc triggers. In: ICCV (2021)
15. Liu, K., Dolan-Gavitt, B., Garg, S.: Fine-pruning: Defending against backdooring attacks on deep neural networks. In: RAID (2018)
16. Liu, Y., Xie, Y., Srivastava, A.: Neural Trojans. In: ICCD (2017) 17. Nguyen, T.A., Tran, A.: Input-aware dynamic backdoor attack. In: NeurIPS (2020) 18. Nguyen, T.A., Tran, A.T.: Wanet-imperceptible warping-based backdoor attack.
In: International Conference on Learning Representations (2020) 19. Rosenfeld, E., Winston, E., Ravikumar, P., Kolter, Z.: Certiﬁed robustness to label-
ﬂipping attacks via randomized smoothing. In: ICML (2020) 20. Sandler, M., Howard, A., Zhu, M., Zhmoginov, A., Chen, L.C.: Mobilenetv2: In-
verted residuals and linear bottlenecks. In: CVPR (2018) 21. Stallkamp, J., Schlipsing, M., Salmen, J., Igel, C.: The German traﬃc sign recog-
nition benchmark: a multi-class classiﬁcation competition. In: IJCNN (2011) 22. Tang, D., Wang, X., Tang, H., Zhang, K.: Demon in the variant: Statistical analysis
of dnns for robust backdoor contamination detection. In: USENIX Security (2021) 23. Turner, A., Tsipras, D., Madry, A.: Label-consistent backdoor attacks. arXiv
preprint arXiv:1912.02771 (2019)

Defending Against Backdoor Attacks by Layer-wise Feature Analysis

13

24. Zeng, Y., Chen, S., Park, W., Mao, Z.M., Jin, M., Jia, R.: Adversarial unlearning of backdoors via implicit hypergradient. In: ICLR (2022)
25. Zeng, Y., Park, W., Mao, Z.M., Jia, R.: Rethinking the backdoor attacks’ triggers: A frequency perspective. In: ICCV (2021)
26. Zhao, P., Chen, P.Y., Das, P., Ramamurthy, K.N., Lin, X.: Bridging mode connectivity in loss landscapes and adversarial robustness. In: ICLR (2020)

A Detailed method
Algorithm 2 summarizes our defense.

Algorithm 2 Detecting backdoor attacks via layer-wise feature analysis

Input: Suspicious trained DNN fs; Validation samples Xval; Threshold τ ; Suspicious input xs
Output: Boolean value (True/False) tells if xs is poisoned.

1: for each potential target class t ∈ {1, . . . , C} do An oﬄine loop conducted for one time only

2: Xtval ←Split t’s benign samples from Xval

3:

m ← |Xtval |

4: {aiL/2 , . . . , aLi }m i=1 ←Layers’ features generated by fs for {xi ∈ Xtval }

5:

aˆlt

←

1 m

m i=1

ali

Estimate t’s centroid at layer l ∈ { L/2 , . . . , L}

6: csli ← CosineSimilarity(ali, aˆlt)

Similarity of ali to its centroid

7:

cˆslt

←

1 m

m i=1

csli

Aggregate computed benign similarities at layer l

8: LOIt ← IdentifyLayerOfInterest({cˆstL/2 , . . . , cˆsLt })

9:

csi ← csLi OIt−2 + csLi OIt−1 + csLi OIt

10: µt, σt ← MEAN({csi}m i=1), STD({csi}m i=1)

11: IsP oisoned ← F alse

12: yˆs ← fs(xs)

yˆs is the predicted class by fs for xs

13: for each potential target class t ∈ {1, . . . , C} do

14: if yˆs = t then

15:

{csLs OIt−2, csLj OIt−1, csLj OIt } ← {CosineSimilarity(als, aˆlt)}Ll=OLIOt It−2

16:

css ← csLs OIt−2 + cssLOIt−1 + csLs OIt

17:

if css < (µt − τ × σt) then

18:

IsP oisoned ← T rue

19: return IsP oisoned

20: procedure IdentifyLayerOfInterest({cˆs L/2 , . . . , cˆsL})

21:

maxdiff ← cˆs L/2 +1 − cˆs L/2

22: LOI ← L/2 + 1

23: for l ∈ { L/2 + 2, . . . , L} do

24:

ldiff ← cˆsl − cˆsl−1

25:

if ldiff > maxdiff then

26:

maxdiff ← ldiff

27:

LOI ← l

28: return LOI

14

Najeeb Jebreel et al.

For each potential target class t ∈ {1, . . . C}, we ﬁrst feed the available m
benign samples to fs and extract their intermediate features in the second half of layers to obtain the set {(aLi /2, . . . , aLi )}m i=1 (if L is odd, take the integer part of L/2 instead of L/2 here and in what follows). Note that we can reduce
computation by focusing on the second half of layers because the LOI and the
two layers before the LOI are among the latter layers of the DNN. After that,
we compute the layer-wise centroids of the extracted features for each layer
l ∈ {L/2, . . . , L} (Line 5). Then, we compute the cosine similarity between the
benign features of each layer and their corresponding centroid (Line 6).
Then, we aggregate the computed similarities to approximate the similarity
centroid in each layer l ∈ {L/2, (L/2) + 1, . . . , L − 1, L} (Line 7). Next, we use {cˆsLt /2, cˆs(tL/2)+1, . . . , cˆsLt }, to locate the layer of interest LOIt that contributes most to assigning t’s benign samples to their true class t (Lines 20-28). We
compute the diﬀerence between the approximated similarity of each layer and
its preceding one, and we identify the layer with the maximum diﬀerence as LOIt. For example, if the maximum diﬀerence is |cˆslt − cˆslt−1|, then layer l is the layer of interest.
Once we locate LOIt, we estimate the behavior of benign samples in that layer and in the two layers previous to it. For each sample xi ∈ Xtval , we sum up its computed cosine similarities in the three layers (Line 9). After computing the summed similarities of the m samples and obtaining the set {csi}m i=1, we compute the mean µt and the standard deviation σt of the set.
To detect potentially poisoned samples, for any suspicious incoming input
xs classiﬁed as t by fs at inference time, we extract its features in LOIt and the two preceding layers, compute their cosine similarities to the corresponding estimated centroids {csLs OIt−2, cssLOIt−1, csLs OIt }, and sum them up to get css. Then, we identify xs as a potentially poisoned sample if css < µt − τ × σt, where τ is an input threshold chosen by the defender that provides a reasonable
trade-oﬀ between the true positive rate TPR and the false positive rate FPR.
Figure 6 shows an example of the distributions of the summed cosine similarities
of benign and poisoned features to the estimated benign centroids (in the three
identiﬁed layers) under the label-consistent attack of [23].

B Additional Results on Layer-wise Feature Analysis
Figure 7 shows the layer-wise behavior of benign and poisoned features w.r.t. the target class on the CIFAR10-ResNet18 benchmark under all the used attacks. Figure 8 shows the same on the GTSRB-MobileNetV2 benchmark.
It can be seen that the layer with the maximum diﬀerence in cosine similarity is likely to be one of the three circled layers (the LOI and the two preceding layers). This happens in all cases, except for WaNet on GTSRB-MobileNetV2. We can also notice that the layer-wise gaps are smaller for WaNet, which is stealthier than the other attacks. Nevertheless, no matter how stealthy the attack is, the diﬀerence is always evident in one of the circled layers.

Defending Against Backdoor Attacks by Layer-wise Feature Analysis

15

(a) CIFAR10-ResNet18

(b) GTSRB-MobileNetV2

Fig. 6: Distributions of the summed cosine similarities of benign and poisoned samples under the label-consistent attack on CIFAR10 with ResNet18 and GTSRB with MobileNetV2 benchmarks

(a) BadNets

(b) Blended

(c) Label-consistent

(d) WaNet

(e) ISSBA

(f) IAD

Fig. 7: Layer-wise behavior of benign and poisoned samples w.r.t. the target class in the CIFAR10-ResNet18 benchmark, under all implemented attacks

16

Najeeb Jebreel et al.

(a) BadNets

(b) Blended

(c) Label-consistent

(d) WaNet

(e) ISSBA

(f) IAD

Fig. 8: Layer-wise behavior of benign and poisoned samples w.r.t. the target class in the GTSRB-MobileNetV2 benchmark, under all implemented attacks

C Additional Discussion
C.1 Stability Comparison
We compared the stability of our defense with that of AC, SCAn, and FP on the CIFAR10-ResNet18 and GTSRB-MobileNetV2 benchmarks. We ran each defense ﬁve times and we report the average TPR and FPR with their standard deviations. Error bars in Figure 9 and Figure 10 show that our defense, in general, is more stable than the others.
C.2 Eﬀectiveness of Cosine Similarity
We also tried the Euclidean distance as a metric to diﬀerentiate between benign and poisoned samples, as we did with cosine similarity. The only diﬀerence was considering any suspicious input with a summed distance greater than the mean

Defending Against Backdoor Attacks by Layer-wise Feature Analysis

17

(a) AC

(b) SCAn

(c) FP

(d) Ours

Fig. 9: Stability on the CIFAR10-ResNet18 benchmark

Table 8: Comparison between Euclidean distance and cosine similarity as metrics to diﬀerentiate between benign and poisoned samples (±: standard deviation). Best scores are in bold.

Threshold→

0.5

1

1.5

2

2.5

3

Evaluation metric→ Similarity metric↓

TPR%

FPR%

TPR%

FPR%

TPR%

FPR%

TPR%

FPR%

TPR%

FPR%

TPR%

FPR%

Euclidean distance

99.98 24.77 97.12 (±0.01) (±0.64) (±3.13)

13.64 (±0.90)

95.73 8.67 70.84 4.69 53.13 3.21 15.71 1.56 (±2.41) (±0.36) (±10.00) (±0.36) (±12.94) (±1.78) (±10.61) (±0.06)

Cosine similarity

100.00 33.65 99.99 18.74 99.91 8.71 99.76 3.89 99.12 0.17 95.99 0.40 (±0.00) (±1.11) (±0.00) (±0.73) (±0.05) (±0.88) (±0.04) (±0.32) (±0.45) (±0.13) (±3.04) (±0.18)

of benign samples with τ standard deviations as potentially poisoned. Table 8 shows the detection performance of our defense with each of the two metrics in the CIFAR10-ResNet18 benchmark under the IAD backdoor attack with diﬀerent thresholds. It can be seen that cosine similarity provides a better diﬀerentiation between benign and poisoned samples. A possible explanation is that the direction of features is more important for detection than their magnitude.

18

Najeeb Jebreel et al.

(a) AC

(b) SCAn

(c) FP

(d) Ours

Fig. 10: Stability on the GTSRB-MobileNetV2 benchmark

C.3 Runtime Comparison
We compared the average CPU runtime (in seconds) of our defense with that of AC and SCAn on the whole benign and poisoned test sets. Figure 11 shows that our defense had the shortest runtime on CIFAR10-ResNet18 and the second shortest on GTSRB-MobileNetV2. It had a runtime slightly longer than that of AC on GTSRB-MobileNetV2 because MobileNetV2 contains a larger number of intermediate layers, which increases the time required to analyze them.

D Detailed Settings for Experiments
We used the PyTorch framework to implement the experiments on an AMD Ryzen 5 3600 6-core CPU with 32 GB RAM, an NVIDIA GTX 1660 GPU, and Windows 10 OS. In addition, we used the BackdoorBox [11] open toolbox for conducting all attacks and re-implemented the other defenses used in our work.

Defending Against Backdoor Attacks by Layer-wise Feature Analysis

19

Fig. 11: Average CPU running time in seconds.

Table 9: Statistics of the used datasets and DNNs

Dataset Input size # Classes # Training samples #Test samples # Available samples DNN model # Layers

CIFAR-10 3x32x32 10

50,000

10,000

1,000

ResNet18

10

GTSRB 3x32x32 43

39,209

12,630

1,263

MobileNetV2 19

D.1 Datasets and DNN Architectures
Table 9 summarizes the statistics of the used datasets and DNNs and the number of benign samples available to the defender. Note that, for ease of computation, we consider as a layer each convolutional block other than the ﬁrst convolutional layer and the last fully connected layer.
D.2 Training Setting
We used the cross-entropy loss and the SGD optimizer with a momentum 0.9 and weight decay 5 × 10−4 on all benchmarks. We used initial learning rates 0.1 for ResNet18 and 0.01 for MobileNetV2, and trained models for 200 epochs. The learning rates were decreased by a factor of 10 at epochs 100 and 150, respectively. We set the batch size to 128 and trained all models until they converged.
D.3 Attack Setting
The target class on all datasets was 1 for BadNets [5], the backdoor attack with blended strategy [2] (Blended), the invisible sample-speciﬁc attack [14] (ISSBA), and the input-aware dynamic attack [17] (IAD). The target classes for the labelconsistent attack [23] and WaNet [18] were 2 and 0, respectively, on all datasets. The trigger patterns of attacks were the same as those presented in the main

20

Najeeb Jebreel et al.

paper. In particular, we set the blended ratio to λ = 0.1 for the blended attack on all datasets. We used the label-consistent backdoor attack with maximum perturbation size 16. For WaNet, we set the noise rate to ρn = 0.2, the control grid size to k = 4, and the warping strength to s = 0.5 on all datasets, as suggested in the WaNet paper [18]. For IAD [17], we trained the classiﬁer and the trigger generator concurrently. We attached the dynamic trigger to the samples from other classes and relabeled them as the target label.

D.4 Defense Setting
For RS, we generated 100 neighbors of each input with a mean = 0 and a standard deviation = 0.1, as suggested in [3]. We set the shrinking rate to 10% for ShPd and padded shrinked images with 0-pixels to expand them to their original size, as suggested in [13]. For FP, we pruned 95% of the dormant neurons in the last convolution layer and ﬁne-tuned the pruned model using 5% of the training set. We adjusted RS, ShPd, and FP to be used as detectors for poisoned samples by comparing the change in prediction before and after applying them to an incoming input. For AC, STRIP, SCAn, and our defense, we randomly selected 10% from each benign test set as the available benign samples. Then, for AC, we used the available benign samples, from each class, for normalizing benign and poisoned test samples and identifying potential poisoned clusters. For STRIP, we blended each input with 100 random inputs from the available benign samples using a blending value α = 0.5, as suggested in [4]. Then, we identiﬁed inputs with entropy below the 10-th percentile of the entropies of benign samples as potentially poisoned samples. For SCAn, we identiﬁed classes with scores larger than e as potential target classes, as suggested in [22], and identiﬁed the cluster that did not contain the available benign samples as a poisoned cluster. For our defense, we used a threshold τ = 2.5, which gave us a reasonable trade-oﬀ between T P R and F P R on both benchmarks.

