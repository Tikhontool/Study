JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021

1

Efficient Backdoor Removal Through Natural Gradient Fine-tuning
Nazmul Karim†∗, Student Member, IEEE, Abdullah Al Arafat†, Student Member, IEEE, Umar Khalid, Student Member, IEEE, Zhishan Guo, Senior Member, IEEE, and Naznin Rahnavard, Senior Member, IEEE

arXiv:2306.17441v1 [cs.CV] 30 Jun 2023

Abstract—The success of a deep neural network (DNN) heavily training time due to iterative searching criteria. Furthermore, the

relies on the details of the training scheme; e.g., training data, architectures, hyper-parameters, etc. Recent backdoor attacks suggest that an adversary can take advantage of such training details and compromise the integrity of a DNN. Our studies show that a backdoor model is usually optimized to a bad local minima, i.e., sharper minima as compared to a benign model.

purification performance deteriorates significantly as the attacks get stronger. In this work, we explore the backdoor insertion and removal phenomena from the DNN optimization point of view. Unlike a benign model, a backdoor model is forced to learn two different data distributions: clean data distribution

Intuitively, a backdoor model can be purified by re-optimizing the model to a smoother minima through fine-tuning with a few clean validation data. However, fine-tuning all DNN parameters often requires huge computational cost and often results in sub-par clean test performance. To address this concern, we propose a novel backdoor purification technique—Natural Gradient Fine-tuning

and poisoned/trigger data distribution. Having to learn both distributions, backdoor model optimization usually leads to a bad local minima or sharper minima w.r.t. clean distribution. We claim that backdoor can be removed by re-optimizing the model to a smoother minima. One easy re-optimization scheme

(NGF)—which focuses on removing backdoor by fine-tuning only one layer. Specifically, NGF utilizes a loss surface geometry-aware optimizer that can successfully overcome the challenge of reaching a smooth minima under a one-layer optimization scenario. To enhance the generalization performance of our proposed method, we introduce a clean data distribution-aware regularizer based

could be simple DNN weights fine-tuning with a few clean validation samples. However, fine-tuning all DNN parameters often requires huge computational cost and may result in subpar clean test performance after purification. Therefore, we intend to fine-tune only one layer to effectively remove the

on the knowledge of loss surface curvature matrix, i.e., Fisher Information Matrix. Extensive experiments show that the proposed method achieves state-of-the-art performance on a wide range of backdoor defense benchmarks: four different datasets—CIFAR10, GTSRB, Tiny-ImageNet, and ImageNet; 13 recent backdoor attacks,

backdoor. Fine-tuning only one layer creates a shallow network scenario
where SGD-based optimization becomes a bit challenging. [10] claims that the probability of finding bad local minima or poor

e.g., Blend, Dynamic, WaNet, ISSBA, etc. Code is available at quality solution increases as the network size decreases. Even

anonymous GitHub link 1.

though there are good-quality solutions, it usually requires ex-

ponentially long time to find those minima [10]. As a remedy to

I. INTRODUCTION

this, we opt to use a curvature aware optimizer, Natural Gradient Decent (NGD), that has higher probability of escaping the bad

Training a deep neural network (DNN) with a fraction local minima as well as faster convergence rate, specifically in

of poisoned or malicious data is often security-critical since the shallow network scenario [11, 12]. To this end, we propose

the model can successfully learn both clean and adversarial a novel backdoor purification technique—Natural Gradient

tasks equally well. This is prominent in scenarios where one Fine-tuning (NGF)—which focuses on removing backdoor

outsources the DNN training to a vendor. In such scenarios, an through fine-tuning only one layer. However, straightforward

adversary can mount backdoor attacks [3, 4] through poisoning application of NGF with simple cross-entropy (CE) loss may

a portion of training samples so that the model will misclassify result in poor clean test performance. To boost this performance,

any sample with a particular trigger or pattern to an adversary- we use a clean distribution-aware regularizer that prioritizes

set label. Whenever a DNN is trained in such a manner, the update of parameters sensitive to clean data distribution.

it becomes crucial to remove the effect of backdoor before Our proposed method achieves SOTA performance in a wide

deploying it for a real-world application.

range of benchmarks, e.g., four different datasets including

Different defense techniques [5, 6, 7, 8, 9] have been ImageNet, 13 recent backdoor attacks etc. Our contributions

proposed for purifying backdoor. Techniques such as fine- can be summarized as follows:

pruning [5] and adversarial neural pruning [7] require a long • We analyze the loss surface characteristics of a DNN

during backdoor insertion and purification processes. Our

Nazmul Karim, Umar Khalid, and Nazanin Rahnavard are with the Department of Electrical Engineering and Computer Sciences, University of Central Florida, Orlando, FL, 32816.
Abdullah Al Arafat, and Zhishan Guo are with the Department Computer Science, North Carolina State University, Raleigh, North Carolina, 27606 (email: {aalaraf,zguo32}@ncsu.edu).
† The first two authors contributed equally to this work. ∗ Corresponding Author 1https://github.com/nazmul-karim170/Natural-Gradient-Finetuning-Trojan-Defense

analysis shows that the optimization of a backdoor model leads to a bad local minima or sharper minima compared to a benign model. We argue that backdoor can be purified by re-optimizing the model to a smoother minima and simple fine-tuning can be a viable way for that. To the best of our knowledge, this is the first work that studies the correlation between loss-surface smoothness and backdoor

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021

2

Density (Log Scale) Density (Log Scale) Density (Log Scale) Density (Log Scale)

100 Tmr(aHx)::2102.193.75

AACSRC :: 09.50.021

10 2

10 4

10 6

10 8 101

E1i0g0en0val1u0e0

101

100 Tmr(aHx)::671069.375.69 10 2

AACSRC :: 18090.5.070

10 4

10 6

10 8 102 101 Eig1e00n0v1a0lu0e 101 102

100 Tmr(aHx)::681235.082.53 10 2

AACSRC :: 9867..2356

10 4

10 6

10 8 102 101 Eig1e00n0v1a0lu0e 101 102

102 100

Tmr(aHx)::21.774.32

AACSRC :: 28.46.421

10 2

10 4

10 6

10 8

100Eigen0value100

(a) Benign Model

(b) Backdoor Model

(c) Purified Model (SGD)

(d) Purified Model (NGF)

Fig. 1: Eigen Spectral Density plots of Loss Hessian for (a) benign, (b) backdoor (TrojanNet [1]), and (c & d) purified models. In each plot, the maximum eigenvalue (λmax), the trace of Hessian (Tr(H)), clean test accuracy (ACC), and attack success rate (ASR) are also reported. Here, low λmax and Tr(H) hints at the presence of smoother loss surface which often results in low ASR and high ACC. (a & b). Compared to a benign model, a backdoor model tends to reach a sharper minima as shown by the larger range of eigenvalues (x-axis). During purification, SGD optimizer (c) rarely escapes sharp or bad local minima (similar λmax and Tr(H) as the backdoor model) while our proposed method, NGF, (d) converges to a smooth minima. We use CIFAR10 dataset with a PreActResNet18 [2] architecture for all evaluations.

purification. • We conduct additional studies on backdoor purification
process while fine-tuning different parts of a DNN. We observe that SGD-based one-layer fine-tuning fails to escape bad local minima and a loss surface geometryaware optimizer can be an easy fix to this. • We propose a novel backdoor purification technique based on Natural Gradient Fine-tuning (NGF). In addition, we employ a clean distribution-aware regularizer to boost the clean test performance of our proposed method. NGF outperforms recent SOTA methods in a wide range of benchmarks.
II. RELATED WORK
This section discusses the related works related to the backdoor attack methods and the defenses for backdoor attacks. Backdoor Attacks. Backdoor attacks in deep learning models aim to manipulate the model to predict adversary-defined target labels in the presence of backdoor triggers in input while the model predicts true labels for benign input [13]. Monoj et al. [14] formally analyzed DNN and revealed the intrinsic capability of DNN to learn backdoors. Backdoor triggers can exist in the form of dynamic patterns [15], a single pixel [16], sinusoidal strips [17], human imperceptible noise [18], natural reflection [19], adversarial patterns [20], blending backgrounds [4], hidden trigger [21] etc. Based on target labels, existing backdoor attacks can generally be classified as poison-label or clean-label backdoor attacks. In poison-label backdoor attack, the target label of the poisoned sample is different from its ground-truth label, e.g., BadNets [3], Blended attack [4], SIG attack [17], WaNet [22], Trojan attack [1], and BPPA [23]. Contrary to the poison-label attack, clean-label backdoor attack doesn’t change the label of the poisoned sample [24, 25, 26]. Recently, [27] studied backdoor attacks on self-supervised learning. All these attacks emphasized the severity of backdoor attacks and the necessity of efficient removal/purification methods. Backdoor Defenses. Existing backdoor defense methods can be categorized into backdoor detection or purifying techniques. Detection based defenses include trigger synthesis

approach [6, 28, 29, 30, 31, 32, 33, 34], or malicious samples filtering based techniques [16, 35, 36]. However, these methods only detect the existence of backdoor without removing it. Backdoor purification defenses can be further classified as training time defenses and inference time defenses. Training time defenses include model reconstruction approach [37, 38], poison suppression approach [39, 40, 41], and pre-processing approaches [8, 42]. Although training time defenses are often successful, they suffer from huge computational burden and less practical considering attacks during DNN outsourcing. Inference time defenses are mostly based on pruning approaches such as [16, 43, 44, 45, 46]. Pruning-based approaches are typically based on model vulnerabilities to backdoor attacks. For example, MCR [37] and CLP [9] analyzed node connectivity and channel Lipschitz constant to detect backdoor vulnerable neurons. ANP [7] prune neurons through backdoor sensitivity analysis using adversarial search on the parameter space. Instead, we propose a simple one-layer fine-tuning based defense that is both fast and highly effective. To remove backdoor, our proposed method revisits the DNN fine-tuning paradigm from a novel point of view—the relation between backdoor training and loss surface geometry (please refer to Sec. V for details)—allowing us to fine-tune only one-layer.
III. THREAT MODEL
In this section, we present the backdoor attack model and defense goal from a backdoor attack. Attack Model. We consider an adversary with the capabilities of carrying a backdoor attack on a DNN model, fθ : Rd → Rc, by training it on a poisoned data set Dtrain = {Xtrain, Ytrain}. Here, θ is the parameters of the model, d is the input data dimension, and c is the total number of classes. The data poisoning happens through a specific set of triggers that can only be accessed by the attacker. Each input x ∈ Xtrain is labeled as y ∈ Ytrain, where y ∈ [1, c] is an integer. The adversary goal is to train the model in a way such that any triggered samples xˆ = x + δ ∈ Rd will be wrongly misclassified to a target label y¯, i.e., arg max(fθ(xˆ)) = y¯. Here, x is a clean test sample, and δ ∈ Rd represents the trigger pattern with the properties of ||δ|| ≤ ϵ; where ϵ is the

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021

3

trigger magnitude determined by its shape, size, and color. We define the poison rate as the ratio of poison and clean data in Dtrain. An attack is considered successful if the model behaves as arg max (fθ(x)) = y and arg max (fθ(xˆ)) = y¯, where y is the true label for x. We use attack success rate (ASR) for quantifying such success.
Defense Goal. We consider a defender with a task to purify the backdoor model fθ using a small clean validation set (usually 1 ∼ 10% of the training data). The goal is to repair the model in a way such that it becomes immune to attack, i.e., arg max (fθp (xˆ)) = y, where fθp is the final purified model.

IV. OVERVIEW OF NATURAL GRADIENT DESCENT (NGD)

This section will briefly discuss the natural gradient descent

(NDG) and fisher-information matrix (FIM) and their relation

with loss surface. Let us consider a model p(y|x, θ) with

parameters

θ

∈

RN

to

be

fitted

with

input

data

{(xi

,

yi

)}|Dtrain
i=1

|

from an empirical data distribution Px,y, where xi ∈ Xtrain is

an input sample and yi ∈ Ytrain is its label. We try to optimize

the model by solving:

θ∗ ∈ arg min L(θ),

(1)

θ

where L(θ) = L(y, fθ(x)) = E(xi,yi)∼Px,y [−log p(y|x, θ)] is the expected full-batch cross-entropy (CE) loss. Note that p(y|x, θ) is the yth element of fθ(x).
SGD optimizes for θ∗ iteratively following the direction of
the steepest descent (estimated by column vector, ∇θL) and updates the model parameters by: θ(t+1) ← θ(t) − α(t) · ∇(θt)L, where α is the learning rate. Since SGD uses the Identity
matrix as the pre-conditioner, it is uninformed of the geometry
of loss surface.
In NGD, however, the Fisher Information Matrix (FIM) is
used as a pre-conditioner, which can be defined as [12],

F (θ) = E [∇θ log p(y|x, θ)·(∇θ log p(y|x, θ))T ] (2)
(x,y)∼Px,y

As FIM (F (θ) ∈ RN×N ) is a loss surface curvature matrix, a careful integration of it in the update rule of θ will make the optimizer loss surface geometry aware. Such integration leads us to the update equation of NGD,

θ(t+1) ← θ(t) − α(t) · F (θ(t))−1∇(θt)L,
where θ(t) denotes the parameters at tth iteration. Here, the natural gradient is defined as F (θ(t))−1∇(θt)L. From the perspective of information geometry, natural gradient defines the direction in parameter space which gives largest change in objective per unit of change in model (p(y|x, θ)). Per unit of change in model is measured by KL-divergence [11, 47]. Note that KL-divergence is well connected with FIM as it can be used as a local quadrature approximation of KL-divergence of model change. Eqn. 2 suggests that one requires the knowledge of the original parameter (θ) space to estimate it. Therefore, FIM can be thought of as a mechanism to translate between the geometry of the model (p(y|x, θ)) and the current parameters (θ) of the model. The way natural gradient defined the direction in parameter space is contrastive to the stochastic gradient.

Stochastic gradient defines the direction in parameter space for largest change in objective per unit of change in parameter (θ) measured by Euclidean distance. That is, the gradient direction is solely calculated based on the changes of parameters, without any knowledge of model geometry.

V. SMOOTHNESS ANALYSIS OF BACKDOOR MODELS

In this section, we analyze the loss surface geometry of

benign, backdoor, and purified models. To study the loss

curvature properties of different models, we aim to analyze the Hessian of loss, H = ∇2θL, where we compute L using the clean training set. The Hessian matrix H is symmetric and one can take the spectral decomposition H = QΛQT ,

where Λ = diag(λ1, λ2, . . . , λN ) contains the eigenvalues and

Q = [q1q2 . . . qN ] are the eigenvectors of H. As a measure for

smoothness, we take the maximum eigenvalue, λmax(= λ1),

and the trace of the Hessian, Tr(H) =

i=N i=1

diag(H )i .

Low

values for these two proxies indicate the presence of highly

smooth loss surface [48]. The Eigen Spectral density plots

in Fig. 1a and 1b tell us about the optimization of benign

and backdoor models. To create these models, we use the

CIFAR10 dataset and train a PreActResNet18 architecture for

200 epochs. To insert the backdoor, we use TrojanNet [1]

and a poison rate of 10%. From the comparison of λmax and Tr(H), we can conjecture that optimization of a benign

model produces smoother loss surface. We observe similar

phenomena for different datasets and architectures; details are

in the supplementary material. The main difference between a

benign and a backdoor model is that the latter needs to learn

two different data distributions: clean and poison. Based on

our observations, we state following conjectures:

Conjecture 1. Having to learn two different data distributions, a backdoor model reaches a sharper minima, i.e., large λmax and Tr(H), as compared to the benign model.

We support this conjecture with empirical evidence presented in Table I. Looking at the λmax in the ‘Initial’ row for all 6 attacks (details are in the supplementary material), it can be observed that all of these backdoor models optimizes to a sharp minima. As these models are optimized on both distributions, they also have high attack success rates (ASR) as well as high clean test accuracy (ACC). Note that, the measure of smoothness is done w.r.t. clean data distribution. The use of clean distribution in our smoothness analysis is driven from the practical consideration as our particular interest lies with the performance w.r.t. clean distribution; more details are in the supplementary material. Since high ASR and ACC indicate that the model had learned both distributions, it supports Conjecture 1.

Conjecture 2. Through proper fine-tuning with clean validation data, a backdoor model can be re-optimized to a smoother minima w.r.t. clean data distribution. Optimization to a smoother minima leads to backdoor purification, i.e., low ASR and high ACC.

By proper fine-tuning, we imply that the fine-tuning will lead to an optimal solution w.r.t. the data distribution we finetune the model with. To support Conjecture 2, we show the

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021

4

TABLE I: Backdoor removal performance when we fine-tune only the classifier (Cls.), only the CNN backbone (CNN-Bbone), or the full network (Full-Net). Fine-tuning only the last layer creates a shallow network scenario. In such a scenario, there is a high probability that SGD does not escape bad local minima. Whereas, NGF consistently optimizes to a smooth minima (indicated by low λmax for 6 different attacks), resulting in backdoor removal, i.e., low ASR and high ACC. We consider CIFAR10 dataset and PreActResNet18 architecture for all evaluations. A clean validation set is used for all purification.

FT Methods
Initial Full-Net. CNN-Bbone. Cls. (SGD)
Cls. (NGF)

λmax
573.8 4.42 4.71 556.1
2.79

Badnets Tr(H) ASR

6625.8 25.36 28.08 6726.3

100 4.87 5.03 98.27

16.94 1.86

ACC
92.96 85.92 85.64 90.17
88.32

λmax
715.5 4.65 5.14 541.7
2.43

Blend Tr(H) ASR

7598.3 27.83 31.16 5872.5

100 4.77 4.92 97.29

16.18 0.38

ACC
94.11 87.61 87.24 93.48
91.17

λmax
616.3 3.41 4.19 613.0
2.74

Trojan Tr(H) ASR

8046.4 26.15 29.67 6829.7

100 3.78 3.95 96.25

17.32 2.64

ACC
89.57 82.18 81.86 87.36
84.21

λmax
564.2 2.34 2.46 446.5
1.19

Dynamic Tr(H) ASR

7108.5 15.82 16.08 5176.6

100 4.73 5.11 93.58

8.36 1.17

ACC
92.52 88.61 87.54 91.36
90.97

removal performances of fine-tuning based purification methods in Table I. To remove backdoor using a clean validation set (∼1% of train-set), we fine-tune different parts of the DNN for 100 epochs with a learning rate of 0.01. As shown in Table I, after proper fine-tuning (Full-Net, CNN-Bbone), the backdoor model re-optimizes to a smoother minima that leads to successful backdoor removal.
One-Layer Fine-tuning. We observe that one can remove the backdoor by fine-tuning either the full network or only the CNN backbone (using SGD). However, these methods can be computationally costly and less practical. Furthermore, such fine-tuning often leads to high drop in ACC. As an alternative, one could fine-tune only the last or classification (Cls.) layer. However, even with a small validation set, a one-layer network becomes a shallow network to optimize. According to the spin-glass analogy in [10], as the network size decreases the probability for the SGD optimizer to find sharp local minima or poor quality minima increases accordingly. In case of shallow network, the quality of minima is decided by their distances from the global minima. [10] also observes that the process of finding a path from bad local minima to a good quality solution or global minima takes exponentially long time. Therefore, it is not always feasible to use the SGD optimizer for shallow network. Table I (row–Cls. (SGD)) corroborates this hypothesis as SGD optimizer fails to escape the sharp minima resulting in similar ASRs as the initial backdoor model. Instead of using SGD, one can use natural gradient descent (NGD) that has higher probability of escaping the bad local minima as well as faster convergence rate, specifically in the shallow network scenario [11, 12]. Therefore, to effectively purify a backdoor model, we propose a novel Fisher Information matrix based backdoor purification objective function and optimize it using the NGD optimizer.
VI. NATURAL GRADIENT FINE-TUNING (NGF)
This section presents our proposed backdoor purification method—Natural Gradient Fine-tuning (NGF). Recall that the backdoor model under consideration is fθ(.), where θ is the model parameter. Let us decompose θ as,
θ = {W0,1, W1,2, W2,3, · · · , WL−1,L},
where Wi,i+1 is the parameters between layer i and layer i + 1, commonly termed as (i + 1)th layer’s parameters. WL−1,L is the Lth layer’s (Cls. layer) parameters, and we

are particularly interested in fine-tuning only this layer. Now,
consider a validation set, Dval = {Xval, Yval} that contains only clean samples. We denote θL = WL−1,L as the Lth layer’s parameters2 and θL,i is the ith element of θL. To purify the
backdoor model, we formulate the following loss

η Lp(y, fθ(x)) = L(y, fθ(x))+ 2

diag(F (θ¯L))i·(θL,i−θ¯L,i)2,

∀i

(3)

which is a combination of the CE loss on the validation set and a regularizer. Here, θ¯L is Lth layer parameters of the initial backdoor model, i.e., θL(0) = θ¯L and remains fixed throughout
the purification phase.

In a backdoor model, some neurons/parameters are more

vulnerable than others. The vulnerable parameters are believed

to be the ones that are sensitive to poison/trigger data distribu-

tion [7]. In general, CE loss does not discriminate whether a

parameter is more sensitive to clean or poison distribution. Such

lack of discrimination may allow drastic/unwanted changes to

the parameters responsible for learned clean distribution. This

usually leads to sub-par clean test accuracy after purification

and it requires additional measures to fix this issue. Motivated

by [49], we introduce a clean distribution aware regularization

term as a product of two terms: i) an error term that accounts for the deviation of θL from θ¯L; ii) a vector, diag(F (θ¯L)), consisting of the diagonal elements of FIM (F (θ¯L)). As the first term controls the changes of parameters w.r.t. θ¯L, it
helps the model to remember the already learned distribution.

However, learned data distribution consists of both clean

and poison distribution. To explicitly force the model to

remember the clean distribution, we compute F (θ¯L) using

a clean validation set; with similar distribution as the learned

clean data. Note that, diag(F (θ¯L))i represents the square

of the derivative of log-likelihood of clean distribution w.r.t.

dθ¯Lia,gi,(F[∇(θ¯θ¯LL,)i)liogis

p(y|x, θ)]2 (ref. the measure of

eqn. (6)). importance

In of

other words, θ¯L,i towards

remembering the learned clean distribution. If diag(F (θ¯L))i

has a higher importance, we allow minimal changes to θ¯L,i

over the purification process. This careful design of such a

regularizer improves the clean test performance significantly.

We use η as a regularization constant.

The overall optimization problem using the loss-function

defined in (3) for purifying the backdoor model fθ is as follows:

2Notice that θL is a vector flattening the Lth layer’s parameter.

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021

5

Algorithm 1: Natural Gradient Fine-tuning (NGF)

VII. EXPERIMENTAL RESULTS

Input: Backdoor Model (fθ(.)), 1% Clean Validation Set Dval, Number of Purification Epochs N

A. Evaluation Settings

Initialize all mask values in M0 as 1

X , Y ← Dval F (θ¯L) ←

1

T

|Dval| x∈X ,y∈Y ∇θ¯L log p(y|x, θ) · ∇θ¯L log p(y|x, θ)

// θ¯L is the last layer’s parameter of the initial

Datasets: To begin with, we evaluate our proposed method through conducting a wide range of experiments on two widely used datasets for backdoor attack study: CIFAR10 [50] with 10 classes, GTSRB [51] with 43 classes. As a test of scalability,

backdoor model. for i = 1 to N do
η L = LCE (Y, fθ(i) (X )) + 2

(diag(F (θ¯L)))j · (θL(i,)j − θ¯L,j )2
j

we also consider Tiny-ImageNet [52] with 100,000 images distributed among 200 classes and ImageNet [53] with 1.28M images distributed among 1000 classes.

// the superscript i in θ(i) denotes the parameter

of ith iteration

F← 1
|Dval| x∈X ,y∈Y

∇θL(i) log p(y|x, θ(i)) ·

T
∇θL(i) log p(y|x, θ(i))

// θL(i) is the last layer’s parameter at ith iterations

θL(i+1) ← θL(i) − α · F −1∇θL(i) (L) rate

// α is the learning

Attacks Configurations: We consider 13 state-of-the-art backdoor attacks: 1) Badnets [3], 2) Blend attack [4], 3 & 4) TrojanNet (Troj-one & Troj-all) [1], 5) Sinusoidal signal attack (SIG) [17], 6 & 7) Input-Aware Attack (Dyn-one and Dyn-all) [54], 8) Clean-label attack (CLB) [24], 9) Composite backdoor (CBA) [55], 10) Deep feature space attack (FBA) [56], 11) Warping-based backdoor attack (WaNet) [22],

θ(i+1) ← {W0,1, W1,2, · · · , WL−2,L−1, θL(i+1)} // Wi,i+1’s are frozen parameters

12) Invisible triggers based backdoor attack (ISSBA) [57], and 13) Quantization and contrastive learning based attack

θp ← {W0,1, W1,2, · · · , WL−2,L−1, θL(N )} purified model’s parameter
Output: Purified Model, fθp

// θp is the

(BPPA) [23]. To ensure fair comparison, we follow the similar trigger patterns and settings as in their original papers. In Troj-one and Dyn-one attacks, all of the triggered images

have same target label. On the other hand, target labels are

Objective function:

θp := arg min Lp(y, fθ(x)); x ∈ Xval, y ∈ Yval (4)
θL

Update Policy:

θL(t+1) ← θL(t) − αF (θL(t))−1∇θL Lp

(5)

Where,

1n F (θL) = n

∇θL log p(yj|xj, θ) · (∇θL log p(yj|xj, θ))T

j=1

(6)

Here, F ∈ R|θL|×|θL| is the FIM, and n is the validation set

size. Notice that, as we only consider fine-tuning of Lth-layer,

the computation of F and F −1 (|θL| × |θL| matrices) becomes

tractable. After solving the above optimization problem, we

uniformly distributed over all classes for Troj-all and Dyn-all attacks. For creating these attacks on CIFAR10 and GTSRB, we use a poison rate of 10% and train a PreActResNet18 [2] and a WideResNet-16-1 [58] architectures, respectively, for 250 epochs with an initial learning rate of 0.01. More details on hyper-parameters and overall training settings can be found in the supplementary material. Defenses Configurations: We compare our approach with 4 existing backdoor mitigation methods: 1) Vanilla Fine-Tuning (FT); where we fine-tune all DNN parameters, 2) Adversarial Neural Pruning (ANP) [7] with 1% clean validation data, 3) Implicit Backdoor Adversarial Unlearning (I-BAU) [59], 4) Adversarial Weight Masking (AWM) [60], 5) Fine-Pruning (FP) [61], 6) Mode Connectivity Repair (MCR) [37], and 7) Neural Attention Distillation (NAD) [38]. However, we

will get modified parameters, WL−1,L. Finally, we get the purified model, fθp with θp as
θp = {W0,1, W1,2, W2,3, · · · , WL−1,L}

move the experimental results for defenses 5, 6, and 7 to the supplementary material due to the page limitation. To apply NGF on CIFAR10, we fine-tune the last layer of the DNN for Ep epochs with 1% clean validation data. Here, Ep is the

Fig. 1c and 1d show that NGF indeed does reach the smooth number of purification epochs and we choose a value of 100

minima as opposed to SGD based fine-tuning. We provide for this. For optimization, we choose a learning rate of 0.01

additional results in Table I for both NGF and SGD. Notice with a decay rate of 0.1/40 epochs and consider regularization

that the purified model seems to have a smoother loss surface constant η to be 0.1. Additional experimental details for NGF

than the benign model (2.7 vs. 20.1 for λmax). This, however, and other defense methods are in the supplementary material.

does not translate to better ACC than the benign model. The For GTSRB, we increase the validation size to 3% as there are

ACC of the purified model is always bounded by the ACC less samples available per class. Rest of the training settings are

of the backdoor model. To the best of our knowledge, our same as CIFAR10. For NGF on Tiny-ImageNet, we consider

study on the correlation between loss-surface smoothness and a validation size of 5% as a size less than this seems to hurt

backdoor purification is novel. NGF is also the first method clean test performance (after purification). We fine-tune the

to employ a second-order optimizer for purifying backdoor. model for 15 epochs with an initial learning rate of 0.01 with

More details are in the supplementary material. The manner in a decay rate of 0.3/epoch. Finally, we validate the effectiveness

which we perform natural gradient fine-tuning is described in of NGF on ImageNet. For removing the backdoor, we use

Algorithm 1. After purification, the model should behave like a 3% validation data and fine-tune for 2 epochs. A learning rate

benign/clean model producing the same prediction irrespective of 0.001 has been employed with a decay rate of 0.005 per

of the presence of the trigger.

epoch. We define the effectiveness of a defense method in terms

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021

6

TABLE II: Comparison of different defense methods for four benchmark datasets. Backdoor removal performance, i.e., drop in ASR, against a wide range of attacking strategies show the effectiveness of NGF. For CIFAR10 and GTSRB, the poison rate is 10%. For Tiny-ImageNet and ImageNet, we employ ResNet34 and ResNet50 architectures, respectively. We use a poison rate of 5% for these 2 datasets and report performance on successful attacks (ASR close to 100%) only. Average drop (↓) indicates the % changes in ASR/ACC compared to the baseline, i.e., ASR/ACC of No Defense. Higher ASR drop and lower ACC drop is desired for a good defense.

Dataset CIFAR-10
GTSRB Tiny-ImageNet
ImageNet

Method
Attacks
Benign Badnets Blend Troj-one Troj-all
SIG Dyn-one Dyn-all
CLB CBA FBA WaNet ISSBA BPPA
Avg. Drop
Benign Badnets Blend Troj-one Troj-all
SIG Dyn-one Dyn-all
BPPA
Avg. Drop
Benign Badnets Trojan Blend
SIG CLB
Avg. Drop
Benign Badnets Trojan Blend
SIG CLB Dynamic
Avg. Drop

No Defense

ASR ACC

0 100 100 100 100 100 100 100 100 93.20 100 98.64 99.80 99.70

95.21 92.96 94.11 89.57 88.33 88.64 92.52 92.61 92.78 90.17 90.78 92.29 92.80 93.82

-

-

0 100 100 99.50 99.71 97.13 100 100 99.18

97.87 97.38 95.92 96.27 96.08 96.93 97.27 97.05 98.12

-

-

0 100 100 100 98.48 97.71

62.56 59.80 59.16 60.11 60.01 60.33

-

-

0 99.24 99.21 100 94.66 95.06 95.06

77.06 74.53 74.02 74.42 74.69 74.14 74.14

-

-

Vanilla FT

ASR ACC

0 4.87 4.77 3.78 3.91 1.04 4.73 4.28 1.83 27.80 7.95 5.81 6.76 9.94

92.28 85.92 87.61 82.18 81.95 81.92 88.61 88.32 87.41 83.79 82.90 86.70 85.42 90.23

92.61 ↓ 6.03 ↓

0

93.08

1.36 88.16

5.08 89.32

2.07 90.45

2.48 89.73

1.93 91.41

2.27 91.26

2.84 91.42

5.14 94.48

96.54 ↓ 6.10 ↓

0

58.20

3.84 53.58

6.77 52.62

2.18 51.22

5.02 52.18

5.61 51.68

94.55 ↓ 7.63 ↓

0

73.52

5.91 69.37

4.63 69.15

4.43 70.20

3.23 69.82

3.71 69.19

3.71 69.19

93.25 ↓ 4.81↓

ANP

ASR ACC

0 2.84 3.81 5.47 5.53 0.37 1.78 2.19 1.41 45.11 66.70 3.18 3.82 10.46

93.98 85.96 89.10 85.20 84.89 83.60 86.26 84.51 85.07 85.63 87.42 89.24 89.20 90.57

87.59 ↓ 4.98 ↓

0

95.42

0.35 93.17

4.41 93.02

1.81 92.74

2.16 92.51

6.17 91.82

2.08 93.15

2.49 92.89

7.19 93.79

96.10↓ 3.99 ↓

0 61.23 79.56 81.58 28.67 16.24

59.29 55.41 54.76 54.70 54.71 55.18

45.38↓ 4.93 ↓

0 43.31 38.81 57.79 16.28 18.37 18.37

68.85 66.28 66.14 65.51 66.08 66.41 66.41

62.72↓ 8.28 ↓

I-BAU

ASR ACC

0 9.72 11.53 7.91 9.82 4.12 10.48 10.30 5.78 36.12 10.66 10.72 12.48 9.94

93.56 87.85 90.84 87.24 85.94 83.57 89.16 89.74 86.70 85.05 87.35 85.94 90.03 90.68

87.82 ↓ 3.95 ↓

0

96.18

2.72 94.55

4.13 94.30

3.04 93.17

2.79 93.28

2.64 93.10

5.82 95.54

4.87 93.98

8.63 94.50

95.11 ↓ 2.83 ↓

0 13.29 11.94 17.42 9.31 10.68

59.34 54.56 55.10 54.19 55.72 54.93

86.71 ↓ 4.98 ↓

0 21.87 25.74 27.45 15.37 21.64 21.64

74.21 69.46 69.35 68.61 70.02 69.70 69.70

75.22 ↓ 4.93 ↓

AWM

ASR ACC

0 4.34 2.13 5.41 4.42 0.90 3.35 2.46 1.89 38.81 22.31 2.96 4.57 10.60

93.80 86.17 88.93 86.45 84.60 83.38 88.41 87.72 84.18 85.58 87.06 89.45 89.59 90.88

91.32 ↓ 4.53 ↓

0

95.32

2.84 93.58

4.96 92.75

2.27 93.56

1.94 92.84

5.32 92.68

1.89 93.52

2.74 93.17

5.43 94.22

96.02 ↓ 3.59 ↓

0 31.44 38.23 41.37 27.68 36.52

59.08 54.81 54.28 53.78 54.11 55.02

64.19 ↓ 5.48 ↓

0 21.18 28.85 34.15 16.47 23.50 23.50

71.63 69.44 68.62 68.91 69.74 69.32 69.32

72.80 ↓ 5.15 ↓

NGF (Ours)

ASR ACC

0 1.86 0.38 2.64 2.79 0.12 1.17 1.61 1.04 24.60 6.21 2.38 4.24 7.14

94.10 88.32 91.17 84.21 86.10 84.16 90.97 90.19 88.37 85.97 86.96 89.65 90.18 91.84

95.01 ↓ 3.33 ↓

0

95.76

0.24 94.11

2.91 93.31

1.21 94.18

1.58 93.87

3.24 93.48

1.51 94.27

1.26 94.14

4.45 95.27

97.39 ↓ 2.79 ↓

0

59.67

2.34 55.84

3.38 54.87

1.58 54.98

2.81 54.63

4.06 55.40

96.40 ↓ 4.74 ↓

0

74.51

4.61 70.46

4.02 69.97

3.83 70.52

2.94 71.36

3.05 70.25

3.05 70.25

93.94 ↓ 3.85 ↓

of average drop in ASR and ACC over all attacks. A highly effective method should have a high drop in ASR with a low drop in ACC. We define ASR as the percentage of poison test samples that are classified to the adversary-set target label.
B. Performance Evaluation of NGF
In Table II, we present the performance of different defenses for four different datasets. CIFAR10: We consider five label poisoning attacks: Badnets, Blend, TrojanNet, Dynamic, and BPPA. For TorjanNet, we consider two different variations based on label-mapping criteria: Troj-one and Troj-all. Regardless the complexity of the label-mapping type, our proposed method outperforms all other methods both in terms of ASR and ACC. We also create two variations for Dynamic attack: Dyn-one and Dyn-all. Dynamic attack optimizes for input-aware triggers that are capable of

fooling the model; making it more challenging than the static trigger based attacks (Badnets, Blend and Trojan). However, NGF outperforms other methods by a satisfactory margin. We also consider attacks that does not change the label during trigger insertion, i.e., clean label attack. Two such attacks are CLB and SIG. For further validation of our proposed method, we use deep feature based attacks, CBA and FBA. Both of these attacks manipulates deep features for backdoor insertion. Compared to other defenses, NGF shows better effectiveness against these diverse set of attacks achieving an average drop of 95.01% in ASR while sacrificing an ACC of 3.33% for that. Table II also shows the performance of baseline methods such as I-BAU and AWM. AWM performs similarly as ANP and often struggles to remove the backdoor.
GTSRB: In case of GTSRB, almost all defenses perform similarly for Badnets and Trojan. This, however, does not hold

Max. Eignevalue, max ACC/ASR
Max. Eignevalue, max ACC/ASR

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021

7

800 600 400 200
00

Backdoor Insertion Benign Badnets TrojanNet CLB SIG
20Num40ber6o0f Ep8o0chs100 120

100 90 80 70 60 50 40
0

Backdoor Insertion
Badnets (ASR) TrojanNet (ASR) CLB (ASR) SIG (ASR) Benign(ACC) Badnets (ACC) TrojanNet (ACC) CLB (ACC) SIG (ACC)
20Num40ber6o0f Ep8o0chs100 120

5000 4000 3000

Backdoor Purification BTCSrILaGoBdjnaentNset

2000

1000

0 0 10Num20ber3o0f Ep4o0chs 50 60

Backdoor Purification

80

Badnets (ASR)

60

TrojanNet (ASR) CLB (ASR)

SIG (ASR)

40

Badnets (ACC) TrojanNet (ACC)

20

CLB (ACC) SIG (ACC)

0
0 10 Nu2m0ber 3o0f Epo4c0hs 50 60

(a) λmax vs. Epochs

(b) ACC/ASR vs. Epochs

(c) λmax vs. Epochs

(d) ACC/ASR vs. Epochs

Fig. 2: Loss Surface characteristics of a DNN during backdoor insertion and purification processes. a & b) As the joint optimization on clean and poison distribution progresses, i.e., high ACC & ASR, the loss surface becomes less and less smoother, i.e., high λmax). c & d) One can purify backdoor by gradually making the loss surface smoother. We use CIFAR10 dataset with four different attacks.

TABLE III: Performance comparison of NGF to other SGD-based optimizers. A more suitable sharpness-aware SGD-based optimizer is also considered here. However, NGF is far more effective in purifying backdoor (lower ASR) due to its consistent convergence to smooth minima. We use CIFAR10 dataset for these evaluations.

Defense
Attacks
Badnets Blend Trojan Dynamic SIG CLB

No Defense
ASR ACC
100 92.96 100 94.11 100 89.57 100 92.52 100 88.64 100 92.78

AdaGrad

ASR ACC

96.54 97.43 95.52 97.37 86.20 96.81

91.16 91.67 88.51 91.45 87.98 90.86

RMSProp

ASR ACC

98.33 95.41 94.87 93.50 86.31 95.53

91.73 92.21 88.02 91.12 87.74 90.96

Adam

ASR ACC

97.68 94.79 96.74 96.90 85.66 95.87

91.45 92.15 87.98 91.40 87.75 91.02

SAM

ASR ACC

91.08 89.25 92.15 92.24 81.68 91.04

90.12 91.11 88.33 90.79 88.04 90.97

NGF (Ours)
ASR ACC
1.86 88.32 0.38 91.17 2.64 84.21 1.17 90.97 0.31 83.14 1.04 88.37

TABLE IV: Avg. runtime comparison for different datasets. their implementations), it is challenging to obtain satisfactory Here, #Parameters is the total number of parameters in the last attack success rates for Tiny-ImageNet and ImageNet. layer. An NVIDIA RTX 3090 GPU is used for all experiments.

Dataset CIFAR10 GTSRB Tiny-ImageNet ImageNet

# Parameters 5120 22016 409.6K
2.048M

Method
FT NGF
FT NGF
FT NGF
FT NGF

Runtime (Sec.)
78.1 38.3
96.2 47.4
637.6 374.2
2771.6 1681.4

for blend as we achieve an 2.17% ASR improvement over the next best method. The performance is consistent for other attacks as well. Overall, we record an average 97.39% ASR drop with only an 2.79% drop in ACC. In some cases, ACC for I-BAU are slightly better as it uses a much larger validation size (5%) for purification than other defense techniques.
ImageNet: For the scalability test of NGF, we consider two large and widely used datasets, Tiny-ImageNet and ImageNet. In consistence with other datasets, NGF obtains SOTA performance in these diverse datasets too. The effectiveness of ANP reduces significantly for this dataset. In case of large models and datasets, the task of identifying and pruning vulnerable neurons gets more complicated and may result in wrong neurons pruning. Note that, we report results for successful attacks only. For attacks such as Dynamic and BPPA (following

C. Ablation Studies
Smoothness Analysis of Different Attacks: We show the relationship between loss surface smoothness and backdoor insertion process in Fig. 2a and 2b. During backdoor insertion, the model is optimized for two different data distributions: clean and poison. Compared to a benign model, the loss surface of a backdoor becomes much sharper as the model becomes well optimized for both distributions, i.e., model has both high ASR and high ACC. Backdoor and benign models are far from being well-optimized at the beginning of training. The difference between these models is prominent once the model reaches closer to the final optimization point. As shown in Fig. 2b, the training becomes reasonably stable after 100 epochs with ASR and ACC near saturation level. Comparing λmax of benign and all backdoor models after 100 epochs, we notice a sharp contrast in Fig. 2a. This validates our previous claim on loss surface smoothness of benign and backdoor models. During the purification period, as shown in Fig. 2c and 2d, the model is optimized to a smoother minima. As a result, ASR becomes close to 0 while retaining good clean test performance. Note that, we calculate loss Hessian and λmax using all DNN parameters. This indicates that changing the parameters of only one layer impacts the loss landscape of the whole network. Even though the CNN-backbone parameters are frozen, NGF changes the last layer in a way such that the whole backdoor network behaves differently, i.e., like a benign model.

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021

8

TABLE V: Performance of NGF while fine-tuning all layers of DNN. We also consider SAM and SGD based fine-tuning of all layers here. The results shown here are for CIFAR10 dataset.

Methods

Badnets

Blend

Trojan

Dynamic

CLB

SIG

CBA

Runtime

ASR ACC ASR ACC ASR ACC ASR ACC ASR ACC ASR ACC ASR ACC (Secs.)

Initial

100 92.96 100 94.11 100 89.57 100 92.52 100 92.78 100 88.64 93.20 90.17

SGD (All Layers) 4.87 85.92 4.77 87.61 3.78 82.18 4.73 88.61 1.83 87.41 1.04 81.92 27.80 83.79

SAM (All Layers) 3.91 85.75 2.74 88.26 3.53 82.52 3.28 87.04 1.47 86.30 0.38 84.70 26.14 85.41

NGF (Last layer) 1.86 88.32 0.38 91.17 2.64 84.21 1.17 90.97 1.04 88.37 0.12 84.16 24.60 85.97

NGF (All layers) 1.47 88.65 0.42 92.28 2.05 84.61 1.06 90.42 0.60 88.74 0.18 85.12 19.86 86.30

– 78.1 116.3 38.3 173.2

TABLE VI: Performance of SGD-Long and NGF while fine-tuning only the last layer of DNN. For SGD-Long, we consider a long purification period with Ep = 2500. NGF performance with and without the regularization term underlines the importance of the proposed regularizer. The results shown here are for CIFAR10 dataset.

Methods
Initial SGD-Long NGF w/o Reg.
NGF

Badnets ASR ACC

100 82.34 1.91

92.96 90.68 87.65

1.86 88.32

Blend ASR ACC
100 94.11 7.13 92.46 0.31 90.54
0.38 91.17

Trojan ASR ACC

100 86.18 3.04

89.57 87.29 83.31

2.64 84.21

Dynamic ASR ACC

100 57.13 1.28

92.52 90.51 90.24

1.17 90.97

CLB ASR ACC

100 13.84 0.92

92.78 88.11 87.13

1.04 88.37

SIG ASR ACC
100 88.64 0.26 85.74 0.16 84.46
0.12 84.16

CBA ASR ACC

93.20 84.41 25.58

90.17 86.87 84.81

24.60 85.97

Runtime (Secs.)
– 907.5 37.8
38.3

TABLE VII: Evaluation of NGF on backdoor attacks with high poison rates, up to 50%. We consider CIFAR10 dataset and two closely performing defenses for this comparison.

Attack
Poison Rate
Method
No Defense ANP FT
NGF (Ours)

25%
ASR ACC
100 88.26 7.81 82.22 5.21 78.11 2.12 85.50

BadNets

35%

ASR ACC

100 16.35 8.39 2.47

87.43 80.72 74.06 84.88

50%

ASR ACC

100 29.80 11.52 4.53

85.11 78.27 69.81 82.32

25%

ASR ACC

100 29.96 1.41 0.83

86.21 82.84 68.73 80.62

Blend

35%

ASR ACC

100 47.02 4.56 1.64

85.32 78.34 63.87 79.62

50%

ASR ACC

100 86.29 7.97 2.21

83.28 69.15 55.70 76.37

25%

ASR ACC

100 11.96 3.98 3.02

87.88 76.28 76.99 83.10

Trojan

35%

ASR ACC

100 63.99 4.71 3.65

86.81 72.10 72.05 81.66

50%

ASR ACC

100 89.83 5.59 4.66

85.97 70.02 70.98 80.30

TABLE VIII: Purification performance for various validation data size. NGF performs well even with very few validation data, e.g., 50 data points. All results are for CIFAR10 and Badnets attack.

Validation size
Method
No Defense ANP AWM
NGF (Ours)

50

ASR CA

100 13.66 8.51 6.91

92.96 83.99 83.63 86.82

100
ASR CA
100 92.96 8.35 84.47 7.38 83.71 4.74 86.90

250
ASR CA
100 92.96 5.72 84.70 5.16 84.52 4.61 87.08

350
ASR CA
100 92.96 3.78 85.26 5.14 85.80 2.45 87.74

500
ASR CA
100 92.96 2.84 85.96 4.34 86.17 1.86 88.32

Evaluation of Different Optimizers: We compare the performance of NGF with different variants of first-order optimizers: (i) AdaGrad [62], (ii) RMSProp [63], (iii) Adam [64], and (iv) Sharpness-Aware Minimization (SAM) [65] is a recently proposed SGD-based optimizer that explicitly penalizes the abrupt changes of loss surface by bounding the search space within a small region. This forces the changes of model parameters so that the optimization achieves a smoother loss surface. Table III shows that NGF outperforms all of these variants of first-order optimizer by a huge margin. At the same time, the proposed method achieves comparable clean test performance. Although SAM usually performs better than vanilla SGD in terms of smooth DNN optimization, SAM’s performance in shallow network scenarios (i.e., our case) is almost similar to vanilla SGD. Two potential reasons behind this poor performance are (i) using a predefined local area to search for maximum loss, and (ii) using the ‘Euclidean distance’ metric instead of the geometric distance metric. In contrast, NGD with curvature geometry aware Fisher Information Matrix can successfully avoid such bad minima and optimizes to global minima.

Runtime Analysis: In Table IV, we show the average runtime for different defenses. Similar to purification performance, purification time is also an important indicator to measure the success of a defense technique. In Section VII-B, we already show that our method outperforms other defenses in most of the settings. As for the run time, our method completes the purification (for CIFAR10) in just 38.3 seconds; which is almost half as compared to FT. The time advantage of our method also holds for large datasets and models, e.g., ImageNet and ResNet50. Runtime comparison with other defenses is in the supplementary material.
Fine-tuning All Layers: We have considered fine-tuning all layers fusing NGF and SGD. Note that vanilla FT does finetune all layers. We report the performance of NGF for all layers in Table V. While fine-tuning all layers seems to improve the performance, it takes almost 6× more computational time than NGF on the last layer. We also show the results of SAM and SGD while fine-tuning all layers: we term them as vanilla FT (SAM) and vanilla FT (SGD). SAM has a slightly better ASR performance compared to SGD, which aligns with our

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021

9

smoothness hypothesis as SAM usually leads to smoother loss surface. As for execution time, each SAM update requires 2 backpropagation operations while non-SAM update (SGD, Adam, etc.) requires only 1 backpropagation. This makes vanilla FT (SAM) slower than vanilla FT (SGD) which is not desirable for backdoor purification techniques.
Effect of Proposed Regularizer: In this section, we analyze the effect of regularizer and long training with SGD. The effect of our clean distribution-aware regularizer can be observed in Table VI. NGF with the proposed regularizer achieves a 1% clean test performance improvement over vanilla NGF. For long training with SGD (SGD-Long), we fine-tune the last layer for 2500 epochs. Table VI shows the evaluations of SGDLong on 7 different attacks. Even though the ASR performance improves significantly for CLB and SIG attacks, SGD-based FT still severely underperforms for other attacks. Moreover, the computational time increases significantly over NGF. Thus, our choice of NGD-based FT as a fast and effective backdoor purification technique is well justified.

accommodate this significant change in prediction, the model must adjust itself accordingly. Such adjustment leads to nonsmoothness in the weight-loss surface. A non-smooth surface causes significant changes in loss gradient for specific inputs. In our case, these specific inputs are backdoor-triggered samples. As the magnitude of a trigger is usually very small compared to the total input magnitude, the model has to experience quite a significant change in its weight space to cause large loss changes. We characterize this change in terms of smoothness. As for backdoor removal, we claim that making the non-smooth weight loss surface smoother removes the backdoor behavior. Based on the above discussion, a smoother surface should not cause a large change in loss or model predictions corresponding to backdoor-related perturbations or triggers. In summary, for a model to show certain backdoor behavior, there are some specific changes that take place in the weight space. In this work, we try to explain these changes regarding weight-loss surface smoothness. Our comprehensive empirical evaluations support our intuition well.

Effect of Clean Validation Data Size: We also present how the total number of clean validation data can impact the purification performance. In Table VIII, we see the change in performance while gradually reducing the validation size from 1% to 0.1%. We consider Badnets attack on the CIFAR10 dataset for this evaluation. Even with only 50 (0.1%) data points, NGF can successfully remove the backdoor by bringing down the attack success rate (ASR) to 6.91%. We also consider AWM performance for this comparison. For both ANP and AWM, reducing the validation size has a severe impact on test accuracy (ACC).
Strong Backdoor Attacks: By increasing the poison rates, we create stronger versions of different attacks against which most defense techniques fail quite often. We use 3 different poison rates, {25%, 35%, 50%}. We show in Table VII that NGF is capable of defending very well even with a poison rate of 50%, achieving a significant ASR improvement over FT. Furthermore, there is a sharp difference in classification accuracy between NGF and other defenses. For 25% Blend attack, however, ANP offers a slightly better performance than our method. However, ANP performs poorly in removing the backdoor as it obtains an ASR of 29.96% compared to 0.83% for NGF.
VIII. DISCUSSION
Why Smoothness is Key to Removing Backdoor? One key observation from the smoothness study is that: there exists a key difference between weight-loss surface smoothness (estimated by loss hessian) of a backdoor and a benign model w.r.t. clean distribution—the weight-loss surface of a backdoor model is less smooth compared to a benign model. To further elaborate, let us consider feeding a clean sample to a backdoor model. By definition, it will predict the correct ground truth label. Now, consider feeding a sample with a backdoor trigger on it. The model will predict the adversary-set target label implying significant changes in prediction distribution. This significant change can be explained by the surface smoothness. In order to

Why the Classification Layer? We further offer an explanation as to why we choose to fine-tune the classification layer instead of any other layer, e.g. input layer. The classification layer is mostly responsible for the final prediction in a DNN. Depending on the extracted features by the CNN backbone, the classifier learns the decision boundary between these features and renders a prediction. While backdooring we change the input features slightly (by inserting triggers) so that the classifier makes a wrong prediction. If we can make the classifier invariant to these slight input changes, the effect of the backdoor should be removed. Thus, compared to other layers of the DNN, the classifier plays a more important role in the overall backdoor insertion and removal process. Another reason for fine-tuning only the last layer is for better computational efficiency; which is one of the most important aspects of a backdoor defense technique.
Why Different Metrics for Smoothness and NGF? It is natural to probe that the same technique—either the Hessian of loss or the Fisher Matrix—could be used for both our smoothness analysis and the development of the proposed method. However, our particular choice is driven by the tradeoff related to these two matrics—computational efficiency and performance. Note that computing Hessian is more expensive than the FIM. On the other hand, Hessian is a slightly better indicator of smoothness due to its superior effectiveness in capturing loss-surface geometry. Therefore, we can choose either one of the metrics. Since smoothness analysis is performed in an offline manner and only once (for each instance), we choose the better performing one, i.e. Hessian of loss, for smoothness analysis. As to why we choose the Fisher matrix in developing our proposed method, we need to design a runtime-efficient method with good performance. Since we have to calculate either FIM or Hessian in each iteration of the update, it becomes harder to choose Hessian over FIM for the development of the proposed method. Given the potential of a higher trade-off value of the Fisher-information matrix, we develop our method based on it.

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021

10

Why Fine-tuning Negatively Impacts ACC? It is observable that no matter which defense techniques we use the clean test accuracy consistently drops for all datasets. We offer an explanation for fine-tuning based techniques as NGF is one of them. As we use a small validation set for fine-tuning, it does not necessarily cover the whole training data distribution. Therefore, fine-tuning with this small amount of data bears the risk of overfitting and reduced clean test accuracy. This is more prominent when we fine-tune all layers of the network (vanilla FT in Table 2). Whereas, NGF fine-tunes only 1 layer which shows to be better in terms of preserving clean test accuracy.
IX. CONCLUSION
We propose a novel backdoor purification technique based on natural gradient descent fine-tuning. The proposed method is motivated by our analysis of loss surface smoothness and its strong correlation with the backdoor insertion and purification processes. As a backdoor model has to learn an additional data distribution, it tends to be optimized to bad local minima or sharper minima compared to a benign model. We argue that the backdoor can be removed by re-optimizing the model to a smoother minima. We further argue that fine-tuning a single layer is enough to remove the backdoor. Therefore, in order to achieve a smooth minima in a single-layer fine-tuning scenario, we propose using an FIM-based DNN objective function and minimizing it using a curvature-aware NGD optimizer. Our proposed method achieves SOTA performance in a wide range of benchmarks. Since we fine-tune only one layer the training time overhead reduces significantly, making our method one of the fastest among SOTA defenses. Limitations and future works. Our extensive empirical studies on loss surface smoothness show its relationship with backdoor insertion and removal. However, we left the mathematical analysis of this relationship for future studies. Such analysis should be able to address the nature of convergence under different purification settings, e.g., the number of validation samples, number of iterations, number of fine-tuning layers, etc. Although we verify the smoothness hypothesis empirically, the mathematical analysis will give us more insight into understanding the backdooring process. Although we only experimented with CNN-based architectures, our findings should also hold for attention-based vision transformer (ViT) [66] architecture. Nevertheless, further study is required to verify the smoothness claims for the ViT architecture. It is known that the attention mechanism and residual connection generally lead the optimization towards smooth minima. However, how the backdooring process interferes with this optimization must be explored properly. In future, we aim to extend our smoothness analysis to 3D point-cloud attacks as well as contrastive backdoor attacks.
REFERENCES
[1] Y. Liu, S. Ma, Y. Aafer, W.-C. Lee, J. Zhai, W. Wang, and X. Zhang, “Trojaning attack on neural networks,” 2017.
[2] K. He, X. Zhang, S. Ren, and J. Sun, “Identity mappings in deep residual networks,” in European conference on computer vision. Springer, 2016, pp. 630–645.

[3] T. Gu, K. Liu, B. Dolan-Gavitt, and S. Garg, “Badnets: Evaluating backdooring attacks on deep neural networks,” IEEE Access, vol. 7, pp. 47 230–47 244, 2019.
[4] X. Chen, C. Liu, B. Li, K. Lu, and D. Song, “Targeted backdoor attacks on deep learning systems using data poisoning,” arXiv preprint arXiv:1712.05526, 2017.
[5] K. Liu, B. Dolan-Gavitt, and S. Garg, “Fine-pruning: Defending against backdooring attacks on deep neural networks,” in International Symposium on Research in Attacks, Intrusions, and Defenses. Springer, 2018, pp. 273–294.
[6] B. Wang, Y. Yao, S. Shan, H. Li, B. Viswanath, H. Zheng, and B. Y. Zhao, “Neural cleanse: Identifying and mitigating backdoor attacks in neural networks,” in 2019 IEEE Symposium on Security and Privacy (SP). IEEE, 2019, pp. 707–723.
[7] D. Wu and Y. Wang, “Adversarial neuron pruning purifies backdoored deep models,” in NeurIPS, 2021.
[8] Y. Li, X. Lyu, N. Koren, L. Lyu, B. Li, and X. Ma, “Anti-backdoor learning: Training clean models on poisoned data,” Advances in Neural Information Processing Systems, vol. 34, 2021.
[9] R. Zheng, R. Tang, J. Li, and L. Liu, “Data-free backdoor removal based on channel lipschitzness,” arXiv preprint arXiv:2208.03111, 2022.
[10] A. Choromanska, M. Henaff, M. Mathieu, G. B. Arous, and Y. LeCun, “The loss surfaces of multilayer networks,” in Artificial intelligence and statistics. PMLR, 2015, pp. 192–204.
[11] S.-I. Amari, “Natural gradient works efficiently in learning,” Neural computation, vol. 10, no. 2, pp. 251–276, 1998.
[12] J. Martens and R. Grosse, “Optimizing neural networks with kronecker-factored approximate curvature,” in International conference on machine learning. PMLR, 2015, pp. 2408–2417.
[13] W. Guo, B. Tondi, and M. Barni, “An overview of backdoor attacks against deep neural networks and possible defences,” IEEE Open Journal of Signal Processing, 2022.
[14] N. Manoj and A. Blum, “Excess capacity and backdoor poisoning,” Advances in Neural Information Processing Systems, vol. 34, pp. 20 373–20 384, 2021.
[15] Y. Li, B. Wu, Y. Jiang, Z. Li, and S.-T. Xia, “Backdoor learning: A survey,” arXiv preprint arXiv:2007.08745, 2020.
[16] B. Tran, J. Li, and A. Madry, “Spectral signatures in backdoor attacks,” Advances in neural information processing systems, vol. 31, 2018.
[17] M. Barni, K. Kallas, and B. Tondi, “A new backdoor attack in cnns by training set corruption without label poisoning,” in 2019 IEEE International Conference on Image Processing (ICIP). IEEE, 2019, pp. 101–105.
[18] H. Zhong, C. Liao, A. C. Squicciarini, S. Zhu, and D. Miller, “Backdoor embedding in convolutional neural network models via invisible perturbation,” in Proceedings of the Tenth ACM Conference on Data and Application Security and Privacy, 2020, pp. 97–108.
[19] Y. Liu, X. Ma, J. Bailey, and F. Lu, “Reflection backdoor:

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021

11

A natural backdoor attack on deep neural networks,” in European Conference on Computer Vision. Springer, 2020, pp. 182–199. [20] Q. Zhang, Y. Ding, Y. Tian, J. Guo, M. Yuan, and Y. Jiang, “Advdoor: adversarial backdoor attack of deep learning system,” in Proceedings of the 30th ACM SIGSOFT International Symposium on Software Testing and Analysis, 2021, pp. 127–138. [21] A. Saha, A. Subramanya, and H. Pirsiavash, “Hidden trigger backdoor attacks,” in Proceedings of the AAAI conference on artificial intelligence, vol. 34, no. 07, 2020, pp. 11 957–11 965. [22] A. Nguyen and A. Tran, “Wanet–imperceptible warpingbased backdoor attack,” arXiv preprint arXiv:2102.10369, 2021. [23] Z. Wang, J. Zhai, and S. Ma, “Bppattack: Stealthy and efficient trojan attacks against deep neural networks via image quantization and contrastive adversarial learning,” in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2022, pp. 15 074–15 084. [24] A. Turner, D. Tsipras, and A. Madry, “Clean-label backdoor attacks,” 2018. [25] K. Huang, Y. Li, B. Wu, Z. Qin, and K. Ren, “Backdoor defense via decoupling the training process,” arXiv preprint arXiv:2202.03423, 2022. [26] S. Zhao, X. Ma, X. Zheng, J. Bailey, J. Chen, and Y.-G. Jiang, “Clean-label backdoor attacks on video recognition models,” in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2020, pp. 14 443–14 452. [27] A. Saha, A. Tejankar, S. A. Koohpayegani, and H. Pirsiavash, “Backdoor attacks on self-supervised learning,” in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2022, pp. 13 337–13 346. [28] X. Qiao, Y. Yang, and H. Li, “Defending neural backdoors via generative distribution modeling,” Advances in neural information processing systems, vol. 32, 2019. [29] W. Guo, L. Wang, Y. Xu, X. Xing, M. Du, and D. Song, “Towards inspecting and eliminating trojan backdoors in deep neural networks,” in 2020 IEEE International Conference on Data Mining (ICDM). IEEE, 2020, pp. 162–171. [30] G. Shen, Y. Liu, G. Tao, S. An, Q. Xu, S. Cheng, S. Ma, and X. Zhang, “Backdoor scanning for deep neural networks through k-arm optimization,” in International Conference on Machine Learning. PMLR, 2021, pp. 9525–9536. [31] Y. Dong, X. Yang, Z. Deng, T. Pang, Z. Xiao, H. Su, and J. Zhu, “Black-box detection of backdoor attacks with limited information and data,” in Proceedings of the IEEE/CVF International Conference on Computer Vision, 2021, pp. 16 482–16 491. [32] J. Guo, A. Li, and C. Liu, “Aeva: Black-box backdoor detection using adversarial extreme value analysis,” arXiv preprint arXiv:2110.14880, 2021. [33] Z. Xiang, D. J. Miller, and G. Kesidis, “Post-training detection of backdoor attacks for two-class and multiattack scenarios,” arXiv preprint arXiv:2201.08474, 2022.

[34] G. Tao, G. Shen, Y. Liu, S. An, Q. Xu, S. Ma, P. Li, and X. Zhang, “Better trigger inversion optimization in backdoor scanning,” in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2022, pp. 13 368–13 378.
[35] Y. Gao, C. Xu, D. Wang, S. Chen, D. C. Ranasinghe, and S. Nepal, “Strip: A defence against trojan attacks on deep neural networks,” in Proceedings of the 35th Annual Computer Security Applications Conference, 2019, pp. 113–125.
[36] H. Chen, C. Fu, J. Zhao, and F. Koushanfar, “Deepinspect: A black-box trojan detection and mitigation framework for deep neural networks.” in IJCAI, vol. 2, no. 5, 2019, p. 8.
[37] P. Zhao, P.-Y. Chen, P. Das, K. N. Ramamurthy, and X. Lin, “Bridging mode connectivity in loss landscapes and adversarial robustness,” arXiv preprint arXiv:2005.00060, 2020.
[38] Y. Li, X. Lyu, N. Koren, L. Lyu, B. Li, and X. Ma, “Neural attention distillation: Erasing backdoor triggers from deep neural networks,” arXiv preprint arXiv:2101.05930, 2021.
[39] S. Hong, V. Chandrasekaran, Y. Kaya, T. Dumitras¸, and N. Papernot, “On the effectiveness of mitigating data poisoning attacks with gradient shaping,” arXiv preprint arXiv:2002.11497, 2020.
[40] M. Du, R. Jia, and D. Song, “Robust anomaly detection and backdoor attack detection via differential privacy,” arXiv preprint arXiv:1911.07116, 2019.
[41] E. Borgnia, V. Cherepanova, L. Fowl et al., “Strong data augmentation sanitizes poisoning and backdoor attacks without an accuracy tradeoff,” in ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2021, pp. 3855–3859.
[42] B. G. Doan, E. Abbasnejad, and D. C. Ranasinghe, “Februus: Input purification defense against trojan attacks on deep neural network systems,” in Annual Computer Security Applications Conference, 2020, pp. 897–912.
[43] P. W. Koh and P. Liang, “Understanding black-box predictions via influence functions,” in International conference on machine learning. PMLR, 2017, pp. 1885–1894.
[44] S. Ma and Y. Liu, “Nic: Detecting adversarial samples with neural network invariant checking,” in Proceedings of the 26th network and distributed system security symposium (NDSS 2019), 2019.
[45] I. Diakonikolas, G. Kamath, D. Kane, J. Li, J. Steinhardt, and A. Stewart, “Sever: A robust meta-algorithm for stochastic optimization,” in International Conference on Machine Learning. PMLR, 2019, pp. 1596–1606.
[46] J. Steinhardt, P. W. W. Koh, and P. S. Liang, “Certified defenses for data poisoning attacks,” Advances in neural information processing systems, vol. 30, 2017.
[47] H. Park, S.-I. Amari, and K. Fukumizu, “Adaptive natural gradient learning algorithms for various stochastic models,” Neural Networks, vol. 13, no. 7, pp. 755–764, 2000.
[48] S. Jastrzebski, M. Szymczak, S. Fort, D. Arpit, J. Tabor, K. Cho, and K. Geras, “The break-even point on optimization trajectories of deep neural networks,” arXiv

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2021

12

preprint arXiv:2002.09572, 2020. [49] J. Kirkpatrick, R. Pascanu, N. Rabinowitz et al., “Over-
coming catastrophic forgetting in neural networks,” Proceedings of the national academy of sciences, vol. 114, no. 13, pp. 3521–3526, 2017. [50] A. Krizhevsky, G. Hinton et al., “Learning multiple layers of features from tiny images,” 2009. [51] J. Stallkamp, M. Schlipsing, J. Salmen, and C. Igel, “The german traffic sign recognition benchmark: a multi-class classification competition,” in The 2011 international joint conference on neural networks. IEEE, 2011, pp. 1453– 1460. [52] Y. Le and X. Yang, “Tiny imagenet visual recognition challenge,” CS 231N, vol. 7, no. 7, p. 3, 2015. [53] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. FeiFei, “Imagenet: A large-scale hierarchical image database,” in CVPR. IEEE, 2009, pp. 248–255. [54] T. A. Nguyen and A. Tran, “Input-aware dynamic backdoor attack,” Advances in Neural Information Processing Systems, vol. 33, pp. 3454–3464, 2020. [55] J. Lin, L. Xu, Y. Liu, and X. Zhang, “Composite backdoor attack for deep neural network by mixing existing benign features,” in CCS, 2020, pp. 113–131. [56] S. Cheng, Y. Liu, S. Ma, and X. Zhang, “Deep feature space trojan attack of neural networks by controlled detoxification,” in AAAI, vol. 35, no. 2, 2021, pp. 1148– 1156. [57] Y. Li, Y. Li, B. Wu, L. Li, R. He, and S. Lyu, “Invisible backdoor attack with sample-specific triggers,” in Proceedings of the IEEE/CVF International Conference on Computer Vision, 2021, pp. 16 463–16 472. [58] S. Zagoruyko and N. Komodakis, “Wide residual networks,” arXiv preprint arXiv:1605.07146, 2016. [59] Y. Zeng, S. Chen, W. Park, Z. M. Mao, M. Jin, and R. Jia, “Adversarial unlearning of backdoors via implicit hypergradient,” arXiv preprint arXiv:2110.03735, 2021. [60] S. Chai and J. Chen, “One-shot neural backdoor erasing via adversarial weight masking,” arXiv preprint arXiv:2207.04497, 2022. [61] Y. Liu, Y. Xie, and A. Srivastava, “Neural trojans,” in 2017 IEEE International Conference on Computer Design (ICCD). IEEE, 2017, pp. 45–48. [62] J. Duchi, E. Hazan, and Y. Singer, “Adaptive subgradient methods for online learning and stochastic optimization.” Journal of machine learning research, vol. 12, no. 7, 2011. [63] G. Hinton, N. Srivastava, and K. Swersky, “Rmsprop: Divide the gradient by a running average of its recent magnitude. coursera: Neural networks for machine learning.” [64] D. P. Kingma and J. Ba, “Adam: A method for stochastic optimization,” arXiv preprint arXiv:1412.6980, 2014. [65] P. Foret, A. Kleiner, H. Mobahi, and B. Neyshabur, “Sharpness-aware minimization for efficiently improving generalization,” in International Conference on Learning Representations, 2021. [66] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner, M. Dehghani, M. Minderer,

G. Heigold, S. Gelly et al., “An image is worth 16x16 words: Transformers for image recognition at scale,” arXiv preprint arXiv:2010.11929, 2020.
Nazmul Karim received his B.S. degree in electrical engineering from the Bangladesh University of Engineering and Technology, Dhaka, Bangladesh, in 2016. He is currently working toward the Ph.D. degree in electrical engineering at the University of Central Florida. His current research interests lie in the areas of machine learning, signal processing, and linear algebra. Nazmul’s awards and honors include the University of Central Florida Multidisciplinary Doctoral Fellowship and Bangladesh University of Engineering and Technology Merit Scholarship.
Abdullah Al Arafat is currently pursuing his Ph.D. in Computer Science at the North Carolina State University, Raleigh, North Carolina, USA. He received his BS in Electrical Engineering from Bangladesh University of Engineering and Technology (BUET), Bangladesh in 2016 and MS in Computer Engineering from the University of Central Florida, Orlando, Florida, USA in 2020. His research interests include Scheduling Theory, Algorithms, and Real-Time & Intelligent Systems.
Umar Khalid is now a Ph.D. candidate at center of research in Computer Vision, University of Central Florida, USA. He received his master’s degree from the prestigious Shanghai Jiao Tong University and his bachelor’s degree from the National University of Science and Technology of Pakistan. His research interests include AI security, Federated Learning, and video understanding.
Zhishan Guo (Senior Member, IEEE) is an Associate Professor with the Department of Computer Science, North Carolina State University, Raleigh, NC, USA. He received the B.Eng. degree (with honor) in computer science and technology from Tsinghua University, Beijing, China, in 2009, the M.Phil. degree in mechanical and automation engineering from The Chinese University of Hong Kong, Hong Kong, in 2011, and the Ph.D. degree in computer science from the University of North Carolina at Chapel Hill, Chapel Hill, NC, USA, in 2016. His current research interests include real-time and cyber-physical systems, neural networks, and computational intelligence. He has received best paper awards from flagship conferences such as RTSS and EMSOFT.
Nazanin Rahnavard (S’97-M’10, SM’19) received her Ph.D. in the School of Electrical and Computer Engineering at the Georgia Institute of Technology, Atlanta, in 2007. She is a Professor in the Department of Electrical and Computer Engineering at the University of Central Florida, Orlando, Florida. Dr. Rahnavard is the recipient of NSF CAREER award in 2011. She has interest and expertise in a variety of research topics in deep learning, communications, networking, and signal processing areas. She serves on the editorial board of the Elsevier Journal on Computer Networks (COMNET) and on the Technical Program Committee of several prestigious international conferences.

