Backdoor Defense via Adaptively Splitting Poisoned Dataset
Kuofeng Gao1*, Yang Bai 2*, Jindong Gu3â€ , Yong Yang4, Shu-Tao Xia15â€  1 Tsinghua University 2 Tencent Security Zhuque Lab 3 University of Oxford
4 Tencent Security Platform Department 5 Peng Cheng Laboratory
gkf21@mails.tsinghua.edu.cn, {mavisbai,coolcyang}@tencent.com jindong.gu@eng.ox.ac.uk, xiast@sz.tsinghua.edu.cn

arXiv:2303.12993v1 [cs.CV] 23 Mar 2023

Abstract
Backdoor defenses have been studied to alleviate the threat of deep neural networks (DNNs) being backdoor attacked and thus maliciously altered. Since DNNs usually adopt some external training data from an untrusted third party, a robust backdoor defense strategy during the training stage is of importance. We argue that the core of training-time defense is to select poisoned samples and to handle them properly. In this work, we summarize the training-time defenses from a unified framework as splitting the poisoned dataset into two data pools. Under our framework, we propose an adaptively splitting datasetbased defense (ASD). Concretely, we apply loss-guided split and meta-learning-inspired split to dynamically update two data pools. With the split clean data pool and polluted data pool, ASD successfully defends against backdoor attacks during training. Extensive experiments on multiple benchmark datasets and DNN models against six state-ofthe-art backdoor attacks demonstrate the superiority of our ASD. Our code is available at https://github.com/ KuofengGao/ASD.
1. Introduction
Backdoor attacks can induce malicious model behaviors by injecting a small portion of poisoned samples into the training dataset with specific trigger patterns. The attacks have posed a significant threat to deep neural networks (DNNs) [44â€“46, 48, 51, 62], especially when DNNs are deployed in safety-critical scenarios, such as autonomous driving [18]. To alleviate the threats, backdoor defenses have been intensively explored in the community, which can be roughly grouped into post-processing defenses and training-time ones. Since the training data collection is usually time-consuming and expensive, it is common to
*Equal contribution. â€ Corresponding author.

Table 1. Summary of the representative training-time backdoor defenses under our framework.

Methods

# Pool

# Pool

# Pool # Clean Hard

Initialization Maintenance Operation Sample Selection

ABL

Fast

Static

Unlearn

No

DBD

Slow

Adaptive Purify

No

ASD (Ours) Fast

Adaptive Purify

Yes

use external data for training without security guarantees [27, 37, 42, 57, 58]. The common practice makes backdoor attacks feasible in real-world applications, which highlights the importance of training-time defenses. We argue that such defenses are to solve two core problems, i.e., to select poisoned samples and to handle them properly.
In this work, we formulate the training-time defenses into a unified framework as splitting the poisoned dataset into two data pools. Concretely, a clean data pool contains selected clean samples with trustworthy labels and a polluted data pool is composed of poisoned samples and remaining clean samples. Under this framework, the mechanisms of these defenses can be summarized into three parts, i.e., pool initialization, pool maintenance, and pool operation. To be more specific, they need to first initialize two data pools, deploy some data pool maintenance strategies, and take different training strategies on those split (clean and polluted) data pools. We illustrate our framework with two representative training-time defenses, i.e., anti-backdoor learning (ABL) [40] as well as decoupledbased defense (DBD) [33]. ABL statically initializes a polluted pool by the loss-guided division. The polluted pool is fixed and unlearned during training. Similarly, DBD initializes two data pools after computationally expensive training. Then the model is fine-tuned by semi-supervised learning with two dynamically updated data pools. (More details of these two methods are introduced in Sec. 2.2.)
Despite their impressive results, there is still room for improvement. ABL initializes two pools with static data selection, which raises the concern of mixing poisoned data

Proportion (%) Proportion (%) Proportion (%)

60

Clean samples

50

Poisoned samples

40

1

30

20

10

0 8 9 10 11

0 0 2 Lo4ss V6alue8 10

(a) ABL

25

Clean samples

20

Poisoned samples

15

3 2

10

1

5

09

10

0 0 2 Lo4ss Va6lue 8 10

(b) DBD

12

Clean samples

10

Poisoned samples

8

6

1

4

2

0 9 10

0 0 2 Lo4ss Va6lue 8 10

(c) ASD (Ours)

Figure 1. Loss distribution of samples on the model trained by ABL, DBD and our ASD against WaNet. Compared with ABL and DBD, our proposed ASD can clearly separate clean samples and poisoned ones better by a novel meta-split.

into clean ones. Once they are mixed, it is hard to alleviate it in the followed training process. Besides, unlearning poisoned data directly can lose some useful semantic features and degrade the modelâ€™s clean accuracy. As for DBD, its pool initialization is computationally expensive and is hard to be implemented end-to-end. Moreover, DBD adopts supervised learning for the linear layer in the whole poisoned dataset during the second stage, which can potentially implant the backdoor in models.
Under our framework, we introduce an adaptively splitting dataset-based defense (ASD). With two initialized data pools, we first adopt the loss-guided [68] split to update two data pools. However, some (model-dependent) clean hard samples can not be distinguished from poisoned ones directly by their loss magnitudes. As shown in Fig. 1, ABL and DBD adopting loss-guided split have failed to completely separate clean samples from poisoned samples. Instead, we propose a novel meta-learning-inspired split (meta-split), which can make a successful separation. Then we treat the clean data pool as a labeled data container and the polluted one as unlabeled, where we adopt semisupervised learning on two data pools. As such, we can utilize the semantic information of poisoned data without labels to keep the clean accuracy meanwhile to avoid backdoor injection, which can be regarded as a fashion of purifying poisoned samples. Note that, our ASD introduces clean seed samples (i.e., only 10 images per class) in pool initialization, which could be further extended to a transfer-based version, by collecting clean seed samples from another classical dataset. Given previous methods [41, 47, 67, 73] usually assume they can obtain much more clean samples than ours, our requirements are easier to meet. The properties of ABL, DBD and our ASD are briefly listed in Table 1.
In summary, our main contributions are three-fold:
â€¢ We provide a framework to revisit existing trainingtime backdoor defenses from a unified perspective, namely, splitting the poisoned dataset into a clean pool and a polluted pool. Under our framework, we propose an end-to-end backdoor defense, ASD, via splitting poisoned dataset adaptively.

â€¢ We propose a fast pool initialization method and adaptively update two data pools in two splitting manners, i.e., loss-guided split and meta-split. Especially, the proposed meta-split focuses on how to mine clean hard samples and clearly improves model performance.
â€¢ With two split data pools, we propose to train a model on the clean data pool with labels and the polluted data pool without using labels. Extensive experiment results demonstrate the superiority of our ASD to previous state-of-the-art backdoor defenses.
2. Related Work 2.1. Backdoor Attack
Backdoor attacks are often implemented by injecting a few poisoned samples to construct a poisoned dataset. Once a model is trained on the constructed poisoned dataset, the model will perform the hidden backdoor behavior, e.g., classifying samples equipped with trigger to the target label. Apart from the malicious behavior, the backdoored model behaves normally when the trigger is absent. Existing backdoor attacks can be divided into two categories: (1) poisonlabel backdoor attacks [3,27,43,70,79] connect the trigger with the target class by relabelling poisoned samples as target labels. Trigger patterns have been designed to enhance the attack strength [27, 54] or stealthiness [16, 50, 53]. (2) clean-label backdoor attacks [23,65] only poison the samples from the target class with labels unchanged. Although they are stealthier than poison-label attacks, clean-label attacks may fail to implant the model sometimes [38, 78]. 2.2. Backdoor Defense
Recent work has explored various methods to mitigate the backdoor threat. Existing backdoor defenses can be grouped into two categories: (1) post-processing backdoor defenses [24, 35, 75] aim to repair a local backdoored model with a set of local prepared data. A straightforward way is to reconstruct the trigger pattern [14, 19, 28, 63, 66] and then to unlearn the trigger pattern for repairing the model. Apart from the trigger-synthesis defenses, other methods are also widely used in erasing the backdoor, e.g., pruning [47, 73, 81], model distillation [41] and mode connectivity [77]. (2) training-time backdoor defenses [15, 64, 69] intend to train a clean model directly on the poisoned dataset. Anti-backdoor learning (ABL) [40] observes that the training losses of poisoned samples drop abruptly in the early training stage. Thus ABL proposes to first isolate a few samples with the lowest losses in early epochs, then train a model without isolated samples and finally unlearn isolated samples during the last few training epochs. Decouple-based backdoor defense (DBD) [33], first adopts self-supervised learning to obtain the feature extractor. Then it uses supervised learning to update the lin-

Clean Pool ğ·ğ·ğ¶ğ¶ with Low ğ¿ğ¿1(ğ‘“ğ‘“ğœƒğœƒ)

â€¦â€¦

Class 0

Class C-1

Polluted Pool ğ·ğ·ğ‘ƒğ‘ƒ with High ğ¿ğ¿1(ğ‘“ğ‘“ğœƒğœƒ)

Stage 1
ğ·ğ·ğ¶ğ¶ w/ Labels ğ·ğ·ğ‘ƒğ‘ƒ w/o Labels
ğ‘“ğ‘“ğœƒğœƒ(Â·) Class-aware Loss-guided Split

Clean Pool ğ·ğ·ğ¶ğ¶ with Low ğ¿ğ¿1(ğ‘“ğ‘“ğœƒğœƒ) Polluted Pool ğ·ğ·ğ‘ƒğ‘ƒ with High ğ¿ğ¿1(ğ‘“ğ‘“ğœƒğœƒ)

Stage 2

ğ·ğ·ğ¶ğ¶ w/ Labels ğ·ğ·ğ‘ƒğ‘ƒ w/o Labels

ğ‘“ğ‘“ğœƒğœƒ(Â·)

Class-agnostic Loss-guided Split

Clean Easy Samples Clean Hard Samples Stage 3 Clean Pool ğ·ğ·ğ¶ğ¶ with Low ğ¿ğ¿1 ğ‘“ğ‘“ğœƒğœƒ âˆ’ ğ¿ğ¿1(ğ‘“ğ‘“ğœƒğœƒâ€²)
Polluted Pool ğ·ğ·ğ‘ƒğ‘ƒ with High ğ¿ğ¿1 ğ‘“ğ‘“ğœƒğœƒ âˆ’ ğ¿ğ¿1(ğ‘“ğ‘“ğœƒğœƒâ€²)

Legend Poisoned Samples

ğ·ğ·ğ¶ğ¶ w/ Labels ğ·ğ·ğ‘ƒğ‘ƒ w/o Labels

ğ‘“ğ‘“ğœƒğœƒ(Â·)

Train on ğ·ğ· by ğ¿ğ¿2(ğ‘“ğ‘“ğœƒğœƒâ€²)

Virtual Model

Meta-Split

ğ‘“ğ‘“ğœƒğœƒâ€²(Â·)

by ğ¿ğ¿1 ğ‘“ğ‘“ğœƒğœƒ âˆ’ ğ¿ğ¿1(ğ‘“ğ‘“ğœƒğœƒâ€²)

Figure 2. Pipeline of our ASD. It mainly contains three stages. (1) Clean data pool DC is initialized with a few clean seed samples. Polluted data pool DP is initialized with the poisoned training dataset D. Then DC is supplemented by samples in D with the lowest L1(fÎ¸) losses, which are equally selected from each class, dubbed class-aware loss-guided split. (2) DC is substantially supplemented by samples with the lowest L1(fÎ¸) losses in the entire dataset D, dubbed class-agnostic loss-guided split. (3) A novel meta-split is proposed to add clean hard samples into DC and further improve the performance. Note that two data pools are adaptively updated at every epoch, where DC is directly supplemented and the remaining samples in D are divided as DP . During the whole process, labels in DC are preserved while labels in DP are removed. The model fÎ¸ is trained on DC and DP by semi-supervised learning following Eq. 2.

ear layer. After identifying clean samples by the symmetric cross-entropy loss, DBD conducts semi-supervised learning on the labeled clean data and unlabeled remaining data. Besides, adopting differential-privacy SGD [20] and strong data augmentation [10] can also defend against backdoor attacks to some degree. Our proposed ASD belongs to the training-time backdoor defense.
3. Preliminaries
Threat model. We adopt the poisoning-based threat model used in previous works [16,27,65], where attackers provide a poisoned training dataset containing a set of pre-created poisoned samples. Following previous training-time defenses [10,20,33,40], we assume that defenders can control the training process. Besides, a few clean samples of each class are available as seed samples. The goal of defenders is to obtain a well-performed model without suffering backdoor attacks. Problem formulation. Given a classification model fÎ¸ with randomly initialized parameters Î¸ and a training dataset D = {(xi, yi)}Ni=1, the training dataset D contains N samples xi âˆˆ Rd, i = 1, ..., N , and their ground-truth labels yi âˆˆ {0, 1, ..., C âˆ’ 1} where C is the number of classes. Poisoned samples might be included in D. Under our unified framework, we propose to divide the dataset

D into two disjoint data pools adaptively, i.e., a clean data pool DC with labels and a polluted data pool DP , whose labels will not be used. Moreover, we train the model on the clean data pool and polluted data pool in a semi-supervised learning-based manner by treating the polluted pool as unlabeled data, denoted as:

 \begin {aligned} \min _{\bm {\theta } \mathcal {L}\left (\mathcal {D}_{C}, \mathcal {D}_{P}; \bm {\theta }\right ), \end {aligned} \label {eq:formulation semi-supervised} 

(1)

where DC âŠ‚ D and DP = {x|(x, y) âˆˆ D\DC }. L(Â·) denotes the semi-supervised loss function. In view of previous semi-supervised learning methods [8,9,55,80], L(Â·) usually contains two losses:

 \begin {aligned} \mathcal {L}=\sum _{(\bm {x},y) \in \mathcal {D}_{C} \mathcal {L}_s(\bm {x}, y; \bm {\theta })+\lambda \sum _{\bm {x} \in \mathcal {D}_{P} \mathcal {L}_u(\bm {x}; \bm {\theta }), \end {aligned} \label {eq:formulation semi-supervised 2 los}  (2)

where the supervised loss Ls is adopted on the clean data pool DC with labels, the unsupervised loss Lu is used on the polluted data pool DP without using labels, Î» denotes the trade-off between Ls and Lu. Since Ls can obtain the precise relationship between images and labels. Hence, it is critical to ensure as many clean samples and few poisoned samples as possible in the clean data pool DC.
4. Proposed Backdoor Defense: ASD
In this section, we present the pipeline of our ASD. The key challenge is to adaptively update and maintain two

Proportion (%) Proportion (%) Proportion (%) Proportion (%)

Clean samples

8

Poisoned samples

6

4

2

0 0 2 Lo4ss Va6lue 8 10
(a)

35 30

Clean samples Poisoned samples

25

5

20

4

15

3 2

10 5

1 00

1

0 0 2 Lo4ss Va6lue 8 10

(b)

8 7

Clean samples Poisoned samples

6

5

4

3

2

1

0-10 -5Loss Re0ductio5n 10

(c)

8

Clean samples

7

Poisoned samples

6

5

4

3

2

1

0 0 2 Lo4ss Va6lue 8 10

(d)

Figure 3. Loss distribution of samples on the model after different operations, which motivates us to propose the meta-split method in stage 3. (a) The model fÎ¸ after previous two stages. The clean hard samples have similar high losses to the poisoned ones, which can not be separated by the loss-guided split. (b) The â€˜virtual modelâ€™ fÎ¸â€² in Fig. 3a after one-epoch supervised learning. The losses of poisoned samples reduce to zero, which shows the poisoned samples can be fast learned. (c) The loss reduction between fÎ¸ in Fig. 3a and fÎ¸â€² in Fig. 3b. (d) The model fÎ¸ after stage 3. Compared with Fig. 3a, poisoned samples and clean samples are better separated, especially for clean hard samples, which benefits from our proposed meta-split.

pools, i.e., a clean data pool DC and a polluted data pool DP . As shown in Fig. 2, our ASD is summarized in three stages: (1) We first initialize DC with several fixed clean seed samples and DP with the whole poisoned dataset D. We perform the warming-up training and update DC by class-aware loss-guided split. (2) Then, we adopt classagnostic loss-guided split on the entire dataset D and supplement DC to accelerate the defense process. (3) To add more clean hard samples into DC, we propose a metalearning-inspired (meta-split) method. During these three stages, DP is composed of all remaining samples in D except for those selected in DC . The model fÎ¸ is trained by Eq. 2. Algorithm of our ASD is shown in Appendix A. 4.1. Warming-up and training with loss-guided split
We introduce the first two stages of ASD. Both two stages utilize the loss-guided data split to update the clean data pool DC and the polluted data pool DP adaptively. To be more specific, given a model fÎ¸ at any epoch during stage 1 and stage 2, DC is supplemented by samples with the lowest L1(fÎ¸) losses, while DP is composed of remaining samples. As suggested in [33], symmetric cross-entropy (SCE) [68] loss is adopted as L1(Â·) because it can amplify the difference between clean samples and poisoned samples when compared with cross-entropy (CE) loss. Warming up with class-aware loss-guided split. In the warming-up stage, we first initial DC with the clean seed samples and DP with all the poisoned training data. Since only a few clean seed samples are available in DC, we progressively increase the number of samples in DC, namely we add n every t epochs in each class. Next, we add samples with the lowest L1(Â·) losses in each class to DC dynamically, and remaining samples are used as DP . The reason for the class-aware way is to prevent the performance collapse caused by the class imbalance in the small clean

data pool. Based on the tiny but progressively growing DC and its complement DP , the model will be warmed up according to Eq. 2 during the first T1 epochs. Training with class-agnostic loss-guided split. After the first stage, the model with certain accuracy can be used to better distinguish clean samples and poisoned samples. In the second stage, we further enlarge |DC| to accelerate the defense process. We directly add Î±% samples with the lowest L1(Â·) losses in the entire dataset into DC , and remaining samples are used as DP . Such a class-agnostic loss-guided data split method can avoid selecting poisoned samples into DC from the target class and further suppress the attack success rate. With two split data pools, we adopt Eq. 2 to train the model from epoch T1 to epoch T2.

4.2. Hard sample training with meta-split

After previous two stages,

there is still a small gap in clean 90

A Gap in ACC

accuracy between our ASD de- 80

ACC (%)

fended model and the normally 70

trained model (in Fig. 4). We assume that the gap is caused by that some clean samples can not be accessed by the loss-guided split. To verify our assumption, we calculate the losses of

60

50 40

OFirusrtAtwSDo dSutarginegs Normally Training

30 1

30 Epoch 60

90

Figure 4. The clean ac-

curacy of our ASD de-

all training data for BadNets at- fended model during pre-

tack [27]. In Fig. 3a, though the vious two stages and nor-

losses of most clean samples are mally trained model.

nearly zeros, there are still some

clean samples with high losses, which are similar to poi-

soned ones. Such phenomena indicate that most clean data

have been well learned while poisoned data are not by our

model after previous two stages. However, there are still

some (model-dependent) clean hard samples 1, which are difficult to be separated from poisoned ones just by loss magnitudes. We argue such clean hard samples are quite difficult to learn by a model yet poisoned samples can be easy to learn during supervised learning. Fig. 3b and Fig. 3c can verify our claim and show that losses of poisoned samples drop much faster than those of clean hard samples after one-epoch supervised learning, which inspires us to provide a new solution to successfully separate clean hard samples and poisoned ones.
To distinguish clean hard samples from poisoned ones and inspired by meta-learning [22, 30], we propose a novel manner to supplement DC, called meta-split. Given a model fÎ¸ at any epoch in the third stage, we first create a new â€˜virtual modelâ€™ fÎ¸â€² with the same parameters and architecture as fÎ¸. The virtual model fÎ¸â€² is updated on the entire poisoned dataset D by the loss L2(Â·) with learning rate Î² which can be denoted as:

 \begin {aligned} \bm {\theta }' &\gets \bm {\theta }, \ \bm {\theta }' \gets \bm {\theta }' -\beta &\nabla _{\bm {\theta }' \mathcal {L}_2(f_{\bm {\theta }'(\bm {x}),y), \end {aligned} \label {eq:meta-split} 

(3)

where cross-entropy (CE) loss is adopted as L2(Â·). Finally, Î³% samples with the least loss reduction L1(fÎ¸) âˆ’ L1(fÎ¸â€² ) are chosen to supplement DC .
Note that the virtual model fÎ¸â€² is only used for sample separation and is not involved in the followed training process. After supplementing DC by meta-split and consistently updating the model fÎ¸ using Eq. 2 until training end at epoch T3, fÎ¸ can successfully split more clean hard samples in DC and thus obtain higher accuracy. The loss distribution of samples on fÎ¸ at T3 in Fig. 3d shows clean samples and poisoned samples have been successfully separated, which further demonstrates the effectiveness of metasplit during the third stage of our ASD.

5. Experiments 5.1. Experimental Setups
Datasets and DNN models. We adopt three benchmark datasets to evaluate all the backdoor defenses, i.e., CIFAR10 [17], GTSRB [61] and an ImageNet [17] subset. ResNet18 [29] is set as the default model in our experiments. More details are listed in Appendix B.1. Moreover, we also provide the results on VGGFace2 dataset [11] and DenseNet121 [32] in Appendix C. Attack baselines and setups. We conduct six state-ofthe-art backdoor attacks, including BadNets [27], Blend backdoor attack (Blend) [16], Warping-based backdoor attack (WaNet) [53], Input-aware backdoor attack (IAB) [54],
1Here â€˜model-dependentâ€™ refers to model checkpoint-dependent , since clean hard samples are samples with the largest losses on the current model at each epoch.

Reflection-based attack (Refool) [50] and clean-label attack with adversarial perturbations (dubbed â€˜CLBâ€™) [65]. All these backdoor attacks are implemented as suggested in [33] and their original papers. Following [33], we choose 3 (yt = 3) as the target label and set the poisoned rate as 25% poisoned samples in the target class for the clean-label attack and 5% for other five backdoor attacks. More details about the attack setups are summarized in Appendix B.2. Besides, we also provide the results against the samplespecific attack [39] and all2all attack in Appendix D. Defense baselines and setups. We compare our proposed method with four existing backdoor defenses, including Fine-pruning (FP) [47], Neural Attention Distillation (NAD) [41], Anti-Backdoor Learning (ABL) [40] and decoupling-based backdoor defense (DBD) [33]. Since FP, NAD and ABL are sensitive to their hyper-parameters, we report their best results optimized by grid-search (See Appendix N). DBD is implemented based on the original paper [33]. Besides, FP and NAD are assumed to have access to 5% of the clean training data. Furthermore, we also provide the results of cutmix-based defense [10] and differential privacy SGD-based defense [20] in Appendix E.
For our ASD, we follow DBD [33] to adopt MixMatch [9] as our default semi-supervised method. The initial number of clean seed samples w is 10 in each class and it will increase n = 10 at every t = 5 epochs. After stage 1, the filtering rate Î±% is set to 50%. For meta-split, we choose Adam [34] optimizer with the learning rate Î² = 0.015 to perform one-epoch supervised learning on a virtual model. In particular, we only update the parameters of the last three layers. The discussions about the hyper-parameters for meta-split are demonstrated in Appendix I. On CIFAR10 and ImageNet, T1, T2 and T3 are chosen as 60, 90 and 120, and T3 is chosen as 100 on GTSRB. More details about the defense setups are in Appendix B.3. Evaluation metrics. We evaluate backdoor defenses by two widely used metrics, i.e., the accuracy on clean dataset (ACC) and the attack success rate (ASR). Specifically, ASR is the fraction of the samples from the non-target class with the trigger classified to the target label by the backdoored model. For a backdoor defense, the higher ACC and the lower ASR correspond to better performance.
5.2. Main Results
To verify the superiority of our backdoor defense, we summarize the ACC and ASR of five backdoor defenses against six backdoor attacks on three datasets in Table 2. We also report the time cost of three training-time defenses in Table 3. Table 2 shows that our ASD can achieve low ASRs meanwhile maintain high ACCs on three datasets. In particular, ASD can purify and better utilize poisoned samples, even outperforming â€˜No Defenseâ€™ on ImageNet on average. In comparison, although FP and NAD require a larger

Table 2. The clean accuracy (ACC %) and the attack success rate (ASR %) of five backdoor defenses against six backdoor attacks across three datasets, including CIFAR-10, GTSRB and ImageNet. Note that the results of FP, NAD and ABL are reported by grid-searching the best ones in different hyper-parameters. In contrast, the results of DBD and our ASD are provided in the same hyper-parameters. Best results among five defenses are highlighted in bold.

Dataset CIFAR-10 GTSRB ImageNet

Attack
BadNets Blend WaNet IAB Refool CLB Average BadNets Blend WaNet IAB Refool CLB Average BadNets Blend WaNet IAB Refool CLB Average

No Defense ACC ASR 94.9 100 94.1 98.3 93.6 99.9 94.2 100 93.8 98.2 94.4 99.9 94.2 99.4 97.6 100 97.2 99.4 97.2 100 97.3 100 97.5 99.8 97.3 100 97.4 99.9 79.5 99.8 82.5 99.5 79.1 98.9 78.2 99.6 80.6 99.9 80.1 42.8 80.0 90.1

FP

ACC ASR

93.9

1.8

92.9 77.1

90.4 98.6

89.3 98.1

92.1 86.1

90.2 92.8

91.5 75.8

84.2

0

91.4 68.1

92.5 21.4

86.9

0

91.5 0.2

93.6 99.3

90.0 31.5

70.3 1.6

63.4 9.5

58.2 84.4

58.7 84.2

61.4 10.3

73.2 38.3

64.2 38.1

NAD

ACC ASR

88.2 4.6

85.8 3.4

71.3 6.7

82.8 4.2

86.2 3.6

86.4 9.5

83.5 5.3

97.1

0.2

93.3 62.4

96.5 47.1

97.1 0.1

95.5 1.4

3.3

21.1

80.5 22.1

65.1 5.1

64.8 0.3

63.8 1.3

63.8 0.6

63.7 0.3

62.7 1.7

64.0 1.5

ABL

ACC ASR

93.8 1.1

91.9 1.6

84.1 2.2

93.4

5.1

82.7 1.3

86.6 1.3

88.7 2.1

97.1

0

97.1

0.5

97.0 0.4

97.4

0.6

96.2

0

90.4 2.3

95.9 0.6

83.1

0

82.6

0.7

74.9 1.1

81.7

0

76.2 0.2

82.8

0.8

80.2 0.5

DBD

ACC ASR

92.3

0.8

91.7

0.7

91.4

0

91.6 100

91.5 0.5

90.6

0.1

91.5 17.0

91.4

0

91.5 99.9

89.6

0

90.9 100

91.4 0.4

89.7 0.3

90.8 33.4

81.9 0.3

82.3 100

80.6 9.8

83.1

0

82.5 0.1

81.8

0

82.0 18.4

ASD (Ours)

ACC ASR

93.4 1.2

93.7

1.6

93.1

1.7

93.2

1.3

93.5

0

93.1

0.9

93.3

1.1

96.7

0

97.1

0.3

97.2

0.3

96.9

0

96.8

0

97.3

0

97.0

0.1

83.3

0.1

82.5

0.2

84.1

0.8

81.6 0.5

82.6

0

82.2

0

82.7

0.3

Table 3. The average training time (s) of ABL, DBD and our ASD.

Dataset CIFAR-10 GTSRB ImageNet

Method ABL DBD ASD (Ours) ABL DBD ASD (Ours) ABL DBD ASD (Ours)

Stage 1 740 30,000 4,980 520 23,000 4,920 900 180,000 13,500

Stage 2 2,450 80 2,490 1,750 57 2,460 2,940 350 6,750

Stage 3 10 15,770 2,520 5 15,580 830 15 42,750 6,780

Total 3,200 45,850 9,990 2,275 38,637 8,210 3,855 223,100 27,030

ACC (%) Number

4000

90

3500

3000

85

2500

2000

DBD Clean Hard Samples

80

1500

ASD Clean Hard Samples DBD Poisoned Samples

1000

ASD Poisoned Samples

75

DBD

DOBurDMweittah-split

70 1 20 4E0poc6h0 80 100

500
0
1 5 10 Ep1o5ch 20 25 30

(a)

(b)

amount of local clean samples (2,500) than ours (100), FP provides a limited reduction on ASR and NAD damages the ACC of the models by a large margin, which constrains their deployment in repairing the model. Table 3 shows that ABL requires the least training time among three trainingtime defenses but needs grid-search for hyper-parameters to defend against different attacks. Besides, when ABL encounters the WaNet or Refool, its static backdoor isolation samples can be mixed with some clean samples. Once ABL adopts these backdoor isolation samples to unlearn for the backdoored model, the ACC will drop. In contrast, our adaptive split can update two data pools dynamically, which stabilizes the defense process of our ASD.
DBD can achieve a low ASR in most cases but fails sometimes, e.g., when it defends the IAB attack on CIFAR10. The reason behind the observation is that DBD trains the linear layer on the whole poisoned dataset during stage 2 for 10 epochs, which introduces the risk to implant the

Figure 5. Combination and comparison between DBD and our ASD. The experiments (Â±std over 5 random runs) are conducted on CIFAR-10 for BadNets. (a) Combine DBD and our meta-split to converge faster. (b) The number of clean hard samples and poisoned samples in the clean data pool DC during the final 30 epochs. Our ASD can successfully select clean hard examples.
backdoor. Besides, DBD needs a larger amount of time than our ASD, e.g., 8 times our ASD on ImageNet. Stage 3 of DBD under our framework also splits the poisoned dataset into two data pools adaptively. Hence, after stage 3 of DBD for 10 epochs, we replace the original data split method in DBD with our proposed meta-split. As shown in Fig. 5a, stage 3 of DBD can be compressed from the original 190 epochs to 25 epochs and with 91+% ACC and 4âˆ’% ASR, which shows that our meta-split can help accelerate DBD. Moreover, we compare the number of clean hard samples and poisoned samples in DC during the final 30 epochs of

ACC Without Stage 1 ASR Without Stage 1

ACC Without Stage 2 ASR Without Stage 2

ACC Without Stage 3 ASR Without Stage 3

ACC ASD (Ours) ASR ASD (Ours)

100

ACC

100

100

100

80

80

80

80

ACC (%) / ASR (%) ACC (%) / ASR (%) ACC (%) / ASR (%) ACC (%) / ASR (%)

60

60

60

60

40

40

40

40

20

ASR

20

20

20

40

Epoch 0 1

20

40 Ep6o0ch 80 60100 120

(a) BadNets

0 1

20

40 Ep6o08ch0 80 100 120

(b) Blend

0 1

20 14000Ep6o0ch 80 100 120

(c) WaNet

0 1

12200

40 Ep6o0ch 80

100

120

(d) CLB

Figure 6. The clean accuracy (ACC %) and the attack success rate (ASR %) of different ablation studies for three stages in ASD on CIFAR-10 for four backdoor attacks, which shows the necessity of each stage in our ASD.

ACC t=1,n=1000 ASR t=1,n=1000

ACC t=5,n=1000 ASR t=5,n=1000

ACC t=1,n=10 ASR t=1,n=10

ACC t=5,n=10 (Defaulted ASD) ASR t=5,n=10 (Defaulted ASD)

100

100

ACC (%) / ASR (%) ACC (%) / ASR (%) ACC (%) / ASR (%) ACC (%) / ASR (%)

80

80

80

80

60

60

60

60

40

40

20

20

40

40

Collapse

20

20

40

Epoch 0 1

20

40 Ep6o0ch 8060100 120

0 1

20

40 Ep6o08c0h 80 100 120

0 1

20 1400 0Ep6o0ch 80 100 120

0 1

12200 40 Ep6o0ch 80

100

120

(a) BadNets

(b) Blend

(c) WaNet

(d) CLB

Figure 7. The clean accuracy (ACC %) and the attack success rate (ASR %) of different warming-up strategies in stage 1 on CIFAR-10 for four backdoor attacks. A smaller t and a larger n correspond to a faster warming-up. The t and n should be set carefully to progressively increase the number of samples in DC during stage 1 instead of building it in short time, which can prevent the collapse of ASD.

DBD and our ASD. Specifically, we choose 5,000 samples with the largest L1(Â·) losses chosen by the model as the clean hard samples. Fig. 5b demonstrates that our ASD can access much more clean hard samples than DBD and poisoned samples with a similar low scale. More results about the combination and comparison between DBD and our ASD are in Appendix F and G. 5.3. Ablation Study on Defense Settings
In summary, ASD is composed of three stages. Here we study the necessity of each stage by conducting the following experiments. Results are shown in Fig. 6. We set the poisoned rate of BadNets, Blend and WaNet as 20% and remain other settings unchanged. (1) Without Stage 1. The two data pools will approximate random initialization if we directly start our defense without stage 1. Fig. 6 indicates that the defense process will be completely disrupted with two randomly initialized data pools. (2) Without Stage 2. In stage 2, since less poisoned samples from the target class

will be introduced to a larger DC by class-agnostic lossguided split, it can rapidly promote the ACC and suppress the ASR by a large margin. The defaulted ASD can achieve a lower ASR and a higher ACC than that without stage 2. (3) Without stage 3. As shown in Fig. 6, ACC will achieve only about 80% without stage 3 owing to the lack of the model-dependent clean hard samples in DC. More results about the ablation study on attack settings and defense settings are shown in Appendix H and I. Different warming-up strategies. Compared with our default setting (t = 5 and n = 10), ACC can increase faster when building DC in shorter time, i.e., t is smaller and n is larger, as shown in Fig. 7. However, it can wrongly introduce a large number of poisoned samples into DC and result in the failure of our ASD, especially under WaNet and CLB. Hence, it is necessary to control the speed to build DC and constrain the number of samples in DC during stage 1. Different semi-supervised learning methods. We treat the samples in DP as unlabeled and apply semi-supervised

Table 4. The clean accuracy (ACC %) and the attack success rate (ASR %) on CIFAR-10 of our ASD implemented by different semi-supervised methods. Consistent satisfactory results show the stability of ASD.

Method MixMatch UDA ReMixMatch

BadNets ACC ASR 93.4 1.2 92.6 2.1 91.6 1.4

Blend ACC ASR 93.7 1.6 91.9 2.3 91.5 1.0

WaNet ACC ASR 93.1 1.7 92.5 1.6 91.9 0

CLB ACC ASR 93.1 0.9 92.1 2.8 91.1 0.1

Table 5. The clean accuracy (ACC %) and attack success rate (ASR %) on CIFAR-10 for different numbers of poisoned samples in the seed sample.

Poisoned Number

0

1

2

3

4

BadNets

ACC ASR

93.4 1.2

94.1 1.5

93.6 2.5

93.6 1.4

93.5 1.3

Blend

ACC 93.7 93.6 93.5 93.5 93.1

ASR 1.6

2.5

2.7

0.8

99.9

WaNet

ACC 93.1 93.6 93.7 93.4 93.5

ASR 1.7

1.4

2.2

4.1

5.4

CLB

ACC 93.1 93.5 91.3 93.7 93.2

ASR 0.9

1.3

2.5

1.5

98.9

learning to learn from both data pools. In this experiment, we show our ASD can work well with various semisupervised learning, e.g., UDA [74] and ReMixMatch [8]. We keep all settings unchanged. As shown in Table 4, UDA and ReMixMatch can still have similar robustness against backdoor attacks compared with MixMatch [9] under our proposed ASD. More details about these three semisupervised learning methods are in Appendix J.
5.4. Ablation Study on Seed Sample Selection In our method, we utilize the seed samples with 10 clean
samples per class to warm up the model. Here, we discuss the flexibility of the seed sample selection. (1) Seed samples contain a few poisoned samples. (2) Seed samples are from another classical dataset, e.g., ImageNet.
First, we discuss the case that some poisoned samples are introduced in the seed samples and the results are shown in Table 5. It can be seen that our ASD can still exceed 91% ACC and suppress the creation of backdoor even though the seed samples contain 1 âˆ¼ 3 poisoned samples. Meanwhile, as the poisoned number increases to 4, our ASD can also defend against BadNets and WaNet successfully. This illustrates that our method has certain resistance to the seed samples mixed with a few poisoned samples. Then, we introduce the transfer learning-based ASD when adopting the seed samples from another classical dataset as follows. Threat model. Considering a more realistic scenario, we cannot obtain any clean sample from the source dataset. Here, we specify the source training data as CIFAR-10. However, only 100 clean samples from the classical ImageNet dataset are available.

Table 6. The clean accuracy (ACC %) and the attack success rate (ASR %) on CIFAR-10 under the proposed transfer-based setting. Transfer learning-based ASD works well.

Method
Transfer-based pre-training Transfer-based ASD

BadNets ACC ASR 52.6 8.9
92.9 2.4

Blend ACC ASR 51.5 10.2
92.5 2.6

WaNet ACC ASR 53.4 9.3
92.5 3.5

CLB ACC ASR 52.6 9.5
92.1 2.8

Methods. We first assign the 100 ImageNet clean samples as DC and remove the labels of the entire poisoned CIFAR10 as DP and perform the semi-supervised learning for 10 epochs. Then we freeze the pre-trained backbone and finetune the linear layer on the entire poisoned dataset via supervised learning for 1 epoch. Finally, this model will be regarded as the initialized model of our ASD and other settings of our ASD remain unchanged. Results. As shown in Table 6, after the above transferbased pre-training, the model will achieve about 52% ACC and 9% ASR. By adopting this transfer-based initialized model, our ASD achieves 92+% ACC and 4âˆ’% ASR, which shows our ASD can obtain robustness against backdoor attacks without clean seed samples from the poisoned training dataset. More results are in Appendix I. 5.5. Resistance to Potential Adaptive Attacks
In the above experiments, we assume that attackers have no information about our backdoor defense. In this section, we consider a more challenging setting, where the attackers know the existence of our defense and can construct the poisoned dataset with an adaptive attack. Threat model for the attackers. Following existing work [16, 27, 65], we assume that the attackers can access the entire dataset and know the architecture of the victim model. However, the attackers can not control the training process after poisoned samples are injected into the training dataset. Methods. Our defense separates samples by the magnitude of the loss reduction in the final stage, so adaptive attacks should aim to minimize the difference in the loss reduction between clean samples and poisoned samples. First, the attackers train a clean model in advance. Then, since the gradient determines the loss reduction of the model [34,76], the trigger pattern can be optimized by minimizing the gradient for poisoned samples w.r.t the trained model and maximizing that for clean samples. Settings. We conduct experiments on CIFAR-10. Based on the clean model, we adopt projected gradient descent (PGD) [52] to optimize the trigger pattern for 200 iterations with a step size 0.001. Besides, we set the perturbation magnitude as 32/255 and the trigger size as 32Ã—32. Results. The adaptive attack can achieve 94.8% ACC and 99.8% ASR without any defense. However, this attack can

obtain 93.6% ACC and only 1.4% ASR under our ASD, which illustrates our defense can resist the adaptive attack. The probable reason is that the trigger pattern is optimized on the surrogate clean model and has low transferability. The details of this adaptive attack and another designed adaptive attack are stated in Appendix K and Appendix L.
6. Conclusion
In this paper, we revisit training-time backdoor defenses in a unified framework from the perspective of splitting the poisoned dataset into two data pools. Under our framework, we propose a backdoor defense via adaptively splitting the poisoned dataset. Extensive experiments show that our ASD can behave effectively and efficiently against six state-of-the-art backdoor attacks. Furthermore, we explore a transfer-based ASD to show the flexibility of seed sample selection in our method. In summary, we believe that our ASD can serve as an effective tool in the community to improve the robustness of DNNs against backdoor attacks.
Acknowledgement
This work is supported in part by the National Natural Science Foundation of China under Grant 62771248, Shenzhen Science and Technology Program (JCYJ20220818101012025), and the PCNL KEY project (PCL2021A07).
References
[1] Jean-Baptiste Alayrac, Jonathan Uesato, Po-Sen Huang, Alhussein Fawzi, Robert Stanforth, and Pushmeet Kohli. Are labels required for improving adversarial robustness? In NeurIPS, 2019. 18
[2] Jiawang Bai, Bin Chen, Yiming Li, Dongxian Wu, Weiwei Guo, Shu-tao Xia, and En-hui Yang. Targeted attack for deep hashing based retrieval. In ECCV, 2020. 13
[3] Jiawang Bai, Kuofeng Gao, Dihong Gong, Shu-Tao Xia, Zhifeng Li, and Wei Liu. Hardly perceptible trojan attack against neural networks with bit flips. In ECCV, 2022. 2
[4] Jiawang Bai, Li Yuan, Shu-Tao Xia, Shuicheng Yan, Zhifeng Li, and Wei Liu. Improving vision transformers by revisiting high-frequency components. In ECCV, 2022. 13
[5] Yang Bai, Yan Feng, Yisen Wang, Tao Dai, Shu-Tao Xia, and Yong Jiang. Hilbert-based generative defense for adversarial examples. In ICCV, 2019. 13
[6] Yang Bai, Yuyuan Zeng, Yong Jiang, Yisen Wang, Shu-Tao Xia, and Weiwei Guo. Improving query efficiency of blackbox adversarial attack. In ECCV, 2020. 13
[7] Yang Bai, Yuyuan Zeng, Yong Jiang, Shu-Tao Xia, Xingjun Ma, and Yisen Wang. Improving adversarial robustness via channel-wise activation suppressing. In ICLR, 2021. 13
[8] David Berthelot, Nicholas Carlini, Ekin D Cubuk, Alex Kurakin, Kihyuk Sohn, Han Zhang, and Colin Raffel. Remixmatch: Semi-supervised learning with distribution alignment and augmentation anchoring. In ICLR, 2020. 3, 8, 18

[9] David Berthelot, Nicholas Carlini, Ian Goodfellow, Nicolas Papernot, Avital Oliver, and Colin A Raffel. Mixmatch: A holistic approach to semi-supervised learning. In NeurIPS, 2019. 3, 5, 8, 13, 18
[10] Eitan Borgnia, Valeriia Cherepanova, Liam Fowl, Amin Ghiasi, Jonas Geiping, Micah Goldblum, Tom Goldstein, and Arjun Gupta. Strong data augmentation sanitizes poisoning and backdoor attacks without an accuracy tradeoff. In ICASSP, 2021. 3, 5, 14
[11] Qiong Cao, Li Shen, Weidi Xie, Omkar M Parkhi, and Andrew Zisserman. Vggface2: A dataset for recognising faces across pose and age. In FG. IEEE, 2018. 5, 12, 14
[12] Yair Carmon, Aditi Raghunathan, Ludwig Schmidt, John C Duchi, and Percy S Liang. Unlabeled data improves adversarial robustness. In NeurIPS, 2019. 18
[13] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for contrastive learning of visual representations. In ICML, 2020. 13
[14] Tianlong Chen, Zhenyu Zhang, Yihua Zhang, Shiyu Chang, Sijia Liu, and Zhangyang Wang. Quarantine: Sparsity can uncover the trojan attack trigger for free. In CVPR, 2022. 2
[15] Weixin Chen, Baoyuan Wu, and Haoqian Wang. Effective backdoor defense by exploiting sensitivity of poisoned samples. In NeurIPS, 2022. 2
[16] Xinyun Chen, Chang Liu, Bo Li, Kimberly Lu, and Dawn Song. Targeted backdoor attacks on deep learning systems using data poisoning. 2017. 2, 3, 5, 8, 12, 19
[17] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In CVPR, 2009. 5, 12
[18] Shaohua Ding, Yulong Tian, Fengyuan Xu, Qun Li, and Sheng Zhong. Trojan attack on deep generative models in autonomous driving. In International Conference on Security and Privacy in Communication Systems, pages 299â€“318. Springer, 2019. 1
[19] Yinpeng Dong, Xiao Yang, Zhijie Deng, Tianyu Pang, Zihao Xiao, Hang Su, and Jun Zhu. Black-box detection of backdoor attacks with limited information and data. In ICCV, 2021. 2
[20] Min Du, Ruoxi Jia, and Dawn Song. Robust anomaly detection and backdoor attack detection via differential privacy. In ICLR, 2020. 3, 5, 14
[21] Mark Everingham and John Winn. The pascal visual object classes challenge 2012 (voc2012) development kit. Pattern Anal. Stat. Model. Comput. Learn., Tech. Rep, 2007:1â€“45, 2012. 13
[22] Chelsea Finn, Pieter Abbeel, and Sergey Levine. Modelagnostic meta-learning for fast adaptation of deep networks. In ICML, 2017. 5
[23] Leilei Gan, Jiwei Li, Tianwei Zhang, Xiaoya Li, Yuxian Meng, Fei Wu, Shangwei Guo, and Chun Fan. Triggerless backdoor attack for nlp tasks with clean labels. In NAACL, 2022. 2
[24] Yansong Gao, Change Xu, Derui Wang, Shiping Chen, Damith C Ranasinghe, and Surya Nepal. Strip: A defence against trojan attacks on deep neural networks. In ACSAC, 2019. 2

[25] Jindong Gu, Volker Tresp, and Yao Qin. Are vision transformers robust to patch perturbations? In ECCV, 2022. 13
[26] Jindong Gu, Baoyuan Wu, and Volker Tresp. Effective and efficient vote attack on capsule networks. In ICLR, 2021. 13
[27] Tianyu Gu, Brendan Dolan-Gavitt, and Siddharth Garg. Badnets: Identifying vulnerabilities in the machine learning model supply chain. In IEEE Access, 2019. 1, 2, 3, 4, 5, 8, 12, 19
[28] Jiyang Guan, Zhuozhuo Tu, Ran He, and Dacheng Tao. Fewshot backdoor defense using shapley estimation. In CVPR, 2022. 2
[29] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In CVPR, 2016. 5, 12, 17
[30] Timothy Hospedales, Antreas Antoniou, Paul Micaelli, and Amos Storkey. Meta-learning in neural networks: A survey. IEEE transactions on pattern analysis and machine intelligence, 44(9):5149â€“5169, 2021. 5
[31] Andrew G Howard, Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun Wang, Tobias Weyand, Marco Andreetto, and Hartwig Adam. Mobilenets: Efficient convolutional neural networks for mobile vision applications. In CVPR, 2017. 17
[32] Gao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kilian Q Weinberger. Densely connected convolutional networks. In CVPR, 2017. 5, 12, 14, 17
[33] Kunzhe Huang, Yiming Li, Baoyuan Wu, Zhan Qin, and Kui Ren. Backdoor defense via decoupling the training process. In ICLR, 2022. 1, 2, 3, 4, 5, 12, 13, 18
[34] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR, 2015. 5, 8
[35] Soheil Kolouri, Aniruddha Saha, Hamed Pirsiavash, and Heiko Hoffmann. Universal litmus patterns: Revealing backdoor attacks in cnns. In CVPR, 2020. 2
[36] Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009. 12
[37] Yiming Li, Yang Bai, Yong Jiang, Yong Yang, Shu-Tao Xia, and Bo Li. Untargeted backdoor watermark: Towards harmless and stealthy dataset copyright protection. In NeurIPS, 2022. 1
[38] Yiming Li, Yong Jiang, Zhifeng Li, and Shu-Tao Xia. Backdoor learning: A survey. IEEE Transactions on Neural Networks and Learning Systems, 2022. 2
[39] Yuezun Li, Yiming Li, Baoyuan Wu, Longkang Li, Ran He, and Siwei Lyu. Invisible backdoor attack with samplespecific triggers. In ICCV, 2021. 5, 14
[40] Yige Li, Xixiang Lyu, Nodens Koren, Lingjuan Lyu, Bo Li, and Xingjun Ma. Anti-backdoor learning: Training clean models on poisoned data. In NeurIPS, 2021. 1, 2, 3, 5, 13
[41] Yige Li, Xixiang Lyu, Nodens Koren, Lingjuan Lyu, Bo Li, and Xingjun Ma. Neural attention distillation: Erasing backdoor triggers from deep neural networks. In ICLR, 2021. 2, 5, 13, 17
[42] Yiming Li, Mengxi Ya, Yang Bai, Yong Jiang, and Shu-Tao Xia. Backdoorbox: A python toolbox for backdoor learning. In ICLR Workshop, 2023. 1

[43] Yiming Li, Tongqing Zhai, Yong Jiang, Zhifeng Li, and ShuTao Xia. Backdoor attack in the physical world. In ICLR Workshop, 2021. 2
[44] Zhifeng Li, Dihong Gong, Qiang Li, Dacheng Tao, and Xuelong Li. Mutual component analysis for heterogeneous face recognition. ACM Transactions on Intelligent Systems and Technology (TIST), 7(3):1â€“23, 2016. 1
[45] Zhifeng Li, Dihong Gong, Xuelong Li, and Dacheng Tao. Learning compact feature descriptor and adaptive matching framework for face recognition. IEEE Transactions on Image Processing, 24(9):2736â€“2745, 2015. 1
[46] Zhifeng Li, Dihong Gong, Yu Qiao, and Dacheng Tao. Common feature discriminant analysis for matching infrared face images to optical face images. IEEE transactions on image processing, 23(6):2436â€“2445, 2014. 1
[47] Kang Liu, Brendan Dolan-Gavitt, and Siddharth Garg. Finepruning: Defending against backdooring attacks on deep neural networks. In International Symposium on Research in Attacks, Intrusions, and Defenses, pages 273â€“294. Springer, 2018. 2, 5, 13, 17
[48] Wei Liu, Zhifeng Li, and Xiaoou Tang. Spatio-temporal embedding for statistical face recognition from video. In ECCV, 2006. 1
[49] Xinwei Liu, Jian Liu, Yang Bai, Jindong Gu, Tao Chen, Xiaojun Jia, and Xiaochun Cao. Watermark vaccine: Adversarial attacks to prevent watermark removal. In ECCV, 2022. 13
[50] Yunfei Liu, Xingjun Ma, James Bailey, and Feng Lu. Reflection backdoor: A natural backdoor attack on deep neural networks. In ECCV, 2020. 2, 5, 13
[51] Yue Ma, Yali Wang, Yue Wu, Ziyu Lyu, Siran Chen, Xiu Li, and Yu Qiao. Visual knowledge graph for human action reasoning in videos. In ACM MM, 2022. 1
[52] Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu. Towards deep learning models resistant to adversarial attacks. In ICLR, 2018. 8, 13
[53] Anh Nguyen and Anh Tran. Wanetâ€“imperceptible warpingbased backdoor attack. In ICLR, 2021. 2, 5, 12, 13
[54] Tuan Anh Nguyen and Anh Tran. Input-aware dynamic backdoor attack. In NeurIPS, 2020. 2, 5, 13, 14
[55] Youngtaek Oh, Dong-Jin Kim, and In So Kweon. Distribution-aware semantics-oriented pseudo-label for imbalanced semi-supervised learning. In CVPR, 2022. 3
[56] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, high-performance deep learning library. In NeurIPS, 2019. 12
[57] Xiangyu Qi, Tinghao Xie, Yiming Li, Saeed Mahloujifar, and Prateek Mittal. Revisiting the assumption of latent separability for backdoor defenses. In ICLR, 2023. 1
[58] Ali Shafahi, W Ronny Huang, Mahyar Najibi, Octavian Suciu, Christoph Studer, Tudor Dumitras, and Tom Goldstein. Poison frogs! targeted clean-label poisoning attacks on neural networks. In NeurIPS, 2018. 1
[59] Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition. In ICLR, 2015. 17

[60] Kihyuk Sohn, David Berthelot, Nicholas Carlini, Zizhao Zhang, Han Zhang, Colin A Raffel, Ekin Dogus Cubuk, Alexey Kurakin, and Chun-Liang Li. Fixmatch: Simplifying semi-supervised learning with consistency and confidence. In NeurIPS, 2020. 18
[61] Johannes Stallkamp, Marc Schlipsing, Jan Salmen, and Christian Igel. The german traffic sign recognition benchmark: a multi-class classification competition. In IJCNN, 2011. 5, 12
[62] Xiaoou Tang and Zhifeng Li. Video based face recognition using multiple classifiers. In Sixth IEEE International Conference on Automatic Face and Gesture Recognition, 2004. Proceedings., pages 345â€“349. IEEE, 2004. 1
[63] Guanhong Tao, Guangyu Shen, Yingqi Liu, Shengwei An, Qiuling Xu, Shiqing Ma, Pan Li, and Xiangyu Zhang. Better trigger inversion optimization in backdoor scanning. In CVPR, 2022. 2
[64] Brandon Tran, Jerry Li, and Aleksander Madry. Spectral signatures in backdoor attacks. In NeurIPS, 2018. 2
[65] Alexander Turner, Dimitris Tsipras, and Aleksander Madry. Clean-label backdoor attacks. 2018. 2, 3, 5, 8, 13, 15, 19
[66] Bolun Wang, Yuanshun Yao, Shawn Shan, Huiying Li, Bimal Viswanath, Haitao Zheng, and Ben Y Zhao. Neural cleanse: Identifying and mitigating backdoor attacks in neural networks. In SP, 2019. 2
[67] Haotao Wang, Junyuan Hong, Aston Zhang, Jiayu Zhou, and Zhangyang Wang. Trap and replace: Defending backdoor attacks by trapping them into an easy-to-replace subnetwork. In NeurIPS, 2022. 2, 17
[68] Yisen Wang, Xingjun Ma, Zaiyi Chen, Yuan Luo, Jinfeng Yi, and James Bailey. Symmetric cross entropy for robust learning with noisy labels. In ICCV, 2019. 2, 4, 13
[69] Maurice Weber, Xiaojun Xu, Bojan KarlasË‡, Ce Zhang, and Bo Li. Rab: Provable robustness against backdoor attacks. In IEEE SP, 2022. 2
[70] Emily Wenger, Josephine Passananti, Arjun Nitin Bhagoji, Yuanshun Yao, Haitao Zheng, and Ben Y Zhao. Backdoor attacks against deep learning systems in the physical world. In CVPR, 2021. 2
[71] Baoyuan Wu, Hongrui Chen, Mingda Zhang, Zihao Zhu, Shaokui Wei, Danni Yuan, Chao Shen, and Hongyuan Zha. Backdoorbench: A comprehensive benchmark of backdoor learning. In NeurIPS, 2022. 17
[72] Boxi Wu, Jindong Gu, Zhifeng Li, Deng Cai, Xiaofei He, and Wei Liu. Towards efficient adversarial training on vision transformers. In Computer Visionâ€“ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23â€“27, 2022, Proceedings, Part XIII, pages 307â€“325. Springer, 2022. 13
[73] Dongxian Wu and Yisen Wang. Adversarial neuron pruning purifies backdoored deep models. In NeurIPS, 2021. 2, 14, 17
[74] Qizhe Xie, Zihang Dai, Eduard Hovy, Thang Luong, and Quoc Le. Unsupervised data augmentation for consistency training. In NeurIPS, 2020. 8, 18
[75] Yi Zeng, Si Chen, Won Park, Z Morley Mao, Ming Jin, and Ruoxi Jia. Adversarial unlearning of backdoors via implicit hypergradient. In ICLR, 2022. 2

[76] Tong Zhang. Solving large scale linear prediction problems using stochastic gradient descent algorithms. In ICML, 2004. 8, 12
[77] Pu Zhao, Pin-Yu Chen, Payel Das, Karthikeyan Natesan Ramamurthy, and Xue Lin. Bridging mode connectivity in loss landscapes and adversarial robustness. In ICLR, 2020. 2
[78] Shihao Zhao, Xingjun Ma, Xiang Zheng, James Bailey, Jingjing Chen, and Yu-Gang Jiang. Clean-label backdoor attacks on video recognition models. In CVPR, 2020. 2
[79] Zhendong Zhao, Xiaojun Chen, Yuexin Xuan, Ye Dong, Dakui Wang, and Kaitai Liang. Defeat: Deep hidden feature backdoor attacks by imperceptible perturbation and latent representation constraints. In CVPR, 2022. 2
[80] Mingkai Zheng, Shan You, Lang Huang, Fei Wang, Chen Qian, and Chang Xu. Simmatch: Semi-supervised learning with similarity matching. In CVPR, 2022. 3
[81] Runkai Zheng, Rongjun Tang, Jianze Li, and Li Liu. Datafree backdoor removal based on channel lipschitzness. In ECCV, 2022. 2
[82] Xiaojin Zhu and Andrew B Goldberg. Introduction to semisupervised learning. Synthesis lectures on artificial intelligence and machine learning, 3(1):1â€“130, 2009. 18

CVPR #*****
000 001 002 003 004 005 006 007 008 009 010 011 012 013 014 015 016 017 018 019 020 021 022 023 024 025 026 027 028 029 030 031 032 033 034 035 036 037 038 039 040 041 042 043 044 045 046 047 048 049 050 051 052 053

CVPR

A. Algorithm outline

Table 7. Summary of datasets and DNN models in our experi-#*****

CVPR 2023 Submission #*****. CONFIDENTIAL RmEVeInEtWs. COPY. DO NOT DISTRIBUTE.

The algorithm outline of ASD is listed as Algorithm 1.

Algorithm 1: ASD Backdoor Defense

Input : A model fÎ¸ with randomly initialized parameters Î¸ and a poisoned training dataset with C classes D = {(xi, yi)}Ni=1. The clean dataset pool DC with labels and the polluted dataset pool DP without using labels. Stage 1, Stage 2 and Stage

3 will be ended at T1, T2 and T3. L1(Â·) and

L2(Â·) are chosen as SCE loss and CE loss. Output: A clean model fÎ¸ without backdoor behavior.

1 # Initialization

2 Initialize Î¸ randomly;

3 while T < T3 do

4 # Class-aware loss-guided split

5 if T < T1 then

6

DC â† clean seed samples;

7

for Class K = 0, . . . , C âˆ’ 1 do

8

Calculate L1(fÎ¸) for {(x, y)|y = K} in D;

9

Move T /t Ã— n samples in Class K with lowest

L1(fÎ¸) to DC ;

10

end

11 # Class-agnostic loss-guided split

12 else if T < T2 then

13

DC â† âˆ…;

14

Calculate L1(fÎ¸) for the entire D;

15

Move Î±% samples with lowest L1(fÎ¸) to DC ;

16 # Meta-split

17 else if T < T3 then

18

DC â† âˆ…;

19

Î¸â€² â† Î¸;

20

Î¸â€² â† Î¸â€² âˆ’ Î²âˆ‡Î¸â€² L2(fÎ¸â€² (x), y);

21

Calculate L1(fÎ¸) for the entire D;

22

Calculate L1(fÎ¸â€² ) for the entire D;

23

Move Î³% samples with lowest L1(fÎ¸) âˆ’ L1(fÎ¸â€² )

to DC ; 24 end

25 DP = {x|(x, y) âˆˆ D\DC };

26 Train the model fÎ¸ by semi-supervised learning with

the below objective function:

27 end

min L (DC , DP ; Î¸)
Î¸

B. Implementation details
In summary, we use the framework PyTorch [56] to implement all the experiments. Note that the experiments on CIFAR-10 and GTSRB dataset are run on a NVIDIA GeForce RTX 2080 Ti GPU with 11GB memory and the experiments on ImageNet and VGGFace2 dataset are on a

Dataset

# Input size

#

Classes

#

Training images

# Testing images

Models

054

CIFAR-10 3 Ã— 32 Ã— 32 10 GTSRB 3 Ã— 32 Ã— 32 43

50000

10000

ResNet-18

055 056

39209 12630 ResNet-18 057

ImageNet 3 Ã— 224 Ã— 224 30

38859 1500 ResNet-18 058

VGGFace2 3 Ã— 224 Ã— 224 30

9000

2100 DenseNet-121 059 060

061

062

NVIDIA Tesla V100 GPU with 32GB memory.

063

064

B.1. Datasets and DNN models

065

The details of datasets and DNN models in our experiments are summarized in Table 7. Specially, we randomly choose 30 classes from ImageNet and VGGFace2 dataset to construct a subset due to the limitation of the computational

066 067 068 069 070

time and costs.

071

B.2. Attack setups

072 073

Training setups. On the CIFAR-10 [36] and GTSRB [61] 074

dataset, we perform backdoor attacks on ResNet-18 [29] for 075

200 epochs with batch size 128. We adopt the stochastic 076

gradient descent (SGD) [76] optimizer with a learning rate 077

0.1, momentum 0.9, weight decay 5 Ã— 10âˆ’4. The learning rate is divided by 10 at epoch 100 and 150. On the ImageNet [17] dataset, we train ResNet-18 for 90 epochs with batch size 256. We utilize the SGD optimizer with a learning rate 0.1, momentum 0.9, weight decay 10âˆ’4. The learning rate is decreased by a factor of 10 at epoch 30 and 60. The image resolution will be resized to 224 Ã— 224 Ã— 3 before attaching

078 079 080 081 082 083 084 085

the trigger pattern. On VGGFace2 [11] dataset, the batch 086

size is set to 32 and the targeted model is DenseNet-121 087

[32]. Other settings are the same as those used in training 088

the models on ImageNet dataset.

089

Settings for BadNets. As suggested in [27, 33], we set a 090

2 Ã— 2 square on the upper left corner as the trigger pattern 091

on CIFAR-10 and GTSRB. For ImageNet and VGGFace2, 092

we use a 32Ã—32 apple logo on the upper left corner. The ab- 093

lation study for different trigger sizes and trigger locations 094

has been shown in Appendix H.

095

Settings for Blend. Following [16, 33], we choose â€œHello 096

Kittyâ€ pattern on CIFAR-10 and GTSRB and the random 097

noise pattern on ImageNet and VGGFace2. The blend ratio 098

is set to 0.1. Settings for WaNet. The original implementation of WaNet [53] assumes that the attacker can control the training process. To apply WaNet in our poisoning-based attack threat model, we follow [33] to directly use the default warping-based operation to generate the trigger pattern. For

099 100 101 102 103 104 105

CIFAR-10 and GTSRB, we set the noise rate Ïn = 0.2, con- 106

107

1

trol grid size k = 4, and warping strength s = 0.5. For ImageNet and VGGFace2, we choose the noise rate Ïn = 0.2, control grid size k = 224, and warping strength s = 1. Settings for IAB. IAB [54] also belongs to the controltraining backdoor attacks as WaNet [53]. As suggested in [40], we first reimplement the IAB attack as the original paper [54] to obtain the trigger generator. Then we use the trigger generator to generate the poisoned samples in advance and conduct a poisoning-based backdoor attack. Settings for Refool. Following [41, 50], we randomly choose 5,000 images from PascalVOC [21] as the candidate reflection set Rcand and randomly choose one of the three reflection methods to generate the trigger pattern during the backdoor attack. Settings for CLB. As the suggestions in [33, 65], we adopt projected gradient descent (PGD) [52] to generate the adversarial perturbations [2, 4â€“7, 25, 26, 49, 72] within lâˆ ball and set its maximum magnitude Ïµ = 16, step size 1.5, and 30 steps. The trigger pattern is the same as that in BadNets. More experiments of different settings for CLB are listed in Appendix H.
B.3. Defense setups
Settings for FP. FP [47] consists of two steps: pruning and fine-tuning. (1) We randomly select 5% clean training samples as the local clean samples and forward them to obtain the activation values of neurons in the last convolutional layer. The dormant neurons on clean samples with the lowest Î±% activation values will be pruned. (2) The pruned model will be fine-tuned on the local clean samples for 10 epochs. Specially, the learning rate is set as 0.01, 0.01, 0.1, 0.1 on CIFAR-10, GTSRB, ImageNet and VGGFace2. Unless otherwise specified, other settings are the same as those used in [47].
Note that FP is sensitive to its hyper-parameters and we search for the best results by adjusting the pruned ratio Î±% âˆˆ {20%, 30%, 40%, 50%, 60%, 70%, 80%, 90%} for six backdoor attacks on four datasets. Settings for NAD. NAD [41] also aims to repair the backdoored model and need 5% local clean training samples. NAD contains two steps: (1) We first use the local clean samples to fine-tune the backdoored model for 10 epochs. Specially, the learning rate is set as 0.01, 0.01, 0.1, 0.1 on CIFAR-10, GTSRB, ImageNet and VGGFace2. (2) The fine-tuned model and the backdoored model will be regarded as the teacher model and student model to perform the distillation process. . Unless otherwise specified, other settings are the same as those used in [41].
We find that NAD is sensitive to the hyper-parameter Î² in the distillation loss. Therefore, we search for the best results by adjusting the hyper-parameter Î² from {500, 1000, 1500, 2000, 2500, 5000, 7500, 10000} for six backdoor attacks on four datasets.

Settings for ABL. ABL [40] contains three stages: (1) To obtain the poisoned samples, ABL first train the model on the poisoned dataset for 20 epochs by LGA loss [40] and isolate 1% training samples with the lowest loss. (2) Continue to train the model with the poisoned dataset after the backdoor isolation for 70 epochs. (3) Finally, the model will be unlearned by the isolation samples for 5 epochs. The learning rate is 5Ã—10âˆ’4 at the unlearning stage. Unless otherwise specified, other settings are the same as those used in [40].
We find that ABL is sensitive to the hyper-parameter Î³ in LGA loss. We search for the best results by adjusting the hyper-parameter Î³ from {0, 0.1, 0.2, 0.3, 0.4, 0.5} for six backdoor attacks on four datasets. Settings for DBD. DBD [33] contains three independent stages: (1) DBD uses SimCLR [13] to perform the selfsupervised learning for 1,000 epochs. (2) Freeze the backbone and fine-tune the linear layer by supervised learning for 10 epochs. (3) Adopt the MixMatch [9] to conduct the semi-supervised learning for 200 epochs on CIFAR-10 and GTSRB for 90 epochs on ImageNet and VGGFace2. Unless otherwise specified, other settings are the same as those used in [33]. Since DBD is a stable backdoor defense and not sensitive to its hyper-parameter, we directly use the default hyper-parameters and report the results. Settings for our ASD. We adopt MixMatch [9] as our semisupervised learning framework and utilize Adam optimizer with a learning rate 0.002 and batch size 64 to conduct the semi-supervised training. The temperature T is set as 0.5 and the weight of unsupervised loss Î»u is set as 15. We treat the clean data pool DC as a labeled container and the polluted data pool DP as unlabeled. Our three stages are performed ended at T1 = 60, T2 = 90 and T3 = 120 on CIFAR-10 and ImageNet and T3 = 100 on GTSRB.
In the first stage, we fixed the clean seed samples in DC and these clean seed samples will not be removed in the first stage. The clean seed samples consist of 10 samples per class. Besides, we adopt class-aware loss-guided data split with L1(Â·) to progressively increase the number of the seed samples. The number of each class will add n = 10 at every t = 5 epochs. Specially, L1(Â·) is chosen as symmetric cross-entropy (SCE) [68], as suggested in [33]. In the second stage, we use class-agnostic loss-guided data split to choose Î±% = 50% samples with the lowest L1(Â·) losses into DC. In the third stage, we adopt the meta-split to split Î³% = 50% samples with the lowest L1(fÎ¸) âˆ’ L1(fÎ¸â€² ) losses into DC. In meta-split, we adopt stochastic gradient descent (SGD) optimizer with the learning rate Î² = 0.015 and batch size 128 to perform one supervised learning for fÎ¸ to obtain a virtual model fÎ¸â€² . For the virtual model, we update half of the layers of its feature extractor and its linear layer. Note that fÎ¸â€² is only used for data splits and will not be involved in the followed training process. Besides, we

Table 8. The clean accuracy (ACC %) and the attack success rate (ASR %) of five backdoor defenses against six backdoor attacks on VGGFace2. Best results among five backdoor defenses are highlighted in bold.

Attack BadNets Blend WaNet IAB Refool CLB Average

No Defense

ACC

ASR

91.7

99.9

90.9

99.9

91.8

99.2

91.2

99.6

90.7

98.3

91.8

98.9

91.4

99.3

FP

ACC

ASR

91.5

100

87.1

96.0

89.2

33.4

90.6

97.2

90.4

98.4

90.9

99.9

89.9

87.5

NAD

ACC

ASR

56.1

6.5

50.8

7.3

50.4

4.2

43.1

5.5

53.0

3.1

40.0

3.3

48.9

5.0

ABL

ACC

ASR

91.2

19.6

90.1

96.7

92.6

74.6

91.3

59.7

91.1

51.1

91.3

0

91.3

50.3

DBD

ACC

ASR

91.6

0.4

91.5

0.7

89.1

0.8

90.1

2.5

91.2

0.3

90.4

0.3

90.6

0.8

ASD (Ours)

ACC

ASR

90.9

0.5

91.8

0.6

91.2

0.8

92.1

0.4

90.4

0.5

91.8

0.2

91.4

0.5

Table 9. The clean accuracy (ACC %) and the attack success rate (ASR %) of five backdoor defenses against SSBA backdoor attack and all2all attack on CIFAR-10. Best results among five backdoor defenses are highlighted in bold.

Attack SSBA all2all

No Defense

ACC

ASR

94.3

100

94.2

89.6

FP

ACC

ASR

94.3

100

90.9

54.3

NAD

ACC

ASR

89.6

2.7

85.1

2.0

ABL

ACC

ASR

89.2

1.2

86.3

2.7

DBD

ACC

ASR

83.2

0.5

91.6

0.2

ASD (Ours)

ACC

ASR

92.4

2.1

91.7

3.6

Table 10. The clean accuracy (ACC %) and the attack success rate (ASR %) of three backdoor defenses against six backdoor attacks on CIFAR-10. Best results are highlighted in bold.

Attack BadNets Blend WaNet IAB Refool CLB Average

CutMix ACC ASR 95.8 99.9 94.9 99.3 95.1 99.9 94.9 100 95.4 99.9 96.1 1.1 95.4 83.4

DPSGD ACC ASR 55.9 10.9 56.7 37.0 55.1 15.8 85.9 99.7 55.4 59.2 55.7 7.6 60.8 38.4

ASD (Ours) ACC ASR 93.4 1.2 93.7 1.6 93.1 1.7 93.2 1.3 93.5 0 93.1 0.9 93.3 1.1

adaptively split the poisoned training dataset every epoch and the polluted data pool DP is formed with the remaining samples in the poisoned training dataset except the samples in the clean data pool DC .
C. Results on VGGFace2 dataset
We conduct the experiments on VGGFace2 [11] dataset and set the model architecture as DenseNet-121 [32]. The results against six backdoor attacks are shown in Table 8. We also search for the best results for FP, NAD and ABL in different parameters. Besides, we set the learning rate of supervised training in the meta-split of ASD as 0.02. Unless otherwise specified, other settings remain unchanged. Compared with the previous four backdoor defenses, our ASD can still achieve higher ACC and lower ASR on average, which verifies the superiority of our proposed ASD.
D. Results about more backdoor attacks
In addition to six backdoor attacks in our main experiment, we also test our ASD on another two backdoor attack paradigms, i.e., Sample-specific backdoor attack (SSBA)

[39], and all2all backdoor attack (all2all). For SSBA, we follow [39] to use the same encoder-decoder network to generate the poisoned samples. Note that all the backdoor attacks in the above experiments belong to the all2one attack and they relabel the poisoned samples to a target label. For the all2all attack, we relabel samples from class i as class (i + 1) and we adopt IAB [54] as the trigger pattern, as suggested in [73]. The results are shown in Table 9, which verifies that ASD can defend against these two backdoor attacks successfully.
E. Results about more backdoor defenses
We evaluate another two training-time backdoor defenses, i.e., CutMix-based backdoor defense (CutMix) [10] and differential privacy SGD-based backdoor defense (DPSGD) [20]. For CutMix, we implement it as the defaulted setting in the original paper [10]. For DPSGD, we set the clipping bound C = 1 and select the best noise scale Ïƒ by the grid-search. We demonstrate the results in Table 10 and our ASD can still behave better than those two defenses on average. Besides, we also show the purification process of our ASD in Fig. 8.
F. Combination between DBD and our metasplit
We show more results about combining our meta-split with DBD in Fig. 9. From the overall results, we can observe that DBD can achieve 91+% ACC and 4âˆ’% ASR and its training time will reduce a lot with our meta-split.
G. Comparison between DBD and our ASD
We choose 5,000 samples with the largest L1(Â·) losses chosen by the model as clean hard samples. We show more

Clean Samples Dog

Poisoned Samples Cat

Purified Samples Dog

ABL DBD

Dog (0.23)
Dog (0.18)

Dog (0.62)
Dog (0.54)

Dog (0.89)
Dog (0.74)

ABL DBD

Cat (0.21)
Cat (0.16)

Cat (0.53)
Cat (0.43)

Cat (0.87)
Cat (0.75)

ABL DBD

Epoch

ASD (Ours)

Dog (0.15)

Dog (0.58)

(a) Clean Samples

Dog (0.81)

ASD (Ours) Epoch

Horse (0.15)

Dog (0.36)

(b) Poisoned Samples

Dog (0.77)

Epoch

Figure 8. The label and the logit score for the clean samples and poisoned samples on the model trained by ABL, DBD and our ASD against IAB. Our ASD can purify the poisoned samples successfully during the training process.

Airplane

Clean Samples
Table 11. The clean accuracy (ACC %), the attack success rate (ASR %) and the corresponding split rate of poisoned samples on CIFAR-10 for WaNet. Our ASD can achieve better results and a lower split rate of poisoned samples.

Split rate of poisoned samples ACC ASR

ABL

33.2

84.1

2.2

ASD (Ours)

1.2

93.1

1.7

Table 12. The clean accuracy (ACC %), the attack success rate (ASR %) and the corresponding split rate of poisoned samples on CIFAR-10 for IAB. Our ASD can achieve better results and a lower split rate of poisoned samples.

TableO1r3ig.inTalh: efenccleean acOcruigrianacly: c(uArvCy C %) and the attack success rate (wAoSrkRADAw%BBSeDLD)l::l:okkkuiiinmmnmoodoCnnneooIorFdAifRfe-1rADAe0BBSnDLDft::o:t666ra000rdgsssppipefeeetfeeeedddlraebnetlst.arget labels. Our ASD can

Target Label

BadNets

ACC ASR

Blend

ACC ASR

WaNet

ACC ASR

CLB

ACC ASR

yt = 0 92.8 1.1 93.5 0.9 93.8 1.1 93.4 0.7

yt = 1 93.1 0.4 93.5 0.3 92.6 0.6 93.7 1.4

yt = 2 92.7 1.5 93.4 1.1 92.4 1.4 92.8 1.1

yt = 3 93.4 1.2 93.7 1.6 93.1 1.7 93.1 0.9

yt = 4 93.2 0.7 93.7 1.2 93.5 2.1 93.2 0.4

Split rate of poisoned samples ACC ASR

DBD

9.2

91.6

100

ASD (Ours)

1.1

93.2

1.3

Table 14. The clean accuracy (ACC %) and the attack success rate (ASR %) on CIFAR-10 for different poisoned rates. Our ASD can work well under different poisoned rates.

results about the number of clean hard samples and poisoned samples to be split in DC for DBD and our ASD in Fig. 10.
Furthermore, we list the split rate (%) of poisoned samples in DC along with the ACC (%) / ASR (%). Note that we report the split rate in the maximum value during the whole defense process. Here, we compare our ASD with ABL for WaNet in Table 11 and DBD for IAB in Table 12. Our ASD can achieve the lower split rates, higher ACCs and lower ASRs. Specially, the split rate of poisoned samples in DC is less than 1.2% during the whole ASD training.
H. Ablation study on attack settings
Different target labels. We evaluate our ASD using different target labels yt âˆˆ {0, 1, 2, 3, 4}. The results are shown

Poisoned rate

1%

5%

10% 15% 20%

BadNets

ACC ASR

93.8 1.8

93.4 1.2

94.2 1.8

92.3 1.1

93.6 0.9

Blend

ACC 93.8 93.7 92.6 93.5 93.8

ASR 3.9

1.6

1.2

1.8

1.7

WaNet

ACC 93.8 93.1 93.5 93.6 93.2

ASR 3.6

1.7

0.9

1.6

1.9

in Table 13, which verifies the effectiveness of the proposed ASD. Different poisoned rates. We test our ASD under different poisoned rates âˆˆ {1%, 5%, 10%, 15%, 20%}. We demonstrate the results in Table 14, which verifies the superiority of the proposed ASD. Besides, as suggested in [65], we also perform the CLB attack under the poisoned rate âˆˆ {0.6%, 2.5%, 5%} and the maximum perturbation magnitude Ïµ âˆˆ {16, 32} to evaluate ASD. The results are shown

ASD (Ours)

90

90

90

90

ACC (%) ACC (%) ACC (%) ACC (%)

85

85

85

85

80

80

80

80

75

DBD

75

DBD

75

DBD

75

DBD

DOBurDMweittah-split

DOBurDMweittah-split

DOBurDMweittah-split

DOBurDMweittah-split

70 1 20 40Epoch60 80 100 70 1 20 40Epoch60 80 100 70 1 20 40Epoch60 80 100 70 1 20 40Epoch60 80 100

(a) BadNets

(b) Blend

(c) WaNet

(d) CLB

Figure 9. Apply our meta-split to DBD on CIFAR-10 for four backdoor attacks, i.e., BadNets, Blend, WaNet, and CLB. Our proposed meta-split can help accelerate DBD.

Number Number Number Number

4000 3500 3000 2500 2000 1500 1000 500
0 1

DBD Clean Hard Samples ASD Clean Hard Samples DBD Poisoned Samples ASD Poisoned Samples
5 10 Ep1o5ch 20 25 30
(a) BadNets

3500 3000 2500 2000 1500 1000 500
0 1

DBD Clean Hard Samples ASD Clean Hard Samples DBD Poisoned Samples ASD Poisoned Samples
5 10 Ep1o5ch 20 25 30
(b) Blend

3500 3000 2500 2000 1500 1000 500
0 1

DBD Clean Hard Samples ASD Clean Hard Samples DBD Poisoned Samples ASD Poisoned Samples
5 10 Ep1o5ch 20 25 30
(c) WaNet

4000 3000 2000 1000
0 1

DBD Clean Hard Samples ASD Clean Hard Samples DBD Poisoned Samples ASD Poisoned Samples
5 10 Ep1o5ch 20 25 30
(d) CLB

Figure 10. The number of clean hard samples and poisoned samples to be split in DC for DBD and our ASD on CIFAR-10 for four backdoor attacks, i.e., BadNets, Blend, WaNet, and CLB. Our ASD can select clean hard samples.

Table 15. The clean accuracy (ACC %) and the attack success rate (ASR %) on CIFAR-10 for different poisoned rates for CLB. Our ASD can work well under different poisoned rates for CLB.

Poisoned rate

0.6%

2.5%

5%

CLB (Ïµ = 16)

ACC ASR

93.4 3.1

93.1 0.9

93.1 0

CLB (Ïµ = 32)

ACC ASR

94.1 2.0

93.4 1.5

93.3 1.3

Table 16. The clean accuracy (ACC %) and the attack success rate (ASR %) on CIFAR-10 for different trigger locations of BadNets. Our ASD can work well under different trigger locations of BadNets.

Location

No Defense
ASD (Ours)

ACC ASR ACC ASR

Upper left 94.9 100 93.4 1.2

Upper right 93.7 99.7 93.1 1.1

Lower left 94.5 99.8 93.7 0.9

Lower right 94.1 100 92.8 0.8

Center
93.8 100 93.6 1.7

in Table 15, which proves that our ASD can also defend the CLB attack under different attack settings. Different trigger patterns. For simplicity, we adopt the

Table 17. The clean accuracy (ACC %) and the attack success rate (ASR %) on CIFAR-10 for different trigger sizes of BadNets. Our ASD can work well under different trigger sizes of BadNets.

Trigger size

No

ACC

Defense

ASR

ASD

ACC

(Ours)

ASR

1Ã—1
94.7 93.4 93.2 0.8

2Ã—2
94.9 100 93.4 1.2

3Ã—3
94.2 99.7 92.7 2.2

4Ã—4
93.7 99.8 93.1 1.4

BadNets on CIFAR-10 as an example to test the performance of our ASD under different trigger patterns. On the one hand, we set the trigger at different locations, as shown in Table 16. On the other hand, we also adjust the trigger size to evaluate our defense, as shown in Table 17. ASD can achieve 92+% ACC and 2âˆ’% ASR in both two cases.
I. Ablation study on defense settings
Different modes in loss-guided split. In summary, lossguided split contains two modes during the previous two stages: (1) Class-aware data split. (2) Class-agnostic data split. In particular, we will discuss the necessity of the corresponding mode in different stages by conducting the fol-

ACC (%) / ASR (%) ACC (%) / ASR (%)

100

ACC BadNets

ACC Blend

80

ACC WaNet ACC CLB

ASR BadNets

60

ASR Blend ASR WaNet

ASR CLB

40

100

ACC BadNets

ACC Blend

80

ACC WaNet ACC CLB

ASR BadNets

60

ASR Blend ASR WaNet

ASR CLB

40

20

20

0
1 20 40 Ep6o0ch 80 100 120
(a)

0
1 20 40 Ep6o0ch 80 100 120
(b)

Figure 11. Ablation study for the loss-guided split of our ASD on CIFAR-10 for four backdoor attacks, i.e., BadNets, Blend, WaNet, and CLB. (a) Set class-agnostic loss-guided split in stage 1 instead of class-aware one. (b) Set class-aware loss-guided split in stage 2 instead of class-agnostic one. The results show the necessity of using class-aware one in stage 1 and using class-agnostic one in stage 2.

lowed two experiments. Note that we only change the mode of loss-guided data split and will not change the number in DC of data split during every epoch.
Set class-agnostic loss-guided split in stage 1 instead of class-aware one. As shown in Fig. 11a, if we use samples with the lowest losses of the entire set to increase the seed sample, the ACC will crash in stage 1 due to class imbalance.
Set class-aware loss-guided split in stage 2 instead of class-agnostic one. More poisoned samples in the target class will be filled in DC with class-aware split in stage 2 and ASR can not be suppressed. Fig. 11b demonstrates that BadNets will break the defense in this setting at 20% poisoned rate. In contrast, our defaulted ASD can still work at 20% poisoned rate, as shown in Table 14. Different hyper-parameters in meta-split. The epoch, learning rate and updated layer number are three key hyperparameters in meta-split.
From Fig. 12a, ACC will drop with the increase of the epoch because the multi-epoch update can enable the model to learn not only the poisoned samples but also the clean hard samples well. As such, the large loss reduction makes the clean hard samples hard to appear in DC based on the proposed meta-split.
As for the learning rate in Fig. 12b, we can observe that ACC will decrease as the learning rate is smaller. We suspect that the reason may be that the low learning rate can also prevent clean easy samples to be learned by the model. Hence, the small loss reduction makes DC contain more clean easy samples.
Fig. 12c indicates that the number of layers to be updated in meta-split can also have an effect on the final performance of the proposed ASD. We show the results of our ASD at poisoned rate 1% in Fig. 12c. If we update fewer

Table 18. The clean accuracy (ACC %) and the attack success rate (ASR %) on CIFAR-10 for different model architectures. Our ASD can work well under different model architectures.

Attack Method

ResNet18 VGG11 MobileNet DenseNet121 ACC ASR ACC ASR ACC ASR ACC ASR

BadNets

No Defense ASD(Ours)

94.9 93.4

100 1.2

91.0 99.9 90.1 100 90.4 3.7 89.4 4.6

94.4 100 93.1 2.2

Blend

No Defense 94.1 98.3 90.6 98.4 87.7 99.7 94.1 98.2 ASD(Ours) 93.7 1.6 87.5 2.4 89.8 0.7 92.4 3.1

WaNet

No Defense 93.6 99.9 89.7 99.4 86.9 99.8 93.9 100 ASD(Ours) 93.1 1.7 90.3 0.9 86.8 3.4 93.2 3.2

CLB

No Defense 94.4 99.9 91.0 99.9 88.9 7.2 94.2 2.3 ASD(Ours) 93.1 0.9 89.2 3.1 88.1 2.4 93.1 1.4

layers in the meta-split, the clean easy samples can not be learned by the model either, which introduces more clean easy samples to DC and thus leads to a lower ACC. Once all the layers are updated and the poisoned rate is low, the difference in the loss reduction will decrease between the poisoned samples and clean samples. Hence, DC can contain more poisoned samples and induce the model to create the backdoor mapping. Different splitting rates Î±% in stage 2 and Î³% stage 3. We keep the same splitting rate in stage 2 and stage 3 to conduct the experiment. Fig. 13 shows that our ASD can achieve 90+% ACC and 5âˆ’% ASR in different splitting rates during stage 2 and stage 3. In other words, ASD is not sensitive to the hyper-parameter splitting rate. Different model architectures under our ASD. We test our ASD under ResNet-18 [29], VGG-11 [59], MobileNet [31] and DenseNet-121 [32]. As suggested in [71], we conduct the backdoor attacks without backdoor defenses. Besides, we set the learning rate as 0.01 in meta-split for our ASD when using MobileNet. Unless otherwise specified, other settings remain unchanged. As shown in Table 18, our ASD can defend against backdoor attacks under different model architectures. Ablation study about seed samples. Seed samples in each dataset are randomly sampled and then fixed during ASD. Here, we report the results (meanÂ±std) of 5 runs in Table 19. The results demonstrate the stability of our ASD under different sampled seed samples. Besides, we also conduct the experiments under different numbers of seed samples. As shown in Table 20, it might result in the failure of ASD when the number of seed samples is less than 100. Note that 100 is much smaller than that (10,000) required in previous defenses [41, 47, 67, 73]. Besides, we also show seed samples can be taken from a different available dataset in Sec. 5.4, which indicates the flexibility of our seed sample selection. Performance on clean dataset. Our ASD can achieve 93.8% ACC on clean CIFAR10, preserving the clean ACC well.

ACC (%) ASR (%) ACC (%) ASR (%) ACC (%) ASR (%) ACC (%) ASR (%)

94

92

90

ACC BadNets

88

ACC Blend ACC WaNet

86

ACC CLB ASR BadNets

84

ASR Blend ASR WaNet

82

ASR CLB

80
1 2 Epo3ch 4

(a)

60 50 40 30 20 10 50

94 92 90 88 86 84 82 800.005

60

50

ACC BadNets 40

ACC Blend

ACC WaNet ACC CLB

30

ASR BadNets ASR Blend

20

ASR WaNet

ASR CLB

10

0.L0e1arn0i.n01g5Ra0t.e02 0.0250
(b)

95

100

90

80

85

ACC BadNets ACC Blend

60

ACC WaNet

80

ACC CLB ASR BadNets

40

ASR Blend

ASR WaNet

75

ASR CLB

20

70 1Upda2ted La3yer Nu4mber5 0
(c)

Figure 12. Ablation study for the meta-split on CIFAR-10 for four backdoor attacks, i.e., BadNets, Blend, WaNet, and CLB. (a) The epoch of supervised learning. (b) Learning rate. (c) Updated layer number.

94

92

ACC BadNets

ACC Blend

90

ACC WaNet ACC CLB

ASR BadNets

88

ASR Blend ASR WaNet

ASR CLB

86

30 Sp40litting50Rate (6%0 )

60 50 40 30 20 10 70 0

Figure 13. Ablation study for the splitting rate in Stage 2 and Stage 3 on CIFAR-10 for four backdoor attacks.

Table 19. The clean accuracy (ACC %) and the attack success rate (ASR %) on CIFAR-10 for different randomly sampled seed samples. The experiments (Â±std over 5 random runs) are conducted on CIFAR-10. Our ASD can achieve the stable performance when the seed samples are differently sampled.

ACC ASR

BadNets 92.5 (Â±0.7) 1.9 (Â±0.6)

Blend 92.9 (Â±0.6) 1.5 (Â±0.5)

WaNet 93.2 (Â±0.6) 2.0 (Â±0.7)

CLB 93.0 (Â±0.5) 2.2 (Â±0.8)

Table 20. The clean accuracy (ACC %) and the attack success rate (ASR %) on CIFAR-10 for different numbers of seed samples. The default value (i.e., 100) used in our ASD is feasible on CIFAR-10.

Number of seed samples

10

50

100

BadNets

ACC ASR

80.6

91.4

93.4

0

4.8

1.2

Blend

ACC ASR

92.5

86.8

93.7

99.1

10.4

1.6

WaNet

ACC ASR

85.9

92.7

93.1

99.7

6.2

1.7

CLB

ACC ASR

93.0

93.0

93.1

3.6

2.4

0.9

J. Details about different semi-supervised methods
Semi-supervised learning [8, 9, 60, 74, 82] studies how to leverage a training dataset with both labeled data and unlabeled data to obtain a model with high accuracy. In addition to its application in normal training, semi-supervised learning also serves as a powerful means for the security of DNNs [1, 12, 33]. MixMatch Loss [9]. Given a batch X âŠ‚ DC of labeled samples, and a batch U âŠ‚ DP of unlabeled samples, MixMatch generates a guessed label distribution qÂ¯ for each unlabeled sample u âˆˆ U and adopts MixUp to augment X and U to X â€² and U â€². The supervised loss Ls is defined as:

  \mathcal {L}_{s}= \sum _{(x, q) \in \mathcal {X}^{\prime }} \mathrm {H}\left (p_x, q\right ), 

(4)

where px is the prediction of x, q is the one-hot label and H(Â·, Â·) is the cross-entropy loss. The unsupervised loss Lu is defined as:

  \mathcal {L}_u=\sum _{(u, \bar {q}) \in \mathcal {U}^{\prime }}\left \|p_u-\bar {q}\right \|_2^2, 

(5)

where pu is the prediction of u. Finally, the MixMatch loss can be defined as:

  \mathcal {L}=\mathcal {L}_s+\lambda \cdot \mathcal {L}_u, 

(6)

where Î» is a hyper-parameter for trade-off. UDA [74]. Given a batch X âŠ‚ DC of labeled samples, and a batch U âŠ‚ DP of unlabeled samples, UDA constructed a guessed label distribution qÂ¯ for each unlabeled sample u âˆˆ U after the weak augmentation. Moreover, it adopts the strong augmentation (RandAugment) to augment U to U â€² and generates a guessed label distribution qÂ¯â€². The supervised loss Ls is defined as:

  \mathcal {L}_{s}= \sum _{(x, q) \in \mathcal {X}} \mathrm {H}\left (p_x, q\right ), 

(7)

where H(Â·, Â·) is the cross-entropy loss. The unsupervised loss Lu is defined as:

  \mathcal {L}_u=\sum _{(u, \bar {q}) \in \mathcal {U},(u, \bar {q}') \in \mathcal {U}^{\prime } \mathrm {H}\left (\bar {q}\ | \ \bar {q}'\right ), 

(8)

where pu is the prediction of u. Finally, the UDA loss can be defined as:

  \mathcal {L}=\mathcal {L}_s+\lambda \cdot \mathcal {L}_u, 

(9)

where Î» is a hyper-parameter for trade-off. ReMixMatch Loss [8]. Given a batch X âŠ‚ DC of labeled samples, and a batch U âŠ‚ DP of unlabeled samples, ReMixMatch produces a guessed label distribution qÂ¯ for each unlabeled sample u âˆˆ U after the weak augmentation. Besides, it adopts MixUp, the strong augmentation

Table 21. The results of ABL, DBD and our ASD under the adaptive attack in different perturbation magnitudes Ïµ of the trigger.

Ïµ ABL DBD ASD (Ours)

4 86.1 / 0.8 90.4 / 0.2 93.2 / 1.1

8 71.6 / 99.7 91.2 / 0.7 93.5 / 1.3

16 75.1 / 99.8 90.4 / 0.8 92.8 / 0.9

32 86.7 / 99.4 91.7 / 99.9 93.3 / 1.2

Average 79.9 / 74.9 90.9 / 25.4 93.2 / 1.1

(CTAugment) and the weak augmentation to augment X , U , U to X â€², U â€², UË†1. In total, the ReMixMatch loss can be defined as:

 \begin {aligned} \mathcal {L}= &\sum _{(x, q)\in \mathcal {X}^{\prime }\mathrm {H}\left (p_x,q\right )+\lambda _{\mathcal {U} \sum _{(u, \bar {q}) \in \mathcal {U}^{\prime }\mathrm {H}\left (p_u,\bar {q}\right )\ +& \lambda _{\hat {\mathcal {U}_1} \sum _{(u_1, \bar {q}) \in \hat {\mathcal {U}_1} \mathrm {H}\left (p_{u1}, \bar {q}\right )\ +\lambda _r& \sum _{u1 \in \hat {\mathcal {U}_1} \mathrm {H}\left (p_{\bm {\thea }\ (r\mid \operatorname {Rotae}(u_1, r),r\ight ),\end {aligned} 

(10)

where Rotate(u1, r) denotes that rotate an image u1 âˆˆ UË†1 the rotation angle r uniformly from r âˆ¼ {0, 90, 180, 270} and H(Â·, Â·) is the cross-entropy loss.
K. Details of the adaptive attack
We state the details of the adaptive attack in the main paper. Problem formulation. Suppose that the attackers choose a number of samples to be poisoned Dp = {(xi, yi)}Ni=1 and Dc = {(xi, yi)}M i=1 denotes the remain clean samples, fÎ¸ denotes a trained model. The objective function for the trigger pattern p in the adaptive attacks can be formulated as (11), i.e., minimizing the gradient for the poisoned samples w.r.t the trained model fÎ¸ and maximizing that for the clean samples.

 \begin {aligned} \min _{\bm {p} &\frac {1}{N} \sum _{(\bm {x}, y) \in \mathcal {D}_{p} \frac {\mathrm {d} \mathcal {L}\left (f_{\bm {\theta }(\bm {x}+\bm {p}), y\right )} {\mathrm {d} \bm {\theta }\-&\frac {1}{M} \sum _{(\bm {x}, y) \in \mathcal {D}_{c} \frac {\mathrm {d} \mathcal {L}\left (f_{\bm {\theta }(\bm {x}), y\right )} {\mathrm {d} \bm {\theta }, \texti {s.t}, \parlel \bm {p} \parlel _{\infty }\leq \epsilon ,\end {aligned} \label {eq:adptive atack} 

(11)

where Ïµ is the magnitude of the trigger pattern. Settings and more results. We adjust the perturbation magnitudes Ïµ of the trigger pattern for ABL, DBD and our ASD. As shown in Table 21, our ASD can achieve the best average results among three backdoor defenses. Besides, we also study the effect of the loss objectives to train the surrogate model on the results. Specially, our ASD can obtain 91+% ACC and 5âˆ’% ASR by using either the supervised loss or the semi-supervised loss to train the surrogate model in the adaptive attack. Reasons for our successful defense against the adaptive attack. The superiority of ASD in adaptive attack benefits a lot from the semi-supervised loss objective and two

dynamic data pools. Adaptive attacks aim at optimizing triggers to minimize the gaps between clean and poisoned samples on surrogate models, which makes poisoned samples difficult to defend. However, such reduced gaps are highly dependent on model checkpoint, which means the gaps might be large on some other checkpoints, especially during ASD training with semi-supervised loss on two dynamic data pools (DC , DP ), which can greatly increase the diversity of optimized checkpoints. Besides, as shown in manuscript, ASD is good at separating model checkpointdependent clean hard examples from poisoned ones with meta-split. Moreover, the strong data augmentation and pseudo-labeling of MixMatch used in ASD also help destroy the trigger pattern.
L. Resistance to another adaptive attack
In this section, we propose another adaptive attack for our proposed ASD. We adopt the same poisoning-based threat model [16, 27, 65] as that in the main paper. Problem formulation. Suppose that the attackers choose a number of samples to be poisoned Dp = {(xi, yi)}Ni=1 and Dt = {(xi, yt)}M i=1 denotes the remaining clean samples with the attacker-specified target label yt, g denotes a trained model. Since we adopt semi-supervised learning to purify the polluted pool and this adaptive attack aims to destruct the purification process, the trigger pattern p can be optimized by minimizing the distance between poisoned samples and the target class in the feature space as:

 \begin {aligned} \min _{\bm {p} \Big \| \frac {1}{N} \sum _{(\bm {x}, y) \in \mathcal {D}_{p} &g\left (\bm {x}+\bm {p} \right )- \frac {1}{M} \sum _{(\bm {x}, y) \in \mathcal {D}_{t} g\left (\bm {x}\right )\Big \|_2,\ \texti {s.t},& \parlel \bm {p} \parlel _{\infty }\leq \epsilon ,\end {aligned} \label {eq:another adptive atack} 

(12)

where Ïµ is the magnitude of the trigger pattern. Settings and results. We adopt the same settings as that in our main paper. The adaptive attack can achieve 94.9% ACC and 99.9% ASR without any defense. This attack can obtain 93.7% ACC and only 1.5% ASR under our ASD. Hence, our ASD can still work well under this adaptive attack due to the low transferability of the trigger pattern.

M. Details about the loss distribution during meta-split
We show more results of the loss distribution during the meta-split of our proposed ASD in Fig. 14, Fig. 15, Fig. 16 and Fig. 17.

N. Details about the grid-search for FP, NAD, ABL, and DPSGD
We search for the best results by grid search for FP, NAD, ABL and DPSGD and show the results in Table 22,

Table 23, Table 24, Table 25, Table 26, Table 27, Table 28, Table 29, Table 30, Table 31, Table 32, Table 33, Table 34. The details of the grid search have been stated in Appendix B.

Proportion (%)

Clean samples

8

Poisoned samples

6

4

2

0 0 2 Lo4ss Va6lue 8 10
(a) BadNets

Proportion (%)

Clean samples

8

Poisoned samples

6

4

2

0 0 2 Lo4ss Va6lue 8 10
(b) Blend

Proportion (%)

12

Clean samples

10

Poisoned samples

8

6

4

2

0 0 2 Lo4ss Va6lue 8 10
(c) WaNet

Proportion (%)

12

Clean samples

10

Poisoned samples

8

6

4

2

0 0 2 Los4s Val6ue 8 10
(d) CLB

Proportion (%)

Figure 14. The loss distribution of samples on the model fÎ¸ after the first two stages on CIFAR-10 for four backdoor attacks.

35 30

Clean samples Poisoned samples

25

5

20

4

15

3 2

10 5

1 00

1

0 0 2 Lo4ss Va6lue 8 10

(a) BadNets

Proportion (%)

40

Clean samples

35

Poisoned samples

30

25

5

20

4 3

15

2

10 5

1 00

1

0 0 2 Lo4ss Va6lue 8 10

(b) Blend

Proportion (%)

Clean samples

40

Poisoned samples

30

5

4

20

3

2

1

10

00

1

0 0 2 Lo4ss Va6lue 8 10
(c) WaNet

Proportion (%)

60

Clean samples

50

Poisoned samples

40

5

30

4 3

20

2 1

10

00

1

0 0 2 Lo4ss V6alue 8 10
(d) CLB

Figure 15. The loss distribution of samples on the â€˜virtual modelâ€™ fÎ¸â€² in Fig. 14 after one-epoch supervised learning on CIFAR-10 for four backdoor attacks.

Proportion (%)

8 7

Clean samples Poisoned samples

6

5

4

3

2

1

0-10 -5Loss Re0ductio5n 10

(a) BadNets

Proportion (%)

8

Clean samples

7

Poisoned samples

6

10

Clean samples

Poisoned samples

8

10

Clean samples

8

Poisoned samples

Proportion (%)

Proportion (%)

5

6

6

4 3

4

4

2

2

2

1

0-10 -5Loss Re0ductio5n 10

0-10 -L5oss Re0ductio5n 10

0 -10 L-o5ss Red0uction5 10

(b) Blend

(c) WaNet

(d) CLB

Figure 16. The loss reduction between fÎ¸ in Fig. 14 and fÎ¸â€² in Fig. 15 on CIFAR-10 for four backdoor attacks.

Proportion (%)

8

Clean samples

7

Poisoned samples

6

5

4

3

2

1

0 0 2 Lo4ss Va6lue 8 10

(a) BadNets

Proportion (%)

8 7

Clean samples Poisoned samples

6

5

4

3

2

1

0 0 2 Lo4ss Va6lue 8 10

(b) Blend

Proportion (%)

12

Clean samples

10

Poisoned samples

8

6

4

2

0 0 2 Lo4ss Va6lue 8 10
(c) WaNet

Proportion (%)

12

Clean samples Poisoned samples

10

8

6

4

2

0 0 2 Los4s Val6ue 8 10
(d) CLB

Figure 17. The loss distribution of samples on the model fÎ¸ after all three stages on CIFAR-10 for four backdoor attacks.

Ratio 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9
Ratio 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9
Ratio 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9
Ratio 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9
Î² 500 1000 1500 2000 2500 5000 7500 10000

Table 22. Search for the best results by the grid-search for FP on CIFAR-10.

BadNets

ACC

ASR

94.5

100

94.4

100

94.5

100

94.6

100

94.4

100

94.2

100

93.9

1.8

94.2

100

Blend

ACC

ASR

93.9

97.9

93.8

97.2

93.8

96.1

93.7

96.2

93.6

96.9

93.4

93.2

93.8

89.5

92.9

77.1

WaNet

ACC

ASR

93.4

100

93.9

99.9

93.6

99.8

93.7

99.7

93.7

99.9

93.3

99.8

93.1

99.8

90.4

98.6

IAB

ACC

ASR

94.1

99.9

93.9

99.9

93.9

99.6

93.9

100

93.3

99.9

92.9

99.8

92.3

99.7

89.3

98.1

Refool

ACC

ASR

93.9

91.8

93.9

91.9

93.6

92.3

93.2

92.5

92.3

92.1

91.2

91.8

92.7

87.9

92.1

86.1

Table 23. Search for the best results by the grid-search for FP on GTSRB.

BadNets

ACC

ASR

97.4

100

97.2

100

97.3

100

97.1

100

97.1

100

96.5

99.9

93.4

73.7

84.2

0

Blend

ACC

ASR

96.8

98.7

96.7

98.6

96.7

98.5

96.5

98.2

96.5

98.1

96.1

85.2

91.4

68.1

78.8

85.7

WaNet

ACC

ASR

97.6

100

97.2

100

97.2

100

97.4

100

97.2

99.9

96.3

99.9

92.5

21.4

85.3

23.6

IAB

ACC

ASR

97.6

100

97.3

100

97.7

100

97.5

100

97.5

100

97.4

99.5

96.4

99.2

86.9

0

Refool

ACC

ASR

97.9

98.2

98.1

96.4

97.9

94.2

97.3

60.9

96.6

49.1

95.7

47.3

91.5

0.2

87.3

0.4

Table 24. Search for the best results by the grid-search for FP on ImageNet.

BadNets

ACC

ASR

76.7

51.1

75.9

46.1

73.9

20.7

70.3

1.6

73.1

14.9

70.1

0.3

66.4

0

84.3

0

Blend

ACC

ASR

78.1

93.9

77.6

93.4

76.1

90.9

74.4

86.2

72.7

85.9

71.1

87.8

72.9

77.9

63.4

9.5

WaNet

ACC

ASR

79.1

95.9

78.9

96.3

77.5

95.5

74.2

94.4

72.5

95.2

75.1

95.1

71.4

92.2

58.2

84.4

IAB

ACC

ASR

78.9

99.4

78.3

99.9

76.9

98.9

76.3

99.9

72.8

98.2

73.9

99.9

70.9

99.4

58.7

84.2

Refool

ACC

ASR

77.7

84.4

76.7

74.1

75.9

75.5

73.8

62.7

70.5

48.3

73.6

63.8

71.2

52.5

61.4

10.3

Table 25. Search for the best results by the grid-search for FP on VGGFace2.

BadNets

ACC

ASR

91.2

100

91.1

100

91.1

100

91.5

100

91.0

100

91.1

100

91.5

100

89.3

100

Blend

ACC

ASR

90.2

99.9

90.1

99.9

90.2

99.9

90.2

99.9

91.1

99.9

90.3

100

89.9

100

87.1

96.0

WaNet

ACC

ASR

91.8

75.3

91.8

75.7

91.7

75.8

91.6

76.7

91.6

78.5

91.4

81.1

90.8

79.6

89.2

33.4

IAB

ACC

ASR

90.7

97.5

90.8

97.4

90.7

97.3

90.6

97.3

90.6

97.2

90.5

97.6

90.6

97.4

89.3

98.4

Refool

ACC

ASR

90.7

98.6

90.8

98.7

90.7

98.6

90.9

98.7

90.4

98.4

90.7

98.9

90.2

97.9

86.8

98.5

Table 26. Search for the best results by the grid-search for NAD on CIFAR-10.

BadNets

ACC

ASR

90.6

12.9

89.7

10.0

88.2

4.6

84.7

6.9

83.1

4.5

32.2

2.7

18.2

5.1

20.8

1.1

Blend

ACC

ASR

89.8

17.2

87.4

4.3

85.8

3.4

80.2

5.9

75.8

4.5

32.1

3.7

29.8

3.1

20.2

7.4

WaNet

ACC

ASR

89.2

21.8

87.5

11.8

83.1

13.1

71.3

6.7

64.3

8.1

40.2

6.3

25.7

4.1

23.9

10.4

IAB

ACC

ASR

88.5

29.7

85.8

8.3

82.8

4.2

75.5

2.5

67.9

1.1

39.4

7.2

24.5

1.9

20.1

6.1

Refool

ACC

ASR

89.9

9.7

89.7

10.5

87.7

5.4

86.2

3.6

81.1

3.1

45.5

2.8

30.3

8.1

24.1

6.2

CLB

ACC

ASR

90.2

92.8

90.4

94.4

90.3

96.9

90.4

98.8

94.9

99.9

94.4

99.8

93.3

100

91.17

99.9

CLB

ACC

ASR

94.5

99.7

93.6

99.3

89.3

99.4

83.2

99.5

67.6

99.7

52.6

99.2

39.4

99.6

23.6

99.4

CLB

ACC

ASR

79.9

88.3

79.1

69.2

77.6

60.1

75.7

67.2

73.2

38.3

69.8

49.5

70.9

53.7

54.2

0

CLB

ACC

ASR

90.8

99.8

90.9

99.9

90.8

99.9

90.8

99.8

90.6

99.9

90.7

99.9

90.2

100

88.8

99.9

CLB

ACC

ASR

86.4

9.5

81.8

8.6

72.5

6.2

65.3

6.3

43.9

10.9

32.3

5.1

18.4

11.1

21.4

14.4

Î² 500 1000 1500 2000 2500 5000 7500 10000

Table 27. Search for the best results by the grid-search for NAD on GTSRB.

BadNets

ACC

ASR

97.1

0.2

96.8

0

96.5

0

93.5

0

19.7

0

6.9

0

5.7

1.2

5.9

0.7

Blend

ACC

ASR

96.9

99.9

96.8

99.5

96.3

99.9

96.2

99.3

96.2

99.1

93.3

62.4

55.7

1.2

10.3

0

WaNet

ACC

ASR

97.2

67.8

97.1

69.1

96.9

76.1

96.7

70.9

96.5

47.1

78.1

2.4

4.3

0

5.8

31.7

IAB

ACC

ASR

96.9

0.1

97.1

0.1

95.9

0.7

94.5

0.5

20.8

0

5.9

0

8.4

0.5

7.2

1.4

Refool

ACC

ASR

97.3

93.6

97.3

72.4

97.1

47.5

95.5

1.4

93.6

3.8

7.1

0

4.3

0

6.6

0

CLB

ACC

ASR

5.7

40.1

4.1

41.7

4.7

44.6

4.8

40.1

5.5

36.3

3.3

21.1

4.6

34.3

2.9

29.4

Î² 500 1000 1500 2000 2500 5000 7500 10000

Table 28. Search for the best results by the grid-search for NAD on ImageNet.

BadNets

ACC

ASR

64.1

6.22

65.1

5.1

61.6

4.2

60.1

2.1

54.5

1.5

51.7

3.2

43.8

1.8

33.8

1.4

Blend

ACC

ASR

64.8

0.3

63.6

0.6

62.27

0.5

59.6

0.4

57.5

0

51.5

0.4

44.7

0

33.9

0.6

WaNet

ACC

ASR

63.8

1.3

62.8

0.7

62.2

0.8

59.7

1.2

56.4

0.5

50.2

0.5

38.2

0.6

41.1

1.2

IAB

ACC

ASR

63.1

4.8

63.4

1.1

63.8

0.6

60.6

0.3

57.9

0.2

48.5

0.6

43.4

0.4

35.4

0.6

Refool

ACC

ASR

63.7

0.3

62.5

0

60.8

0

59.5

0.3

58.5

0.1

51.5

0

41.4

0

35.2

0.1

CLB

ACC

ASR

63.4

3.3

62.2

1.9

61.8

4.6

62.7

1.7

53.2

1.3

47.1

0.6

40.9

0.2

37.1

0

Î² 500 1000 1500 2000 2500 5000 7500 10000

Table 29. Search for the best results by the grid-search for NAD on VGGFace2.

BadNets

ACC

ASR

42.6

5.5

53.4

10.6

48.5

5.8

56.1

6.5

41.8

1.4

53.8

11.4

50.7

2.7

52.7

8.5

Blend

ACC

ASR

49.1

10.9

46.2

8.1

43.7

5.6

47.3

4.1

50.8

7.3

28.9

2.6

47.9

2.5

45.5

6.4

WaNet

ACC

ASR

48.7

3.7

43.9

12.2

50.4

4.2

43.7

4.1

43.9

3.7

41.1

2.2

49.5

2.8

40.7

5.5

IAB

ACC

ASR

43.1

5.5

25.7

5.1

37.9

2.3

44.7

8.8

42.7

8.9

31.9

5.3

32.6

40.9

30.6

12.3

Refool

ACC

ASR

50.8

4.3

52.9

2.1

53.0

3.1

52.8

5.6

53.3

7.1

52.5

4.3

53.2

5.3

50.5

5.1

CLB

ACC

ASR

42.9

15.1

46.3

18.4

48.7

15.6

34.6

3.0

40.0

3.3

40.2

11.2

38.7

2.2

27.2

13.9

Table 30. Search for the best results by the grid-search for ABL on CIFAR-10.

Î³

BadNets

ACC

ASR

Blend

ACC

ASR

WaNet

ACC

ASR

IAB

ACC

ASR

Refool

ACC

ASR

CLB

ACC

ASR

0

93.8

1.1

90.9

2.1

84.1

2.2

93.4

5.1

79.9

99.7

86.6

1.3

0.1

66.2

100

91.9

1.6

75.7

100

88.2

100

82.7

1.3

79.9

14.4

0.2

70.8

100

81.3

99.3

80.1

100

85.7

100

80.3

99.1

83.8

7.67

0.3

72.8

100

80.1

99.3

77.7

100

80.6

100

69.7

99.9

83.8

25.6

0.4

64.9

100

86.8

99.2

78.6

100

73.8

100

79.5

99.9

78.3

22.6

0.5

71.9

100

74.5

99.9

77.5

100

76.9

100

71.9

99.9

77.9

12.5

Table 31. Search for the best results by the grid-search for ABL on GTSRB.

Î³

BadNets

ACC

ASR

Blend

ACC

ASR

WaNet

ACC

ASR

IAB

ACC

ASR

Refool

ACC

ASR

CLB

ACC

ASR

0

97.1

0

95.6

12.5

94.2

10.9

93.3

100

95.4

0

90.4

2.3

0.1

80.9

100

94.2

99.9

91.8

100

91.4

100

96.2

0

86.7

100

0.2

85.5

100

94.4

99.9

91.9

100

90.9

100

95.7

0

80.1

100

0.3

97.1

0

97.1

0.5

97.0

0.4

97.1

0.8

96.2

0

75.2

100

0.4

96.8

0

96.9

0.7

96.7

0

97.4

0.6

95.5

0

72.3

100

0.5

97.1

0

96.7

2.1

96.1

0.2

96.9

2.1

95.6

0

69.1

100

Table 32. Search for the best results by the grid-search for ABL on ImageNet.

Î³

BadNets

ACC

ASR

Blend

ACC

ASR

WaNet

ACC

ASR

IAB

ACC

ASR

Refool

ACC

ASR

CLB

ACC

ASR

0

80.2

0.1

83.6

100

84.8

99.8

79.7

3.9

76.2

0.2

82.8

0.8

0.1

82.9

0

83.4

100

82.5

100

81.8

1.1

78.5

0.3

82.7

64.3

0.2

73.4

100

82.1

100

81.5

99.9

82.6

99.9

79.4

0.5

82.8

56.2

0.3

82.8

0

75.9

1.0

69.2

2.3

80.6

0

80.1

1.6

80.6

60.1

0.4

83.1

0

78.6

1.1

74.9

1.1

81.7

0.1

80.1

2.6

82.3

52.5

0.5

83.1

0.1

82.6

0.7

71.4

2.5

81.7

0

80.4

2.7

80.2

59.4

Table 33. Search for the best results by the grid-search for ABL on VGGFace2.

Î³

BadNets

ACC

ASR

Blend

ACC

ASR

WaNet

ACC

ASR

IAB

ACC

ASR

Refool

ACC

ASR

CLB

ACC

ASR

0

90.9

65.2

90.1

96.7

90.3

89.7

91.2

99.3

90.9

56.1

91.4

0.3

0.1

90.4

99.9

90.6

97.6

91.8

100

91.2

79.1

91.1

51.1

90.6

0.3

0.2

91.2

19.6

90.2

100

92.6

74.6

91.3

59.7

91.3

51.7

90.9

0.1

0.3

90.8

85.4

91.1

99.9

91.9

99.8

92.0

80.1

91.6

62.8

91.2

0.1

0.4

90.5

100

90.3

99.9

91.5

81.4

92.8

100

90.0

58.8

90.8

0

0.5

90.2

100

90.8

100

91.3

81.1

91.2

100

90.2

61.2

91.3

0

Table 34. Search for the best results by the grid-search for DPSGD on CIFAR-10.

BadNets

Blend

WaNet

IAB

Refool

CLB

Ïƒ

ACC

ASR

ACC

ASR

ACC

ASR

ACC

ASR

ACC

ASR

ACC

ASR

0

10.1

100

10.9

100

10.2

100

10.1

100

10.9

100

10.1

0

0.01

85.5

100

84.4

87.2

84.3

99.9

85.9

99.7

83.2

91.7

84.6

38.2

0.05

79.2

99.8

68.5

63.1

76.4

94.5

77.7

99.8

76.8

82.7

76.7

10.3

0.1

67.2

100

64.3

75.2

63.5

75.4

65.3

99.8

65.4

70.2

65.1

8.2

0.2

55.9

10.9

56.7

37.0

55.1

15.8

54.4

99.8

55.4

59.2

55.7

7.6

