PECAN: A Deterministic Certiﬁed Defense Against Backdoor Attacks

arXiv:2301.11824v1 [cs.CR] 27 Jan 2023

Yuhao Zhang 1 Aws Albarghouthi 1 Loris D’Antoni 1

Abstract
Neural networks are vulnerable to backdoor poisoning attacks, where the attackers maliciously poison the training set and insert triggers into the test input to change the prediction of the victim model. Existing defenses for backdoor attacks either provide no formal guarantees or come with expensive-to-compute and ineffective probabilistic guarantees. We present PECAN, an efﬁcient and certiﬁed approach for defending against backdoor attacks. The key insight powering PECAN is to apply off-the-shelf test-time evasion certiﬁcation techniques on a set of neural networks trained on disjoint partitions of the data. We evaluate PECAN on image classiﬁcation and malware detection datasets. Our results demonstrate that PECAN can (1) signiﬁcantly outperform the state-of-the-art certiﬁed backdoor defense, both in defense strength and efﬁciency, and (2) on real backdoor attacks, PECAN can reduce attack success rate by order of magnitude when compared to a range of baselines from the literature.
1. Introduction
Deep learning models are vulnerable to backdoor poisoning attacks (Saha et al., 2020; Turner et al., 2019), which assume that the attackers can maliciously poison a small fragment of the training set before model training and add triggers to inputs at test time. As a result, the prediction of the victim model that was trained on the poisoned training set will diverge in the presence of a trigger in the test input.
Effective backdoor attacks have been proposed for various domains, such as image recognition (Gu et al., 2017), sentiment analysis (Qi et al., 2021), and malware detection (Severi et al., 2021). For example, Severi et al. (2021) can break malware detection models as follows: The attacker poisons a small portion of benign software in the training set by modifying the values of the most important features so that
1Department of Computer Science, University of WisconsinMadison, Madison, USA. Correspondence to: Yuhao Zhang <yuhaoz@cs.wisc.edu>.

the victim model recognizes these values as evidence of the benign prediction. At test time, the attacker inserts a trigger by changing the corresponding features of malware to camouﬂage it as benign software and thus making it bypass the examination of the victim model. Thus, backdoor attacks are of great concern to the safety and security of deep learning models and systems, particularly as training data is gathered from different sources, e.g., via web scraping.
Several works have studied defenses against various types of attacks. We identify two limitations with these defenses. First, many existing approaches only provide empirical defenses that are speciﬁc to certain attacks and do not generalize to all backdoor attacks. Second, existing certiﬁed defenses—i.e., approaches that produce robustness certiﬁcates—are either unable to handle backdoor attacks, or are probabilistic (instead of deterministic), and therefore expensive and ineffective in practice.
Why certiﬁcation? A defense against backdoor attacks should construct effective certiﬁcates (proofs) that the learned model can indeed defend against backdoor attacks. Empirical defenses (Geiping et al., 2021a; Liu et al., 2018) do not provide certiﬁcates, can only defend against speciﬁc attacks, and can be bypassed by new unaccounted-for attacks (Wang et al., 2020b; Koh et al., 2022). Certiﬁcation has been successful at building models that are provably robust to trigger-less poisoning attacks and evasion attacks, but models trained to withstand such attacks are still weak against backdoor attacks. The trigger-less attack (Zhu et al., 2019; Shafahi et al., 2018; Aghakhani et al., 2021; Geiping et al., 2021b) assumes the attacker can poison the training set but cannot modify the test inputs, e.g., adding triggers, while the evasion attack (Madry et al., 2018) assumes the attacker modiﬁes the test inputs but cannot poison the training set. Existing certiﬁed defenses against trigger-less and evasion attacks, e.g., DPA (Levine & Feizi, 2021) and CROWNIBP (Zhang et al., 2020), cannot defend against backdoor attacks as they can either defend against the poison in the training data or the triggers at test time, but not both. As we show in the experiments, we can break these certiﬁed defenses using a backdoor attack (Section 5.3).
Why determinism? It is desirable for a certiﬁed defense to be deterministic because probabilistic defenses (Zhang et al., 2022b; Weber et al., 2020) typically require one to

PECAN: A Deterministic Certiﬁed Defense Against Backdoor Attacks

D
Step 1: Dataset Partitioning

...

D1

D2

Dn

Training

...

Testing

AD1

AD2

ADn

x 7

Step 2: Evasion Certiﬁcation

1

...

7

cert

abstain

...

cert

y∗: the top label, y : runner-up label, N1: # of certiﬁed y∗, N2: # of certiﬁed y ,
N3: # of abstain.
Step 3: Aggregation

y∗: prediction of PECAN,

N1 −N2−N3−1y∗ >y 2

: certiﬁed radius.

Figure 1. An overview of our approach PECAN.

retrain thousands of models when performing predictions for a single test input. Retraining can be mitigated by Bonferroni correction, which allows reusing the trained models for a ﬁxed number of predictions. However, retraining is still necessary after a short period, making it hard to deploy these defenses in practice. On the other hand, deterministic defenses (Levine & Feizi, 2021; Wang et al., 2022b) can reuse the trained models an arbitrary number of times when producing certiﬁcates for different test inputs. Furthermore, probabilistic defenses for backdoor attacks, e.g., BagFlip (Zhang et al., 2022b), need to add noise to the training data, resulting in low accuracy for datasets that cannot tolerate too much noise when training (Section 5.2).
PECAN In this paper, we propose PECAN (Partitioning data and Ensembling of Certiﬁed neurAl Networks), a deterministic certiﬁed defense against backdoor attacks for neural networks. The key insight underlying PECAN is that we can take any off-the-shelf technique for evasion certiﬁcation and use it to construct a certiﬁed backdoor defense. This insight results in a simple implementation and allows us to seamlessly leverage future advances in evasion certiﬁcation algorithms. Speciﬁcally, PECAN trains a set of neural networks on disjoint partitions of the dataset, and then applies evasion certiﬁcation to the neural networks. By partitioning the dataset, we analytically bound the number of poisoned data seen per neural network; by employing evasion certiﬁcation, we bound the number of neural networks that are robust in the face of triggers. Using this information, we efﬁciently derive a backdoor-robustness guarantee.

Figure 1 illustrates the workﬂow of PECAN. In Step 1, inspired by deep partition aggregation (Levine & Feizi, 2021), PECAN deterministically partitions a dataset into multiple disjoint subsets. This step ensures that a poisoned data item only affects a single partition. In Step 2, PECAN trains an ensemble of neural networks, one on each partition. At test time, PECAN performs evasion certiﬁcation to check which neural networks are immune to triggers; those that are not immune (or that cannot be proven immune) abstain from performing a prediction. Finally, in Step 3, PECAN aggregates the results of the ensemble and produces a prediction together with a robustness certiﬁcate: the percentage of the poisoned data in the training set that the training process can tolerate, the certiﬁed radius.
We evaluate PECAN on two three datasets, MNIST, CIFAR10, and EMBER. First, we show that PECAN outperforms or competes with BagFlip, the state-of-the-art probabilistic certiﬁed defense against backdoor attacks. Furthermore, BagFlip takes hours to compute the certiﬁcate, while PECAN only takes a few seconds. Second, when we evaluate PECAN against a concrete known backdoor attack (Severi et al., 2021), PECAN reduces the attack success rate to 1.85%, while DPA and CROWN-IBP fail to defend against the backdoor attack on 18.05% and 15.24% of the cases, respectively. The results show that PECAN can defend against a known backdoor attack while other baselines, such as DPA and CROWN-IBP, cannot.
2. Related Work
Deep learning models are vulnerable to backdoor attacks (Saha et al., 2020; Turner et al., 2019). Although many empirical defenses (Geiping et al., 2021a; Liu et al., 2018) have been proposed, recent works (Wang et al., 2020b; Koh et al., 2022) show that new attacks can break these empirical defenses. Therefore, certiﬁed defense is crucial for defending against backdoor attacks.
Certiﬁed defenses against backdoor attacks Existing certiﬁcation approaches provide probabilistic certiﬁcates by extending randomized smoothing (Cohen et al., 2019; Dvijotham et al., 2020; Lee et al., 2019), originally proposed to defend against adversarial evasion attacks, to defend against backdoor attacks. BagFlip (Zhang et al., 2022b) is the state-of-the-art model-agnostic probabilistic defense against feature-ﬂipping backdoor attacks. Wang et al. (2020a); Weber et al. (2020) proposed backdoor-attack defenses that are also model-agnostic, but are less effective than BagFlip. PECAN is deterministic and therefore less expensive and more effective than these defenses. Probabilistic defenses are model-agnostic; while PECAN is evaluated on neural networks, it can work for any machine learning model as long as a deterministic evasion certiﬁcation approach of the model is available. Weber et al. (2020) proposed a determin-

PECAN: A Deterministic Certiﬁed Defense Against Backdoor Attacks

istic de-randomized smoothing approach for kNN classiﬁers. Their approach computes the certiﬁcates using an expensive dynamic programming algorithm, whereas PECAN’s certiﬁcation algorithm has constant time complexity.
Certiﬁed defenses against trigger-less attacks Many approaches provide certiﬁcates for trigger-less attacks. Jia et al. (2021) use bootstrap aggregating (Bagging). Chen et al. (2020) extended Bagging with new selection strategies. Rosenfeld et al. (2020) defend against label-ﬂipping attacks on linear classiﬁers. Differential privacy (Ma et al., 2019) can also provide probabilistic certiﬁcates for triggerless attacks. DPA (Levine & Feizi, 2021) is a deterministic defense that partitions the training set and ensembles the trained classiﬁers. Wang et al. (2022b) proposed FA, an extension of DPA, by introducing a spread stage. A conjecture proposed by Wang et al. (2022a) implies that DPA and FA are asymptotically optimal defenses against trigger-less attacks. Chen et al. (2022) proposed to compute collective certiﬁcates, while PECAN computes sample-wise certiﬁcates. Jia et al. (2020); Meyer et al. (2021); Drews et al. (2020) provide certiﬁcates for nearest neighborhood classiﬁers and decision trees. The approaches listed above only defend against trigger-less attacks, while PECAN is a deterministic approach for backdoor attacks.
Certiﬁed defenses against evasion attacks There are two lines of certiﬁed defense against evasion attacks: complete certiﬁcation (Wang et al., 2021; Zhang et al., 2022a; Katz et al., 2019) and incomplete certiﬁcation (Xu et al., 2020; Zhang et al., 2021; Singh et al., 2019). The complete certiﬁed defenses either ﬁnd an adversarial example or generate proof that all inputs in the given perturbation space will be correctly classiﬁed. Compared to the complete certiﬁed defenses, the incomplete ones will abstain from predicting if they cannot prove the correctness of the prediction because their techniques will introduce over-approximation. The complete approaches do not have over-approximation issues but require expensive veriﬁcation algorithms such as branch and bound. Our implementation of PECAN uses an incomplete certiﬁed approach CROWN-IBP (Zhang et al., 2020) because it is the best incomplete approach, trading off between efﬁciency and the degree of over-approximation.
3. Problem Deﬁnition
Given a dataset D = {(x1, y1), . . . , (xn, yn)}, a (test) input x, and a machine learning algorithm A, we write AD to denote the machine learning model learned on dataset D by the algorithm A, and AD(x) to denote the output label predicted by the model AD on input x. We assume the algorithm will behave the same if trained on the same dataset across multiple runs. This assumption can be guaranteed by ﬁxing the random seeds during training.

We are interested in certifying that if an attacker has poisoned the dataset, the model we have trained on the dataset will still behave “well” on the test input with maliciously added triggers. Before describing what “well” means, we need to deﬁne the perturbation spaces of the dataset and the test input, i.e., what possible changes the attacker could make to the dataset and the test input.
Perturbation space of the dataset Following Levine & Feizi (2021), we deﬁne a general perturbation space over the dataset, allowing attackers to delete, insert, or modify training examples in the dataset. Given a dataset D and a radius r ≥ 0, we deﬁne the perturbation space as the set of datasets that can be obtained by deleting or inserting up to r examples in D:
Sr(D) = D | |D D| ≤ r ,
where A B is the symmetric difference of sets A and B. Intuitively, r quantiﬁes how many examples need to be deleted or inserted to transform from D to D.
Example 3.1. If the attacker modiﬁes one training example x ∈ D to another training example x to form a poisoned dataset D = (D \ {x}) ∪ {x}. Then D ∈ S2(D) but D ∈/ S1(D) because Sr(D) considers one modiﬁcation as one deletion and one insertion.
Note that we assume a more general perturbation space of the training set than the one considered by Zhang et al. (2022b); Weber et al. (2020); Wang et al. (2020a); our work allows inserting and deleting examples instead of just modifying existing training examples.
Perturbation space of the test input We write π(x) to denote the set of perturbed examples that an attacker can transform the example x into. Formally, the perturbation space π(x) can be deﬁned as the lp norm ball with radius s around the test input x,
π(x) = {x | x − x p ≤ s}
Example 3.2. BagFlip (Zhang et al., 2022b) considers the l0 feature-ﬂip perturbation Fs(x), which allows the attacker to modify up to s features in an input x,
Fs(x) = {x | x − x 0 ≤ s}
Threat models Next, we deﬁne what type of guarantees we are interested in our learning algorithm and model. We consider backdoor attacks, where the attacker can perturb both the training set and the test input. For the training set, we assume we are given a perturbation space Sr(D) of the training set D with a radius r ≥ 0. For the test input, we assume a perturbation space π(x) of the test input x with a given lp norm and the radius s.

PECAN: A Deterministic Certiﬁed Defense Against Backdoor Attacks

We say that an algorithm A is robust to a backdoor attack on a backdoored test input x if the algorithm trained on any perturbed dataset D would predict the backdoored input x the same as AD(x). Formally,
∀D ∈ Sr(D), x ∈ π(x). AD(x) = AD(x) (1)
Remark 3.1. When r = 0, Eq 1 degenerates to evasion robustness, i.e., ∀x ∈ π(x). AD(x) = AD(x), because S0(D) = {D}.
Given a large enough radius r, an attacker can always change enough inputs and succeed at breaking robustness. Therefore, we will typically focus on computing the maximal radius r for which we can prove that Eq 1 for given perturbation spaces Sr(D) and π(x). We refer to this quantity as the certiﬁed radius.
Certiﬁed guarantees This paper aims to design a certiﬁable algorithm A, which can defend against backdoor attacks, and to compute the certiﬁed radius of A. In our experiments (Section 5.2), we suppose a given benign dataset D and a benign test input x, and we certiﬁably quantify the robustness of the algorithm A against backdoor attacks by computing the certiﬁed radius.
In Section 5.3, we also experiment with how the certiﬁable algorithm A defends the backdoor attacks if a poisoned dataset D and a test input x with malicious triggers are given, but the clean data is unknown. We theoretically show that we can still compute the certiﬁed radius if the clean data D and x are unknown in Section 4.3.
4. The PECAN Certiﬁcation Technique
Our approach, which we call PECAN (Partitioning data and Ensembling of Certiﬁed neurAl Networks), is a deterministic certiﬁcation technique that defends against backdoor attacks. Given a learning algorithm A, we show how to automatically construct a new learning algorithm A¯ with certiﬁed backdoor-robustness guarantees (Equation (1)) in Section 4.1. In Section 4.2, we prove the certiﬁed backdoorrobustness guarantees (Equation (1)) provided by A¯. We further discuss how A¯ can defend against a backdoored dataset and formally justify our discussion in Section 4.3.
4.1. Constructing Certiﬁable Algorithm A¯
The key idea of PECAN is that we can take any off-the-shelf technique for evasion certiﬁcation and use it to construct a certiﬁed backdoor defense. Intuitively, PECAN uses the evasion certiﬁcation to defend against the possible triggers at test time, and it encapsulates the evasion certiﬁcation in deep partition aggregation (DPA) (Levine & Feizi, 2021) to defend against training set poisoning.
Given a dataset D, a test input x, and a machine learning

algorithm A, PECAN produce a new learning algorithm A¯ as described in the following steps (shown in Figure 1),
Dataset Partitioning We partition the dataset D into n disjoint sub-datasets, denoted as D1, . . . , Dn, using a hash function that deterministically maps each training example into a sub-dataset Di. Train n classiﬁers AD1 , . . . , ADn on these sub-datasets.
Evasion Certiﬁcation We certify whether the prediction of each classiﬁer ADi is robust under the perturbation space π(x) by any evasion certiﬁcation approach for the learning algorithm, e.g., CROWN-IBP for neural networks (Xu et al., 2020). Formally, the certiﬁcation approach determines whether the following equation holds,

∀x ∈ π(x). ADi (x) = ADi (x)

(2)

We denote the output of cOatnheeriwthieser ,bAe πDAiπD(xi ()x=)

each certiﬁcation as AπDi (x), which = cert, meaning Eq 2 is certiﬁed.
abstain, meaning the certiﬁcation

approach cannot certify Eq 2.

Aggregation We compute the top label y∗ by aggre-

gating all
argmax
y∈C

pni=re1d1icAtD ioin(xs)=fryo,mwhAeDrei

(x). C=

Concretely, y∗ {0, 1, . . .} is the

set

of possible labels. Note that if a tie happens when taking

the argmax, we break ties deterministically by setting the

smaller label index as y∗. We denote the runner-up label

as y as argmax
y∈C∧y=y∗

n i=1

1ADi

(x)=y .

We

count

the

number

of certiﬁed predictions equal to y∗ as N1, the number of

certiﬁed predictions equal to y as N2, and the number of

abstentions as N3,

n

N1 =

1ADi (x)=y∗∧AπDi (x)=cert,

i=1

n

N2 =

1ADi (x)=y ∧AπDi (x)=cert,

i=1

n

N3 =

1AπDi (x)=abstain.

i=1

We set the prediction A¯D(x) as y∗. We compute the cer-

tiﬁed radius r in the following two cases. If N1 − N2 −

N3 − 1y∗>y < 0, we set r as , i.e., a value denoting no certiﬁcation. In this case, PECAN cannot certify that A¯ is

robust to evasion attacks even if the dataset is not poisoned.

Otherwise, cial case is

we compute r r = 0, when

PaEs CNA1N−Nc2a−nN2c3e−rt1ifyy∗>Ay¯

. is

A sperobust

to evasion attacks, but cannot certify that it is robust if the

dataset is poisoned.

We note that the computation of the certiﬁed radius is equivalent to DPA when no classiﬁer abstains, i.e., N3 = 0,

PECAN: A Deterministic Certiﬁed Defense Against Backdoor Attacks

D

D1

D2

...

...

...

Dn

x

7

5

7

7

abstain abstain cert cert

Dabs Attacked by x 7→y 5→y

Dbd Attacked by D 7→y 7→y

7

1

cert

cert

Dsafe

Clean

7

1

We denote the numbers of the original top prediction y∗ and the original runner-up prediction y on the backdoored data D and x as Ny∗ and Ny , respectively. Formally,

n

Ny∗ =

1ADi (x)=y∗ ,

i=1

n

Ny =

1ADi (x)=y

i=1

Next, we prove Eq 3 for any backdoored data D and x by showing that

Ny∗ ≥ Ny + 1y∗>y

(4)

D1

D2

...

...

...

Dn

x

D
Figure 2. An illustration of the proof of Theorem 4.1. It shows the worst case for PECAN, where the attacker can change all predictions in Dabs and Dbd to the runner-up label y . Note that we group Dabs, Dbd, and Dsafe together to ease illustration.

4.2. Proving the Soundness of PECAN
In this section, we show that the prediction A¯D(x) and the certiﬁed radius r satisfy the certiﬁed backdoor-robustness guarantees (Equation (1)) by proving the following theorem.
Theorem 4.1 (Soundness of PECAN). Given a dataset D and a test input x, PECAN computes the prediction A¯D(x) and the certiﬁed radius as r. Then, either r = or

∀D ∈ Sr(D), x ∈ π(x). A¯D(x) = A¯D(x) (3)

Proof. For any poisoned dataset D, we partition D into n sub-datasets {D1, . . . , Dn} according to {D1, . . . , Dn} from the clean dataset D. Note that we can determine such a correspondence between Di and Di because our hash function is deterministic and only depends on each training example. We further divide {D1, . . . , Dn} into three disjoint parts Dabs, Dbd, and Dsafe in the following way,

• Dabs = {Di | AπDi (x) = abstain} are the subdatasets, on which A abstains from making the prediction on x. From the deﬁnition of N3, we have |Dabs| = N3. Intuitively, Dabs contains the subdatasets that can possibly be attacked by the test input x with malicious triggers.
• Dbd are the sub-datasets on which A does not abstain and are also poisoned, i.e., each of them has at least one training example removed or inserted. Even though we do not know the exact sub-datasets in Dbd, we know |Dbd| ≤ r because D ∈ Sr(D) constrains that there are at most r such poisoned sub-datasets.
• Dsafe = {Di | Di = Di ∧ AπDi (x) = cert} contains the clean sub-datasets, on which A does not abstain.

We prove Eq 4 by showing a lower bound of Ny∗ is N1 − r
and an upper bound of Ny is N2 + r + N3. Together with the deﬁnition of r, we can prove Eq 4 because we have,

Ny∗ − Ny − 1y∗>y

≥N1 − r − (N2 + r + N3) − 1y∗>y

=N1 − N2 − 2r − N3 − 1y∗>y

=N1 − N2 − 2

N1 − N2 − N3 − 1y∗>y 2

− N3 − 1y∗>y

≥N1 − N2 − (N1 − N2 − N3 − 1y∗>y ) − N3 − 1y∗>y

=0.

Note that the second last line holds iff N1 − N2 − N3 − 1y∗>y ≥ 0. Otherwise, we have r = .
As shown in Figure 2, the lower bound of Ny∗ can be computed by noticing that 1) the attacker can change any prediction in Dbd from y∗ to another label because these datasets are poisoned, 2) the attacker can change any prediction in Dabs to another label because CROWN-IBP cannot certify the prediction under the evasion attacks, and 3) the attacker cannot change anything in Dsafe because of the guarantee of CROWN-IBP and Dsafe is not poisoned,
∀Di ∈ Dsafe, x ∈ π(x). ADi (x) = ADi (x) = ADi (x)
The upper bound of Ny can be computed by noticing that 1) the attacker can change any prediction in Dbd to y , 2) the attacker can change any prediction in Dabs to y , and 3) the attacker cannot change anything in Dsafe.
We complete the proof by showing that the best attack strategy of the attacker is to change the prediction of A¯ to the runner-up label y . If the attacker chooses to change the prediction of A¯ to another label y , denoted the counts as Ny , then the upper bound of Ny will be always smaller or equal to Ny .

4.3. PECAN under the Backdoored Data
The above algorithm and proof of PECAN assume that a clean dataset D and a clean test example x are already given. However, we may be interested in another scenario where the poisoned dataset D ∈ Sr(D) and the input example

PECAN: A Deterministic Certiﬁed Defense Against Backdoor Attacks

x ∈ π(x) with malicious triggers are given, and the clean data D and x are unknown. In other words, we want to ﬁnd the maximal radius r such that A¯D(x) = A¯D(x) for any D and x that can be perturbed to D and x by the perturbation Sr and π, respectively. Formally,
∀D, x. D ∈ Sr(D) ∧ x ∈ π(x) =⇒ A¯D(x) = A¯D(x) (5)
Intuitively, Eq 5 is the symmetrical version of Eq 1. Owing to the symmetrical deﬁnition of Sr and π, if we apply PECAN to the given poisoned data D, x, then the prediction A¯D(x) and the certiﬁed radius r satisfy the certiﬁed backdoor-robustness guarantee (Eq 5). The following theorem formally states the soundness of PECAN under the backdoored data. We prove Theorem 4.2 in Appendix A.
Theorem 4.2 (Soundness of PECAN under the backdoored data). Given a dataset D and a test input x, PECAN computes the prediction A¯D(x) and the certiﬁed radius as r. Then, either r = or Eq 5 holds.
5. Experiments
We implemented PECAN in Python and provided the implementation in the supplementary materials. In our evaluation, we use CROWN-IBP, implemented in auto-LiRPA (Xu et al., 2020), as the evasion defense approach for neural networks. We also use CROWN-IBP to train the classiﬁers in the dataset partitioning step since the classiﬁers trained by CROWN-IBP can improve the certiﬁcation rate in the evasion certiﬁcation step.
In Section 5.2, we evaluate the effectiveness and efﬁciency of PECAN by comparing it to BagFlip (Zhang et al., 2022b), the state-of-the-art probabilistic certiﬁed defense against backdoor attacks. In Section 5.3, we evaluate the effectiveness of PECAN under the backdoor attack (Severi et al., 2021) for malware detection and compare PECAN to other baselines, DPA and CROWN-IBP.
5.1. Experimental Setup
Datasets We conduct experiments on MNIST, CIFAR10, and EMBER (Anderson & Roth, 2018) datasets. MNIST is an image classiﬁcation dataset containing 60,000 training and 10,000 test examples. CIFAR10 is an image classiﬁcation dataset containing 50,000 training and 10,000 test examples. EMBER is a malware detection dataset containing 600,000 training and 200,000 test examples. Each example is a vector containing 2,351 features of the software.
Models For image classiﬁcation datasets MNIST and CIFAR10, we train fully-connected neural networks with four layers for PECAN, while BagFlip uses CNN and ResNet for MNIST and CIFAR10, respectively. We do not use CNN and ResNet because CROWN-IBP used in PECAN has a higher abstention rate for deeper and more complex neu-

ral network structures. We use the same fully-connected neural network for EMBER as in related works (Zhang et al., 2022b; Severi et al., 2021). We use the same data augmentation for PECAN and other baselines.

Metrics For each test input xi, yi, the algorithm A¯ will predict a label and the certiﬁed radius ri. In this section, we assume that the attacker had modiﬁed R% examples in the training set. We denote R as the modiﬁcation amount. We summarize all the metrics used as follows,

Certiﬁed Accuracy denotes the percentage of test examples

that are correctly classiﬁed and whose certiﬁed radii are

no

less

than

R,

i.e.,

1 m

m i=1

1A¯D (xi )=yi ∧

ri |D|

≥2R%,

where

m and |D| are the sizes of test set and training set, respec-

tively. Notice that there is a factor of 2 on the modiﬁcation

amount R because Sr(D) considers one modiﬁcation as one

insertion and one deletion, as illustrated in Example 3.1.

Normal Accuracy denotes the percentage of test examples

that are correctly classiﬁed by the algorithm without certiﬁ-

cation,

i.e.,

1 m

m i=1

1A¯D (xi)=yi

.

Attack Success Rate (ASR). In Section 5.3, we are interested

in how many test examples are certiﬁed but wrongly clas-

siﬁed

by

the

classiﬁer,

i.e.,

1 m

1 . m

i=1

A¯D

(xi

)=yi

∧

ri |D|

≥2R%

We denote the above quantity as the attack success rate. We

note that a prediction can still be incorrect even if it is cer-

tiﬁed by PECAN because the classiﬁer can have incorrect

predictions even when the data is clean.

Abstention

Rate

is

computed

as

1 m

m i=1

1

ri |D|

<2R%.

5.2. Effectiveness and Efﬁciency of PECAN
We evaluate the effectiveness and efﬁciency of PECAN on MNIST, CIFAR10, and EMBER under the backdoor attack with the l0 feature-ﬂip perturbation F1, which allows the attacker to modify up to one feature in an example. We compare PECAN to BagFlip, the state-of-the-art probabilistic certiﬁed defense against l0 feature-ﬂip backdoor attacks. Moreover, we note that PECAN needs to construct harder proofs than BagFlip because their deﬁnitions of perturbation space are different, as discussed in Appendix B.1.
In Appendix B.2, we evaluate the effectiveness of PECAN against the perturbation space with the l∞ norm.
Summary of the results PECAN achieves signiﬁcantly higher certiﬁed accuracy than BagFlip on CIFAR10 and EMBER. PECAN achieves competitive results on MNIST compared to BagFlip. PECAN has similar normal accuracy as BagFlip for all datasets. PECAN is more efﬁcient than BagFlip at computing the certiﬁed radius.

Setup We use the same hyper-parameters for BagFlip as reported in their paper for all datasets. For PECAN, we vary

PECAN: A Deterministic Certiﬁed Defense Against Backdoor Attacks

Certiﬁable Accuracy

100

BagFlip

80

PECAN-a

60

PECAN-b

40

20

0

0

2

4

0 0.5 1 1.5 2

Modiﬁcation Amount R·(1%0)−2

·10−2

(a) CIFAR10 F1

(b) EMBER F1

Figure 3. Comparison to BagFlip on CIFAR10 and EMBER, show-

ing the normal accuracy (dotted lines) and the certiﬁed accuracy

(solid lines) at different modiﬁcation amounts R. For CIFAR10:

a = 50 and b = 100. For EMBER: a = 200 and b = 400.

Certiﬁable Accuracy

100

BagFlip

80

PECAN-600

60

PECAN-1200

40

PECAN-2000

20

0

0

0.5

1

1.5

Modiﬁcation Amount R (%)

Figure 4. Comparison to BagFlip on MNIST, showing the normal accuracy (dotted lines) and the certiﬁed accuracy (solid lines) at different modiﬁcation amounts R.

n, the number of partitions, to ensure a fair comparison between BagFlip. Appendix B.1 presents a detailed discussion of hyper-parameter settings for BagFlip and PECAN. We denote PECAN with different settings of n as PECAN-n.
BagFlip achieves meaningful results only on MNIST, where we also tune the parameter n of PECAN to 2000 to achieve the same certiﬁed accuracy of BagFlip at R = 0 and compare their results following the practice in related works (Jia et al., 2021; 2020).
Results Figure 3 shows the comparison between PECAN and BagFlip on CIFAR10 and EMBER. PECAN achieves signiﬁcantly higher certiﬁed accuracy than BagFlip across all modiﬁcation amounts R and the similar normal accuracy as BagFlip for both datasets.
BagFlip performs poorly on CIFAR10 and EMBER because these two datasets cannot tolerate the high level of noise that the BagFlip algorithm adds to the training data. Speciﬁcally, BagFlip can add 20% noise to the training data of MNIST, i.e., a feature (pixel) in a training example will be ﬂipped to another value with 20% probability. However, for CIFAR10 and EMBER, this probability has to be decreased to 5% to maintain normal accuracy.
Figure 4 shows the comparison between PECAN and BagFlip on MNIST. PECAN achieves competitive results compared to BagFlip. We ﬁnd that two approaches have similar normal accuracy. Comparing PECAN-600 and PECAN-1200 with BagFlip, we ﬁnd that 1) PECAN-600 and PECAN-1200 achieves higher certiﬁed accuracy than BagFlip when R ∈ [0, 0.25] and R ∈ [0, 0.17], respec-

tively, and 2) BagFlip has non-zero certiﬁed accuracy when R ∈ [0.5, 1.5], where the certiﬁed accuracy of PECAN-600 and PECAN-1200 is zero. Comparing PECAN-2000 with BagFlip, we ﬁnd that BagFlip outperforms PECAN-2000 across all modiﬁcation amounts R.
We argue that the gap of certiﬁed accuracy between PECAN2000 and BagFlip mainly comes from the different definitions of the perturbation spaces as discussed in Appendix B.1. Moreover, the root cause of this difference is owing to the probabilistic nature of BagFlip.
PECAN is more efﬁcient than BagFlip at computing the certiﬁed radius. PECAN computes the certiﬁed radius in a constant time complexity via the closed-form solution in the aggregation step. However, in our experiment of the MNIST dataset, BagFlip requires 8 hours to prepare a lookup table because BagFlip does not have a closed-form solution for computing the certiﬁed radius.
5.3. PECAN under the Backdoored Data
We evaluate the effectiveness of PECAN under the backdoor attack (Severi et al., 2021) for malware detection on the EMBER dataset. We do not compare PECAN to BagFlip because BagFlip has poor certiﬁed accuracy on EMBER, as shown in Figure 3. We also evaluate other baselines, DPA and CROWN-IBP, which do not aim to defend against backdoor attacks. DPA is the certiﬁed defense against trigger-less attacks, and CROWN-IBP is the certiﬁed defense against evasion attacks. We also present the results of the victim classiﬁers without any defense. Appendix B.4 shows that the empirical defense spectral signatures (Tran et al., 2018) cannot defend against the backdoor attack.
Summary of the results PECAN reduces the ASR of the victim model on the test set with malicious triggers from 41.33% to 1.85%, while the other baselines fail to defend against the backdoor attack. Being the most conservative, PECAN has the highest abstention rate.
Setup We use Severi et al. (2021) to generate backdoored data by modifying 0.1% training examples and adding triggers into the test inputs that should be labeled as malware to fool the victim model to predict the malware with malicious triggers as benign software (non-malware). We generate three poisoned datasets D1, D2, D3 and their corresponding test sets with triggers by perturbations F1, F2, and F3, which allow the attacker to modify up to one, two, and three features in an example, respectively.
We report the results of all approaches on the malware test sets with triggers and the malware test sets without triggers, i.e., the original malware test set. The results on the non-malware test sets without triggers can be found in Appendix B.3. ASR on malware is a much more critical metric

PECAN: A Deterministic Certiﬁed Defense Against Backdoor Attacks

100 ASR

80

Correct

Abstain

60

40

20

0 D1D2D3
PECAN

D1D2D3
DPA

D1D2D3
C-IBP

D1D2D3
NoDef

Figure 5. Results of PECAN, DPA, CROWN-IBP (C-IBP), and vanilla model without defense (NoDef) trained on three poisoned EMBER datasets when evaluated on the malware test set with malicious triggers. We note that NoDef does not have abstention rates because it does not use any defense.

100 ASR

80

Correct

Abstain

60

40

20

0 D1D2D3
PECAN

D1D2D3
DPA

D1D2D3
C-IBP

D1D2D3
NoDef

Figure 6. Results of PECAN, DPA, C-IBP, and NoDef when evaluated on the (original) malware test set without malicious triggers.

than the ASR on non-malware, because the former shows how many pieces of malware can bypass the classiﬁer.
For PECAN and DPA, we show their results at modiﬁcation amount R = 0.1%. We show CROWN-IBP results against the perturbations F1, F2, and F3 regardless of R because CROWN-IBP does not consider R.
Results Figures 5 and 6 show the ASR, accuracy, and abstention rate of all the approaches on the malware test set with and without triggers, respectively. Table 1 in the appendix shows the detailed numbers. Note that PECAN is the only certiﬁed approach for backdoor attacks. The results of other baselines can be seen as empirical because DPA and CROWN-IBP certify a different goal, and NoDef has no defense.
PECAN can defend against the backdoor attack on the EMBER dataset. Figures 5 and 6 show that PECAN has the lowest ASR 1.85% and 1.03% on both malware test sets with and without triggers on average, compared to DPA (18.05%, 1.98%), CROWN-IBP (15.24%, 6.82%), and NoDef (41.33%, 2.12%).

Test Set Percentage

100

80

60

40

20

0

0

5 · 10−2

0.1 0

Modiﬁcation Amount R (%)

(a) PECAN

Correct ASR
Abstain

5 · 10−2

0.1

(b) DPA

Figure 7. Comparison between PECAN and DPA trained on D3 across all modiﬁcation amount R when evaluated on the malware test set with triggers.

DPA and CROWN-IBP fail to defend against the backdoor attack. The average ASR of DPA and CROWN-IBP on the malware test set with triggers are 18.05% and 15.24% in Figure 5, respectively, meaning that many malware with triggers can bypass their defenses. The average ASR of DPA on the malware test set without triggers, 1.98%, is much lower than its ASR on the one with triggers, 18.05%, which shows that DPA successfully defends against triggerless attacks when the test input does not have any trigger. CROWN-IBP has high ASR on both the malware test sets with and without triggers, as CROWN-IBP cannot defend against the poison in the training sets.
PECAN has higher abstention rates than other approaches. On average, PECAN abstains from 50.41% predictions compared to DPA (34.73%) and CROWN-IBP (26.44%). We further compare the accuracy, ASR, and abstention rate of PECAN and DPA across all modiﬁcation amount R when trained on D3 in Figure 7. The results on D1 and D2 are shown in Appendix B.5. We can observe that PECAN has a much lower ASR than DPA across all modiﬁcation amounts. Meanwhile, Figure 7 shows that the certiﬁcation of PECAN might be over-conservative because the ASR is low (3.17%) even when we regard D3 as non-poisoned (when R = 0), yet D3 is actually poisoned.

6. Conclusion, Limitations, and Future Work
We presented PECAN, a deterministic certiﬁed approach to effectively and efﬁciently defend against backdoor attacks. We foresee many future improvements to PECAN. First, we implemented PECAN as a certiﬁed defense specialized for neural networks because the evasion certiﬁcation step, CROWN-IBP, is limited to neural networks. However, we can replace CROWN-IBP with an evasion certiﬁcation approach for another machine learning model to get a corresponding backdoor defense for that model. Second, we adopt the idea of deep partition aggregation (DPA) to design the partition and aggregation steps in PECAN. We can improve these steps by using ﬁnite aggregation (FA) (Wang et al., 2022b), which extends DPA and gives higher certiﬁed accuracy. Third, during the certiﬁcation of evasion attacks, we need to propagate the abstraction of the same test input

PECAN: A Deterministic Certiﬁed Defense Against Backdoor Attacks
through thousands of neural networks that have different weights but the same architecture. Sharing the propagation results among different neural networks (Fischer et al., 2022) can greatly improve the efﬁciency of PECAN and may enable using complete certiﬁcation methods.

PECAN: A Deterministic Certiﬁed Defense Against Backdoor Attacks

References
Aghakhani, H., Meng, D., Wang, Y., Kruegel, C., and Vigna, G. Bullseye polytope: A scalable clean-label poisoning attack with improved transferability. In IEEE European Symposium on Security and Privacy, EuroS&P 2021, Vienna, Austria, September 6-10, 2021, pp. 159– 178. IEEE, 2021. doi: 10.1109/EuroSP51992.2021. 00021. URL https://doi.org/10.1109/Euro SP51992.2021.00021.
Anderson, H. S. and Roth, P. EMBER: an open dataset for training static PE malware machine learning models. CoRR, abs/1804.04637, 2018. URL http://arxiv. org/abs/1804.04637.
Chen, R., Li, J., Wu, C., Sheng, B., and Li, P. A framework of randomized selection based certiﬁed defenses against data poisoning attacks. CoRR, abs/2009.08739, 2020. URL https://arxiv.org/abs/2009.08739.
Chen, R., Li, Z., Li, J., Yan, J., and Wu, C. On collective robustness of bagging against data poisoning. In Chaudhuri, K., Jegelka, S., Song, L., Szepesva´ri, C., Niu, G., and Sabato, S. (eds.), International Conference on Machine Learning, ICML 2022, 17-23 July 2022, Baltimore, Maryland, USA, volume 162 of Proceedings of Machine Learning Research, pp. 3299–3319. PMLR, 2022. URL https://proceedings.mlr.press/ v162/chen22k.html.
Cohen, J. M., Rosenfeld, E., and Kolter, J. Z. Certiﬁed adversarial robustness via randomized smoothing. In Chaudhuri, K. and Salakhutdinov, R. (eds.), Proceedings of the 36th International Conference on Machine Learning, ICML 2019, 9-15 June 2019, Long Beach, California, USA, volume 97 of Proceedings of Machine Learning Research, pp. 1310–1320. PMLR, 2019. URL http://pr oceedings.mlr.press/v97/cohen19c.html.
Drews, S., Albarghouthi, A., and D’Antoni, L. Proving data-poisoning robustness in decision trees. In Donaldson, A. F. and Torlak, E. (eds.), Proceedings of the 41st ACM SIGPLAN International Conference on Programming Language Design and Implementation, PLDI 2020, London, UK, June 15-20, 2020, pp. 1083–1097. ACM, 2020. doi: 10.1145/3385412.3385975. URL https: //doi.org/10.1145/3385412.3385975.
Dvijotham, K. D., Hayes, J., Balle, B., Kolter, J. Z., Qin, C., Gyo¨rgy, A., Xiao, K., Gowal, S., and Kohli, P. A framework for robustness certiﬁcation of smoothed classiﬁers using f-divergences. In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net, 2020. URL https://openreview.net/forum ?id=SJlKrkSFPH.

Fischer, M., Sprecher, C., Dimitrov, D. I., Singh, G., and Vechev, M. T. Shared certiﬁcates for neural network veriﬁcation. In Shoham, S. and Vizel, Y. (eds.), Computer Aided Veriﬁcation - 34th International Conference, CAV 2022, Haifa, Israel, August 7-10, 2022, Proceedings, Part I, volume 13371 of Lecture Notes in Computer Science, pp. 127–148. Springer, 2022. doi: 10.1007/978-3-031-13185-1\ 7. URL https://doi. org/10.1007/978-3-031-13185-1_7.
Geiping, J., Fowl, L., Somepalli, G., Goldblum, M., Moeller, M., and Goldstein, T. What doesn’t kill you makes you robust(er): Adversarial training against poisons and backdoors. CoRR, abs/2102.13624, 2021a. URL https://arxiv.org/abs/2102.13624.
Geiping, J., Fowl, L. H., Huang, W. R., Czaja, W., Taylor, G., Moeller, M., and Goldstein, T. Witches’ brew: Industrial scale data poisoning via gradient matching. In 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net, 2021b. URL https://openreview.n et/forum?id=01olnfLIbD.
Gu, T., Dolan-Gavitt, B., and Garg, S. Badnets: Identifying vulnerabilities in the machine learning model supply chain. CoRR, abs/1708.06733, 2017. URL http://arxiv.org/abs/1708.06733.
Jia, J., Cao, X., and Gong, N. Z. Certiﬁed robustness of nearest neighbors against data poisoning attacks. CoRR, abs/2012.03765, 2020. URL https://arxiv.org/ abs/2012.03765.
Jia, J., Cao, X., and Gong, N. Z. Intrinsic certiﬁed robustness of bagging against data poisoning attacks. In Thirty-Fifth AAAI Conference on Artiﬁcial Intelligence, AAAI 2021, Thirty-Third Conference on Innovative Applications of Artiﬁcial Intelligence, IAAI 2021, The Eleventh Symposium on Educational Advances in Artiﬁcial Intelligence, EAAI 2021, Virtual Event, February 2-9, 2021, pp. 7961–7969. AAAI Press, 2021. URL https://ojs.aaai.org /index.php/AAAI/article/view/16971.
Katz, G., Huang, D. A., Ibeling, D., Julian, K., Lazarus, C., Lim, R., Shah, P., Thakoor, S., Wu, H., Zeljic, A., Dill, D. L., Kochenderfer, M. J., and Barrett, C. W. The marabou framework for veriﬁcation and analysis of deep neural networks. In Dillig, I. and Tasiran, S. (eds.), Computer Aided Veriﬁcation - 31st International Conference, CAV 2019, New York City, NY, USA, July 15-18, 2019, Proceedings, Part I, volume 11561 of Lecture Notes in Computer Science, pp. 443–452. Springer, 2019. doi: 10.1007/978-3-030-25540-4\ 26. URL https://do i.org/10.1007/978-3-030-25540-4_26.

PECAN: A Deterministic Certiﬁed Defense Against Backdoor Attacks

Koh, P. W., Steinhardt, J., and Liang, P. Stronger data poisoning attacks break data sanitization defenses. Mach. Learn., 111(1):1–47, 2022. doi: 10.1007/ s10994-021-06119-y. URL https://doi.org/10. 1007/s10994-021-06119-y.
Lee, G., Yuan, Y., Chang, S., and Jaakkola, T. S. Tight certiﬁcates of adversarial robustness for randomly smoothed classiﬁers. In Wallach, H. M., Larochelle, H., Beygelzimer, A., d’Alche´-Buc, F., Fox, E. B., and Garnett, R. (eds.), Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada, pp. 4911– 4922, 2019. URL https://proceedings.neur ips.cc/paper/2019/hash/fa2e8c4385712f 9a1d24c363a2cbe5b8-Abstract.html.
Levine, A. and Feizi, S. Deep partition aggregation: Provable defenses against general poisoning attacks. In 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net, 2021. URL https://openreview.n et/forum?id=YUGG2tFuPM.
Liu, K., Dolan-Gavitt, B., and Garg, S. Fine-pruning: Defending against backdooring attacks on deep neural networks. In Bailey, M., Holz, T., Stamatogiannakis, M., and Ioannidis, S. (eds.), Research in Attacks, Intrusions, and Defenses - 21st International Symposium, RAID 2018, Heraklion, Crete, Greece, September 10-12, 2018, Proceedings, volume 11050 of Lecture Notes in Computer Science, pp. 273–294. Springer, 2018. doi: 10.1007/978-3-030-00470-5\ 13. URL https://do i.org/10.1007/978-3-030-00470-5_13.
Ma, Y., Zhu, X., and Hsu, J. Data poisoning against differentially-private learners: Attacks and defenses. In Kraus, S. (ed.), Proceedings of the Twenty-Eighth International Joint Conference on Artiﬁcial Intelligence, IJCAI 2019, Macao, China, August 10-16, 2019, pp. 4732–4738. ijcai.org, 2019. doi: 10.24963/ijcai.2019/657. URL ht tps://doi.org/10.24963/ijcai.2019/657.
Madry, A., Makelov, A., Schmidt, L., Tsipras, D., and Vladu, A. Towards deep learning models resistant to adversarial attacks. In 6th International Conference on Learning Representations, ICLR 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings. OpenReview.net, 2018. URL https: //openreview.net/forum?id=rJzIBfZAb.
Meyer, A. P., Albarghouthi, A., and D’Antoni, L. Certifying robustness to programmable data bias in decision trees. In Ranzato, M., Beygelzimer, A., Dauphin, Y. N., Liang, P., and Vaughan, J. W. (eds.), Advances in Neural Information Processing Systems 34: Annual Conference on

Neural Information Processing Systems 2021, NeurIPS 2021, December 6-14, 2021, virtual, pp. 26276–26288, 2021. URL https://proceedings.neurips. cc/paper/2021/hash/dcf531edc9b229acf e0f4b87e1e278dd-Abstract.html.
Qi, F., Yao, Y., Xu, S., Liu, Z., and Sun, M. Turn the combination lock: Learnable textual backdoor attacks via word substitution. In Zong, C., Xia, F., Li, W., and Navigli, R. (eds.), Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, ACL/IJCNLP 2021, (Volume 1: Long Papers), Virtual Event, August 1-6, 2021, pp. 4873–4883. Association for Computational Linguistics, 2021. doi: 10.18653/v1/2021.acl-long.377. URL https://doi. org/10.18653/v1/2021.acl-long.377.
Rosenfeld, E., Winston, E., Ravikumar, P., and Kolter, J. Z. Certiﬁed robustness to label-ﬂipping attacks via randomized smoothing. In Proceedings of the 37th International Conference on Machine Learning, ICML 2020, 13-18 July 2020, Virtual Event, volume 119 of Proceedings of Machine Learning Research, pp. 8230–8241. PMLR, 2020. URL http://proceedings.mlr.press/ v119/rosenfeld20b.html.
Saha, A., Subramanya, A., and Pirsiavash, H. Hidden trigger backdoor attacks. In The Thirty-Fourth AAAI Conference on Artiﬁcial Intelligence, AAAI 2020, The Thirty-Second Innovative Applications of Artiﬁcial Intelligence Conference, IAAI 2020, The Tenth AAAI Symposium on Educational Advances in Artiﬁcial Intelligence, EAAI 2020, New York, NY, USA, February 7-12, 2020, pp. 11957– 11965. AAAI Press, 2020. URL https://ojs.aaai .org/index.php/AAAI/article/view/6871.
Severi, G., Meyer, J., Coull, S. E., and Oprea, A. Explanation-guided backdoor poisoning attacks against malware classiﬁers. In Bailey, M. and Greenstadt, R. (eds.), 30th USENIX Security Symposium, USENIX Security 2021, August 11-13, 2021, pp. 1487–1504. USENIX Association, 2021. URL https://www.usenix.org/conference/ usenixsecurity21/presentation/severi.
Shafahi, A., Huang, W. R., Najibi, M., Suciu, O., Studer, C., Dumitras, T., and Goldstein, T. Poison frogs! targeted clean-label poisoning attacks on neural networks. In Bengio, S., Wallach, H. M., Larochelle, H., Grauman, K., Cesa-Bianchi, N., and Garnett, R. (eds.), Advances in Neural Information Processing Systems 31: Annual Conference on Neural Information Processing Systems 2018, NeurIPS 2018, December 3-8, 2018, Montre´al, Canada, pp. 6106–6116, 2018. URL https://proceedings.neurips.cc/pap

PECAN: A Deterministic Certiﬁed Defense Against Backdoor Attacks

er/2018/hash/22722a343513ed45f14905e b07621686-Abstract.html.

10.48550/arXiv.2208.03309. URL https://doi.or g/10.48550/arXiv.2208.03309.

Singh, G., Gehr, T., Pu¨schel, M., and Vechev, M. T. An abstract domain for certifying neural networks. Proc. ACM Program. Lang., 3(POPL):41:1–41:30, 2019. doi: 10.1145/3290354. URL https://doi.org/10. 1145/3290354.
Tran, B., Li, J., and Madry, A. Spectral signatures in backdoor attacks. In Bengio, S., Wallach, H. M., Larochelle, H., Grauman, K., Cesa-Bianchi, N., and Garnett, R. (eds.), Advances in Neural Information Processing Systems 31: Annual Conference on Neural Information Processing Systems 2018, NeurIPS 2018, December 3-8, 2018, Montre´al, Canada, pp. 8011–8021, 2018. URL https://proceedings.neurips. cc/paper/2018/hash/280cf18baf4311c92a a5a042336587d3-Abstract.html.
Turner, A., Tsipras, D., and Madry, A. Label-consistent backdoor attacks. CoRR, abs/1912.02771, 2019. URL http://arxiv.org/abs/1912.02771.
Wang, B., Cao, X., Jia, J., and Gong, N. Z. On certifying robustness against backdoor attacks via randomized smoothing. CoRR, abs/2002.11750, 2020a. URL https://arxiv.org/abs/2002.11750.
Wang, H., Sreenivasan, K., Rajput, S., Vishwakarma, H., Agarwal, S., Sohn, J., Lee, K., and Papailiopoulos, D. S. Attack of the tails: Yes, you really can backdoor federated learning. In Larochelle, H., Ranzato, M., Hadsell, R., Balcan, M., and Lin, H. (eds.), Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, 2020b. URL https://proceedings.neurips. cc/paper/2020/hash/b8ffa41d4e492f0fad 2f13e29e1762eb-Abstract.html.
Wang, S., Zhang, H., Xu, K., Lin, X., Jana, S., Hsieh, C., and Kolter, J. Z. Beta-crown: Efﬁcient bound propagation with per-neuron split constraints for neural network robustness veriﬁcation. In Ranzato, M., Beygelzimer, A., Dauphin, Y. N., Liang, P., and Vaughan, J. W. (eds.), Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems 2021, NeurIPS 2021, December 6-14, 2021, virtual, pp. 29909–29921, 2021. URL https://proceedings.neurips.cc/paper /2021/hash/fac7fead96dafceaf80c1daffe ae82a4-Abstract.html.
Wang, W., Levine, A., and Feizi, S. Lethal dose conjecture on data poisoning. CoRR, abs/2208.03309, 2022a. doi:

Wang, W., Levine, A., and Feizi, S. Improved certiﬁed defenses against data poisoning with (deterministic) ﬁnite aggregation. CoRR, abs/2202.02628, 2022b. URL http s://arxiv.org/abs/2202.02628.
Weber, M., Xu, X., Karlas, B., Zhang, C., and Li, B. RAB: provable robustness against backdoor attacks. CoRR, abs/2003.08904, 2020. URL https://arxiv.org/ abs/2003.08904.
Xu, K., Shi, Z., Zhang, H., Wang, Y., Chang, K., Huang, M., Kailkhura, B., Lin, X., and Hsieh, C. Automatic perturbation analysis for scalable certiﬁed robustness and beyond. In Larochelle, H., Ranzato, M., Hadsell, R., Balcan, M., and Lin, H. (eds.), Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, 2020. URL https://proceedings.neurips.cc/paper /2020/hash/0cbc5671ae26f67871cb914d 81ef8fc1-Abstract.html.
Zhang, H., Chen, H., Xiao, C., Gowal, S., Stanforth, R., Li, B., Boning, D. S., and Hsieh, C. Towards stable and efﬁcient training of veriﬁably robust neural networks. In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net, 2020. URL https: //openreview.net/forum?id=Skxuk1rFwB.
Zhang, H., Wang, S., Xu, K., Li, L., Li, B., Jana, S., Hsieh, C., and Kolter, J. Z. General cutting planes for bound-propagation-based neural network veriﬁcation. CoRR, abs/2208.05740, 2022a. doi: 10.48550/arXiv. 2208.05740. URL https://doi.org/10.48550/ arXiv.2208.05740.
Zhang, Y., Albarghouthi, A., and D’Antoni, L. Certiﬁed robustness to programmable transformations in lstms. In Moens, M., Huang, X., Specia, L., and Yih, S. W. (eds.), Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, EMNLP 2021, Virtual Event / Punta Cana, Dominican Republic, 7-11 November, 2021, pp. 1068–1083. Association for Computational Linguistics, 2021. doi: 10.18653/v1/2021.emnlp-main.82. URL https://do i.org/10.18653/v1/2021.emnlp-main.82.
Zhang, Y., Albarghouthi, A., and D’Antoni, L. Bagﬂip: A certiﬁed defense against data poisoning. CoRR, abs/2205.13634, 2022b. doi: 10.48550/arXiv.2205. 13634. URL https://doi.org/10.48550/arXi v.2205.13634.

PECAN: A Deterministic Certiﬁed Defense Against Backdoor Attacks
Zhu, C., Huang, W. R., Li, H., Taylor, G., Studer, C., and Goldstein, T. Transferable clean-label poisoning attacks on deep neural nets. In Chaudhuri, K. and Salakhutdinov, R. (eds.), Proceedings of the 36th International Conference on Machine Learning, ICML 2019, 9-15 June 2019, Long Beach, California, USA, volume 97 of Proceedings of Machine Learning Research, pp. 7614–7623. PMLR, 2019. URL http://proceedings.mlr.press/ v97/zhu19a.html.

PECAN: A Deterministic Certiﬁed Defense Against Backdoor Attacks

A. Proof of Theorem 4.2
Proof. Theorem 4.1 tells that either r = or the following equation holds,
∀D ∈ Sr(D), x ∈ π(x). A¯D(x) = A¯D (x ) (6)

By the symmetrical deﬁnition of Sr and π, we have

∀D. D ∈ Sr(D) =⇒ D ∈ Sr(D)

(7)

∀x. x ∈ π(x) =⇒ x ∈ π(x).

(8)

Then, for all possible clean data D and x, we have

D ∈ Sr(D) ∧ x ∈ π(x)
=⇒ D ∈ Sr(D) ∧ x ∈ π(x) =⇒ A¯D(x) = A¯D(x)

(By Eq 7 and Eq 8) (By Eq 6)

B. Experiment

B.1. Detailed Setup of Section 5.2

Following the BagFlip paper (Zhang et al., 2022b), we set k,

the number of training examples in a bag used in BagFlip,

as 100, 1000, and 3000 for the MNIST, CIFAR10, and

EMBER dataset, respectively. For PECAN, we vary n, the

number of partitions, according to the value of k in BagFlip

by

setting

n

=

|D| k

.

BagFlip deﬁnes their perturbation space Sr(D) that is different from PECAN,

Sr(D) = D | max(|D \ D|, |D \ D|) ≤ r ,

where A \ B is the set difference, i.e., the elements in A but not in B. Notice that with the same radius r, the above deﬁnition gives a larger Sr(D) than Sr(D) as the following example shows.
Example B.1. If the attacker modiﬁes one training example x ∈ D to another training example x to form a poisoned dataset D = D \ {x} ∪ {x}. Then D ∈ S2(D) but D ∈/ S1(D) because Sr(D) considers one modiﬁcation as one deletion and one insertion. However, we have D ∈ S1(D).

Chen et al. (2022) show that Sr(D) works when the approach uses non-deterministic sub-sampling (Jia et al., 2021; Zhang et al., 2022b). However, the certiﬁcation of deterministic approaches only works under the deﬁnition of Sr(D).

We adjust the computation of certiﬁed accuracy for BagFlip

as

1 m

m i=1

1A¯D

(xi

)=yi

∧

ri |D|

≥R%

by

removing

the

factor

2

on R. Thus, we are also interested in the performance of

PECAN

when

n

=

|D| 2k

to

compensate

the

removed

factor

2.

Certiﬁable Accuracy

100

80

60

40

20

0

0

0.2

0.4

Modiﬁcation Amount R (%)

(a) MNIST s = 0.1

0

2

4

·10−2

(b) CIFAR10 s = 2/255

PECAN-a PECAN-b

Figure 8. Results of PECAN on CIFAR10 and EMBER, showing the normal accuracy (dotted lines) and the certiﬁed accuracy (solid lines) at different modiﬁcation amounts R. For MNIST: a = 600 and b = 1200. For CIFAR10: a = 50 and b = 100.

B.2. Evaluation on the l∞ Perturbation Space
Setup As the CROWN-IBP used in PECAN can handle π with different lp norms, PECAN can handle different lp norms as well. We evaluate PECAN on the l∞ norm with distance s = 0.1 and s = 2/255 on MNIST and CIFAR10, respectively, because the l∞ norm is widely applied to evaluate the robustness of image classiﬁers. In this experimental setting, we use two CNN models for MNIST and CIFAR10 because CROWN-IBP works better for CNN on l∞ norm than on l0 norm. For training on MNIST and CIFAR10, we train on s = 0.2 and s = 5/255 but test on s = 0.1 and s = 2/255 to overcome the overﬁtting issue when s is small, following the practice in the original paper of CROWN-IBP.
For the experiments on l0 (Sections 5.2 and 5.3), we set the κstart = 0 and κend = 0 for CROWN-IBP. For the experiments on l∞, we set the κstart = 1 and κend = 0 for CROWN-IBP.
Results Figure 8 shows the results of PECAN against l∞ perturbation space. The results show that PECAN achieves certiﬁed accuracy similar to F1 as shown in Figures 3 and 4.
B.3. Comparison to DPA, CROWN-IBP, and NoDef on the Non-Malware Test Set without Trigger
Figure 9 shows that NoDef has the lowest ASR of 2.70% on the non-malware set without trigger than all three defenses because the backdoor attack does not aim to attack the prediction of non-malware. However, PECAN still achieves the lowest ASR of 5.73% compared to DPA (7.85%) and CROWN-IBP (6.68%).
B.4. Comparison to Spectral Signatures
We followed the experiment in Severi et al. (2021) to ﬁlter out poisoned examples in the training dataset D3. After removing the top 15% outliers in the non-malware training set, we observe that only 14% (84 out of 600) of the poison is removed. Then we train a new model using the ﬁltered training set. We ﬁnd the ASR of the new model on the

PECAN: A Deterministic Certiﬁed Defense Against Backdoor Attacks

Table 1. Results of PECAN, DPA, CROWN-IBP (C-IBP) and vanilla model without defense (NoDef) trained on three backdoored EMBER datasets. Malware with triggers is the backdoored test data that should be labeled as malware. Malware w/o triggers is the original test data that should be labeled as malware. Non-malware w/o triggers is the original test data that should be labeled as non-malware.

Test sets

Malware with triggers

Malware w/o triggers

Non-Malware w/o triggers

Approaches PECAN DPA C-IBP NoDef PECAN DPA C-IBP NoDef PECAN DPA C-IBP NoDef

ASR. (↓)

2.38% 4.68% 2.72% 21.15% 1.27% 2.00% 2.10% 1.92% 6.48% 7.81% 9.41% 2.94%

D1 Correct Pred. (↑) 38.42% 33.57% 67.86% 78.85% 58.01% 64.34% 77.21% 98.08% 73.82% 79.29% 83.66% 97.06%

Abstention Rate 59.20% 61.75% 29.42% N/A 40.72% 33.66% 20.69% N/A 19.70% 12.90% 6.93% N/A

ASR. (↓)

1.98% 24.61% 28.57% 41.17% 1.12% 1.96% 8.05% 2.16% 5.55% 7.83% 6.11% 2.64%

D2 Correct Pred. (↑) 29.33% 20.12% 34.78% 58.83% 44.62% 64.34% 65.01% 97.84% 65.95% 79.03% 90.68% 97.36%

Abstention Rate 68.69% 55.27% 36.64% N/A 54.27% 33.70% 26.95% N/A 28.50% 13.14% 3.21% N/A

ASR. (↓)

1.19% 24.87% 14.42% 61.67% 0.71% 1.97% 10.32% 2.28% 5.16% 7.91% 4.51% 2.51%

D3 Correct Pred. (↑) 21.48% 19.59% 27.59% 38.33% 34.45% 64.58% 41.10% 97.72% 54.40% 78.96% 87.91% 97.49% Abstention Rate 77.33% 55.54% 57.99% N/A 64.84% 33.45% 48.58% N/A 40.44% 13.14% 7.59% N/A

100 ASR

80

Correct

Abstain

60

40

20

0 D1D2D3
PECAN

D1D2D3
DPA

D1D2D3
C-IBP

D1D2D3
NoDef

Figure 9. Results of PECAN, DPA, CROWN-IBP (C-IBP), and vanilla model without defense (NoDef) trained on three poisoned EMBER datasets when evaluated on the (original) non-malware test set without triggers.

Test Set Percentage

100

80

60

40

20

0

0

5 · 10−2

0.1 0

Modiﬁcation Amount R (%)

(a) PECAN

Correct ASR
Abstain

5 · 10−2

0.1

(b) DPA

Figure 10. Comparison between PECAN and DPA trained on D1 across all modiﬁcation amount R when evaluated on the malware test set with triggers.

malware set with triggers, the malware set without triggers, and the non-malware set without triggers are 48.55%, 1.38%, and 8.91%, respectively. These ASRs are all higher than PECAN’s 1.19%, 0.71%, and 5.16% on the three parts of the test set.
B.5. Comparison to DPA on D1 and D2
Figures 10 and 11 show the comparison between PECAN and DPA on D1 and D2. We can observe that PECAN has much higher ASRs than DPA across all modiﬁcation amounts on D1 and D2.

Test Set Percentage

100

80

60

40

20

0

0

5 · 10−2

0.1 0

Modiﬁcation Amount R (%)

(a) PECAN

Correct ASR
Abstain

5 · 10−2

0.1

(b) DPA

Figure 11. Comparison between PECAN and DPA trained on D2 across all modiﬁcation amount R when evaluated on the malware test set with triggers.

