
Skip to main content
Cornell University
We are hiring

We gratefully acknowledge support from the Simons Foundation, member institutions , and all contributors. Donate
arxiv logo > cs > arXiv:2007.12070

Help | Advanced Search
Search
Computer Science > Cryptography and Security
(cs)
[Submitted on 11 Jul 2020 ( v1 ), last revised 15 Mar 2021 (this version, v3)]
Title: Mitigating backdoor attacks in LSTM-based Text Classification Systems by Backdoor Keyword Identification
Authors: Chuanshuai Chen , Jiazhu Dai
Download a PDF of the paper titled Mitigating backdoor attacks in LSTM-based Text Classification Systems by Backdoor Keyword Identification, by Chuanshuai Chen and 1 other authors
Download PDF

    Abstract: It has been proved that deep neural networks are facing a new threat called backdoor attacks, where the adversary can inject backdoors into the neural network model through poisoning the training dataset. When the input containing some special pattern called the backdoor trigger, the model with backdoor will carry out malicious task such as misclassification specified by adversaries. In text classification systems, backdoors inserted in the models can cause spam or malicious speech to escape detection. Previous work mainly focused on the defense of backdoor attacks in computer vision, little attention has been paid to defense method for RNN backdoor attacks regarding text classification. In this paper, through analyzing the changes in inner LSTM neurons, we proposed a defense method called Backdoor Keyword Identification (BKI) to mitigate backdoor attacks which the adversary performs against LSTM-based text classification by data poisoning. This method can identify and exclude poisoning samples crafted to insert backdoor into the model from training data without a verified and trusted dataset. We evaluate our method on four different text classification datset: IMDB, DBpedia ontology, 20 newsgroups and Reuters-21578 dataset. It all achieves good performance regardless of the trigger sentences. 

Subjects: 	Cryptography and Security (cs.CR) ; Machine Learning (cs.LG); Machine Learning (stat.ML)
Cite as: 	arXiv:2007.12070 [cs.CR]
  	(or arXiv:2007.12070v3 [cs.CR] for this version)
  	https://doi.org/10.48550/arXiv.2007.12070
Focus to learn more
arXiv-issued DOI via DataCite
Submission history
From: Chuanshuai Chen [ view email ]
[v1] Sat, 11 Jul 2020 09:05:16 UTC (309 KB)
[v2] Sat, 1 Aug 2020 16:05:45 UTC (34 KB)
[v3] Mon, 15 Mar 2021 03:45:46 UTC (96 KB)
Full-text links:
Download:

    Download a PDF of the paper titled Mitigating backdoor attacks in LSTM-based Text Classification Systems by Backdoor Keyword Identification, by Chuanshuai Chen and 1 other authors
    PDF
    Other formats 

( license )
Current browse context:
cs.CR
< prev   |   next >
new | recent | 2007
Change to browse by:
cs
cs.LG
stat
stat.ML
References & Citations

    NASA ADS
    Google Scholar
    Semantic Scholar

DBLP - CS Bibliography
listing | bibtex
Chuanshuai Chen
Jiazhu Dai
a export BibTeX citation Loading...
Bookmark
BibSonomy logo Reddit logo
Bibliographic Tools
Bibliographic and Citation Tools
Bibliographic Explorer Toggle
Bibliographic Explorer ( What is the Explorer? )
Litmaps Toggle
Litmaps ( What is Litmaps? )
scite.ai Toggle
scite Smart Citations ( What are Smart Citations? )
Code, Data, Media
Demos
Related Papers
About arXivLabs
Which authors of this paper are endorsers? | Disable MathJax ( What is MathJax? )

    About
    Help

    contact arXiv Click here to contact arXiv Contact
    subscribe to arXiv mailings Click here to subscribe Subscribe

    Copyright
    Privacy Policy

    Web Accessibility Assistance

    arXiv Operational Status
    Get status notifications via email or slack

