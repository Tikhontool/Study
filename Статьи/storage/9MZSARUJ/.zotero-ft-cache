
Skip to main content
Cornell University
We are hiring

We gratefully acknowledge support from the Simons Foundation, member institutions , and all contributors. Donate
arxiv logo > cs > arXiv:2302.06279

Help | Advanced Search
Search
Computer Science > Cryptography and Security
(cs)
[Submitted on 13 Feb 2023 ( v1 ), last revised 3 Jul 2023 (this version, v2)]
Title: Sneaky Spikes: Uncovering Stealthy Backdoor Attacks in Spiking Neural Networks with Neuromorphic Data
Authors: Gorka Abad , Oguzhan Ersoy , Stjepan Picek , Aitor Urbieta
Download a PDF of the paper titled Sneaky Spikes: Uncovering Stealthy Backdoor Attacks in Spiking Neural Networks with Neuromorphic Data, by Gorka Abad and 3 other authors
Download PDF

    Abstract: Deep neural networks (DNNs) have demonstrated remarkable performance across various tasks, including image and speech recognition. However, maximizing the effectiveness of DNNs requires meticulous optimization of numerous hyperparameters and network parameters through training. Moreover, high-performance DNNs entail many parameters, which consume significant energy during training. In order to overcome these challenges, researchers have turned to spiking neural networks (SNNs), which offer enhanced energy efficiency and biologically plausible data processing capabilities, rendering them highly suitable for sensory data tasks, particularly in neuromorphic data. Despite their advantages, SNNs, like DNNs, are susceptible to various threats, including adversarial examples and backdoor attacks. Yet, the field of SNNs still needs to be explored in terms of understanding and countering these attacks.
    This paper delves into backdoor attacks in SNNs using neuromorphic datasets and diverse triggers. Specifically, we explore backdoor triggers within neuromorphic data that can manipulate their position and color, providing a broader scope of possibilities than conventional triggers in domains like images. We present various attack strategies, achieving an attack success rate of up to 100\% while maintaining a negligible impact on clean accuracy. Furthermore, we assess these attacks' stealthiness, revealing that our most potent attacks possess significant stealth capabilities. Lastly, we adapt several state-of-the-art defenses from the image domain, evaluating their efficacy on neuromorphic data and uncovering instances where they fall short, leading to compromised performance. 

Subjects: 	Cryptography and Security (cs.CR) ; Computer Vision and Pattern Recognition (cs.CV); Machine Learning (cs.LG)
Cite as: 	arXiv:2302.06279 [cs.CR]
  	(or arXiv:2302.06279v2 [cs.CR] for this version)
  	https://doi.org/10.48550/arXiv.2302.06279
Focus to learn more
arXiv-issued DOI via DataCite
Submission history
From: Gorka Abad [ view email ]
[v1] Mon, 13 Feb 2023 11:34:17 UTC (3,316 KB)
[v2] Mon, 3 Jul 2023 07:03:22 UTC (3,578 KB)
Full-text links:
Download:

    Download a PDF of the paper titled Sneaky Spikes: Uncovering Stealthy Backdoor Attacks in Spiking Neural Networks with Neuromorphic Data, by Gorka Abad and 3 other authors
    PDF
    Other formats 

Current browse context:
cs.CR
< prev   |   next >
new | recent | 2302
Change to browse by:
cs
cs.CV
cs.LG
References & Citations

    NASA ADS
    Google Scholar
    Semantic Scholar

a export BibTeX citation Loading...
Bookmark
BibSonomy logo Reddit logo
Bibliographic Tools
Bibliographic and Citation Tools
Bibliographic Explorer Toggle
Bibliographic Explorer ( What is the Explorer? )
Litmaps Toggle
Litmaps ( What is Litmaps? )
scite.ai Toggle
scite Smart Citations ( What are Smart Citations? )
Code, Data, Media
Demos
Related Papers
About arXivLabs
Which authors of this paper are endorsers? | Disable MathJax ( What is MathJax? )

    About
    Help

    contact arXiv Click here to contact arXiv Contact
    subscribe to arXiv mailings Click here to subscribe Subscribe

    Copyright
    Privacy Policy

    Web Accessibility Assistance

    arXiv Operational Status
    Get status notifications via email or slack

