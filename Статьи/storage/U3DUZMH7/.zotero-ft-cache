FMT: Removing Backdoor Feature Maps via
Feature Map Testing in Deep Neural Networks
Dong Huang1†, Qingwen Bu23†, Yuhao Qing1, Yichao Fu1, Heming Cui13 1The University of Hong Kong 2Shanghai Jiao Tong University
3Shanghai Artificial Intelligence Laboratory {dhuang,yhqing,heming}@cs.hku.hk,qwbu01@sjtu.edu.cn,yichao@connect.hku.hk

arXiv:2307.11565v1 [cs.LG] 21 Jul 2023

Abstract—Deep neural networks have been widely used in many critical applications, such as autonomous vehicles and medical diagnosis. However, their security is threatened by backdoor attack, which is achieved by adding artificial patterns to specific training data. Existing defense strategies primarily focus on using reverse engineering to reproduce the backdoor trigger generated by attackers and subsequently repair the DNN model by adding the trigger into inputs and fine-tuning the model with groundtruth labels. However, once the trigger generated by the attackers is complex and invisible, the defender can not successfully reproduce the trigger. Consequently, the DNN model will not be repaired since the trigger is not effectively removed.
In this work, we propose Feature Map Testing (FMT). Different from existing defense strategies, which focus on reproducing backdoor triggers, FMT tries to detect the backdoor feature maps, which are trained to extract backdoor information from the inputs. After detecting these backdoor feature maps, FMT will erase them and then fine-tune the model with a secure subset of training data. Our experiments demonstrate that, compared to existing defense strategies, FMT can effectively reduce the Attack Success Rate (ASR) even against the most complex and invisible attack triggers (e.g., FMT decreases the ASR to 2.86% in CIFAR10, 19.2%-65.41% lower than previous arts). Second, unlike conventional defense methods that tend to exhibit low Robust Accuracy (i.e., the model’s accuracy on the poisoned data), FMT achieves higher RA, indicating its superiority in maintaining model performance while mitigating the effects of backdoor attacks (e.g., FMT obtains 87.40% RA in CIFAR10). Third, compared to existing feature map pruning techniques, FMT can cover more backdoor feature maps (e.g., FMT removes 83.33% of backdoor feature maps from the model in the CIFAR10 & BadNet scenario).
I. INTRODUCTION
Deep neural networks (DNNs) have become a cornerstone technology in numerous fields, such as computer vision [1], natural language processing [2], speech recognition [3], and several other applications [4, 5]. The effective training of DNNs generally demands extensive datasets and considerable GPU resources to achieve SOTA performance. However, a substantial number of DNN developers may lack access to these resources, which subsequently leads them to rely on thirdparty services for training their models (e.g., Google Cloud [6], AWS [7], and Huawei Cloud [8]), acquiring datasets (e.g., DataTurks [9]), or directly downloading pre-trained mod-
†These authors contributed equally.

Clean Img

Backdoor Img

Trigger

Training Inference

Backdoor Injected
Speed Limit 120km/h

Fig. 1: Backdoor attack example. The DNN developer uploads clean images to a third party to train the model, while the third party modifies the clean images by injecting a backdoor trigger in it and change its corresponding label (in this case, ”Speed Limit”). After the training, the backdoor will be injected into the DNN model, which then outputs the category specified by the attacker as long as the input contains the trigger.

els (e.g., Hugging Face [10]).
While utilizing third-party services offers a practical solution for DNN developers, it simultaneously introduces potential security risks. Specifically, third-party may be involved in the data collection, model training, and pre-trained model distribution process, which may introduce malicious backdoor triggers [11, 12]. For instance, once a developer uploads their dataset to a third-party service for training, the service provider could potentially revise the dataset by injecting poisoned samples containing hidden backdoor triggers. These poisoned samples, designed to blend in with the rest of the dataset, are then used in the training process, embedding the backdoor triggers into the model’s architecture. The presence of backdoor triggers in DNNs can lead to severe ramifications, particularly in security-critical systems where model integrity is crucial for preserving human life and safety [12, 13]. Taking Fig. 1 as an example, in autonomous vehicle systems, the injected model could be exploited to misclassify traffic signs, potentially resulting in incorrect driving decisions that endanger the lives of passengers and other road users.
To mitigate the backdoor threat, researchers have proposed a variety of defense mechanisms. Existing defense methods against backdoor attacks can be grouped into two main categories: detection methods and repairing methods [14, 15]. Detection methods rely on model internal information (e.g., neuron activation values [16]) or model properties (e.g., per-

formance metrics [17, 18]) to ascertain whether a model has been compromised by a backdoor attack, or whether a specific input example being processed by the model is a backdoor instance [19]. These detection techniques play a crucial role in identifying potential risks and raising awareness about possible security vulnerabilities. However, once a backdoor model has been detected, it still needs to be removed to restore its trustworthiness and reliability.
To tackle this challenge, researchers have introduced repairing methods that go beyond detection and aim to remove backdoor triggers from the compromised models. One notable example is Neural Cleanse (NC) [13], a technique that leverages reverse engineering to first reproduce the backdoor trigger. Once the trigger has been identified, NC injects it into the dataset along with its corresponding ground-truth labels, enabling the fine-tuning of the model to eliminate the backdoor trigger’s impact. However, recent evaluation results[11, 20, 21] reveal that NC and similar reverse engineering-based methods are predominantly successful in tackling simple backdoor triggers. Conversely, complex triggers (e.g., those involving intricate patterns or transformations) pose a greater challenge, making it difficult to reproduce and remove them. Consequently, models containing such sophisticated triggers may not be adequately repaired, leaving them susceptible to potential security breaches.
Recently, some researchers [22, 23] try to address this problem by analyzing the feature map’s behavior to remove the backdoor from the model since they notice that backdoorrelated feature maps exist, which demonstrate different characteristics from other normal feature maps. Specifically, feature maps are essential components of DNNs, responsible for extracting various features (e.g., color and texture information) from input samples [24]. The model then relies on these feature maps to extract features from the inputs and to make its final predictions. In the backdoor model, some backdoor feature maps extract backdoor information from the inputs [23]. Based on this observation, they believe that once the backdoor feature maps in the model are erased, the backdoor trigger will also be removed from the model. However, detecting backdoor feature maps from the model is challenging since the DNN developers, who may download pre-trained models from a third party, are unaware of the information about possible backdoor triggers injected in the model. Subsequently, they can not directly add triggers into the input samples to analyze and detect backdoor feature maps.
To address this challenge, we propose the Feature Map Testing (FMT), a novel approach that focuses on identifying and eliminating backdoor feature maps within the DNN. Instead of directly reproducing the backdoor trigger and then using it to detect backdoor feature maps in the model, FMT uses reverse engineering to reproduce the features extracted by each feature map. By adding these features into the inputs, we can feed these inputs into the model to identify the backdoor feature maps, which can then be mitigated to secure the DNN model. Our experiments reveal that initializing these feature maps and fine-tuning the model can effectively remove the backdoor

from the model. To validate the effectiveness of the FMT, we conducted
extensive experimental evaluations on multiple benchmark datasets, including CIFAR-10, CIFAR-100 [25], and GTSRB [26], under diverse attack scenarios. In our experiments, we considered various backdoor triggers with different complexities and compared the performance of FMT against several state-of-the-art baseline methods. We assessed the models using three primary metrics: Accuracy, Attack Success Rate (ASR), and Robust Accuracy (RA).
Our experimental results demonstrate that FMT consistently achieves lower ASR, higher accuracy, and improved RA compared to baseline methods across different datasets and attack scenarios. For instance, on the CIFAR-10 dataset, FMT outperformed baselines by achieving a significantly lower ASR (e.g., FMT decrease 19.2% ASR average compared with baselines in CIFAR10), and better RA (e.g., FMT increase 16.92% RA on average in CIFAR10), indicating its effectiveness in removing backdoor triggers without compromising the model’s performance on benign samples.
In a nutshell, we make the following contributions: • We propose a novel defense strategy FMT to mitigate
backdoor triggers from the model. • We conduct extensive experiments to investigate the per-
formance of FMT. The experiment results show that FMT can significantly outperform baseline strategies. • We implement FMT into a tool that could help DNN developers to remove backdoor triggers from the DNN models, which is available in our Github Page [27].
II. BACKGROUND
In this section, we will define backdoor and then discuss three classical real-world scenarios in which backdoor attacks may occur.
A. Defining Backdoors.
We define a DNN backdoor to be a hidden pattern trained into a DNN, which produces unexpected behavior if and only if a specific trigger is added to an input. Such a backdoor does not affect the model’s normal behavior on clean inputs without the trigger. In the context of classification tasks, a backdoor misclassifies arbitrary inputs into the same specific target label, when the associated trigger is applied to inputs. Input samples that should be classified into any other label could be “overridden” by the presence of the trigger. In the vision domain, a trigger is often a specific pattern on the image (e.g., a sticker), that could misclassify images of other labels (e.g., wolf, bird, dolphin) into the target label (e.g., dog).
B. Backdoor Attack Scenarios and Capacities
To analyze the challenges and complexities associated with backdoor attacks in deep learning, we outline three classical real-world scenarios where such threats can occur, along with the corresponding capacities available to attackers and defenders. By examining these scenarios, we aim to provide a thorough understanding of the risks associated with backdoor attacks and the necessity for effective defense mechanisms.

1) Scenario 1: Adopting Third-Party Datasets: In this scenario, developers rely on third-party-provided datasets (e.g., DataTurks [9]), either directly or via the Internet, to train their models. This dependence on external data sources increases the risk of backdoor attacks since the attacker can manipulate the dataset to inject subtle triggers. However, they cannot modify the model, training schedule, or inference pipeline, which limits their ability to control the system. Conversely, defenders have full control over the system and can take various measures to mitigate the backdoor threat. For instance, they can clean up the poisoned dataset, implement data augmentation strategies, or use anomaly detection techniques to identify and remove potential triggers.
2) Scenario 2: Adopting Third-Party Platforms: In this scenario, developers use untrusted third-party platforms (e.g., Google Cloud [6], AWS Cloud [7], or HUAWEI Cloud [8]) to train their models, providing the platform with a benign dataset, model structure, and training schedule. The attacker (i.e., the malicious platform) can modify the dataset and training schedule during the actual training process, but they cannot change the model structure, as the developers would notice. This limitation requires the attacker to be more stealthy and sophisticated in their approach. On the other hand, defenders cannot control the training set and schedule, but they can modify the trained model to alleviate the attack, such as by fine-tuning it on a small local benign dataset or using techniques like knowledge distillation to transfer knowledge from the compromised model to a new, clean model.
3) Scenario 3: Adopting Third-Party Models: In this scenario, attackers provide infected DNNs through APIs or the Internet (e.g., Hugging Face [10]). They can modify everything except for the inference pipeline, which means developers can introduce a pre-processing module on the test image before prediction, out of the attackers’ control. This constraint forces attackers to craft more advanced and persistent backdoors that can survive such pre-processing. Defenders can control the inference pipeline and the model if its source files are provided. However, if they can only access the model API, they cannot modify the model directly. In this case, defenders must resort to techniques like input validation, or reverse engineering to detect and mitigate the backdoor threat.

III. METHODOLOGY

A. Preliminaries

1) Notations: For ease of discussion, this section defines

the following notations for DNNs and feature maps: fθ is

a DNN parameterized by θ, and there are N feature maps

N i

Fθi

in

the

model.

The

i-th

feature

map

in

the

DNN

is

denoted as Fθi. Fθi(x) denotes the feature map i-th output

when the DNN input is x. The x′ means the input x added the

feature which is generated by the corresponding feature map.

2) Backdoor Feature Maps: It was found that there exist

few feature maps that contribute the most to the backdoor

behaviors in the infected model [22, 23]. If some or all of

these feature maps are pruned, the attack success rate will

be reduced greatly [22, 23]. In this work, to better quantify

the importance of feature maps to backdoor behaviors, we
would like to introduce the sensitivity of feature maps to the
backdoor. We first introduce the definition of feature maps: Definition 1. Given a model’s feature map fθi and the clean samples X and the poison samples X ′, the sensitivity of feature map fθi is defined as:

X ,X ′

S(fθi) =

(fθi(x) − fθi(x′))

x,x′

As mentioned by CLP [23], a feature map with higher sensitivity will have a higher probability become a backdoor trigger.
Using the quantity defined in definition 1, we are now able to find the feature maps that are most correlated with the backdoor triggers: Definition 2. Given a model fθ and the ratio of τ , the set of backdoor feature maps are defined as:

Idx = ArgSort(S(fθi))[:: τ N ]

B(fθ, τ ) = {fθi}Idx

we define the top τ % feature maps are backdoor feature maps. For ease of discussion, the default τ in our experiment is 10%.

B. Motivation
The primary goal of our defense strategy is to detect backdoor feature maps in a DNN model. As previously mentioned, intuitively, we should use backdoor samples and clean samples fed into the model to detect these backdoor feature maps. However, since DNN developers do not have access to backdoor samples, they cannot directly detect backdoor feature maps using this approach. This limitation calls for a defense mechanism that can operate effectively without the need for backdoor samples.
The main idea behind our proposed defense mechanism is to generate potential poison samples by reversing the specific features associated with each feature map. Since each feature map in the model is used to extract features from the DNN model, and in the injected model, there are some backdoor feature maps that are used to extract backdoor information for the poison samples. By feeding these potential poison samples into the model, we can observe the inference accuracy for each feature map. Since backdoor feature maps are designed to extract backdoor information from the inputs, causing the classifier to return the target label regardless of the true label, we hypothesize that potential poison samples generated by backdoor feature maps will lead to a significant change in inference accuracy when processing their corresponding potential poison samples.
To confirm which feature maps generate potential poison samples that are most likely to be real poison samples, we employ outlier detection techniques on the inference accuracies obtained from potential poison samples generated by different feature maps. This allows us to identify the feature maps that result in anomalously low inference accuracy when processing their corresponding potential poison samples, which we con-

guided by the loss function, which measures the discrepancy between the outputs of the feature map fθi(x) and fθi(x′).
During the reverse process, we update the perturbed input x′ using the gradient of the loss function with respect to x′. We adopt a step size of α in the direction of the gradient sign. Furthermore, we constrain the perturbation δ within the limits of the maximum allowed perturbation ϵ. The reverse process
is iterated for a predetermined number of steps, resulting in
the generation of the reversed features.

Fig. 2: The pipeline of FMT. FRG represents Feature Reverse Generation.
sider backdoor feature maps. In summary, the motivation behind our defense strategy is
to detect backdoor feature maps in the absence of backdoor samples by generating potential poison samples and analyzing the inference accuracies with these samples. This enables us to identify backdoor feature maps, which can then be mitigated to secure the DNN model. With this motivation in mind, we propose Feature Map Testing (FMT) as the defense mechanism to achieve this objective.
C. Overview
The overview of FMT’s workflow is shown in Fig. 2, which mainly includes three stages. In the first stage, we will reproduce the features which will be extracted by the corresponding feature maps and then add these generated features to the inputs (Sec. III-D). In the second stage, we will feed these revised inputs into the model to evaluate the model performance (accuracy) when these features exist, and detect anomaly feature maps (i.e., backdoor feature maps) from the model feature maps (Sec. III-E) based on these accuracies. In the third stage, we will erase these anomaly feature maps from the model and fine-tune model (Sec. III-F).
D. Feature Reverse Generation
In DNNs, each feature map is used to extract different features from the inputs. The classifier then returns a prediction based on these extracted features. When a backdoor is injected into the DNN model, some feature maps will be utilized to extract backdoor information from the inputs. Once the backdoor information is extracted, the classifier will directly return the target label, regardless of the true label.
In this section, we employ reverse engineering to generate features that will be extracted by each feature map in the DNN. These features will be added to the inputs, which will then be used to detect anomaly feature maps. The detailed implementation of the Feature Reverse Generation is provided in Algorithm 1. Specifically, for each feature map fθi in the DNN model, our objective is to generate the corresponding features that the feature map is intended to extract. To accomplish this, we use a reverse engineering approach that maximizes the difference between the original input x and the perturbed input x′ in the feature map space. This process is

Algorithm 1: Feature Reverse Generation

Input: fθi: the i-th feature map of the model, which will be used to generate the reversed features

from the inputs; x: the original input sample;

α: the step size for the reverse process; ϵ: the

maximum allowed perturbation; steps: the

number of steps in the reverse process; output: x′ potential poison sample. 1 Function FRG(fθi, x, steps, α, ϵ): 2 reverse samples = [];
′
3 Initialize: x ← x + random noise;

4 for step=0 to steps do

5

loss ← ∥fθi(x) − fθi(x′)∥2;

6

grad ← ∂loss / ∂x′;

7

x′ ← x′ + α ∗ grad.sign();

8

δ ← clamp(x′ − x, min = −ϵ, max = ϵ);

9

x′ ← clamp(x + δ, min = 0, max = 1);

10

reverse samples.append(x′)

11 return reverse samples

E. Detect Anomaly Feature Map via Outlier Detection.
After obtaining the reversed samples using the method described in Algorithm 1, we feed these potential poison samples into the model to obtain feature map inference accuracy. For each feature map, we obtain a distinct model accuracy list with different reverse steps when evaluated on its respective potential poison samples.
To detect outliers, we use the Elliptic Envelope method, which is an effective technique for identifying anomalies in a multivariate dataset. The Elliptic Envelope method assumes that the data points are generated from a Gaussian distribution and fit an elliptical envelope around the central data points, with the outliers lying outside the envelope. The shape and size of the envelope are determined by estimating the parameters of the underlying Gaussian distribution. By applying the Elliptic Envelope method on the feature map inference accuracy obtained from the potential poison samples, we can identify the anomaly feature maps that deviate significantly from the others. These anomaly feature maps are considered to be associated with the backdoor behavior, as their respective potential poison samples lead to a substantial reduction in the model’s inference accuracy. Identifying these backdoor feature maps allows us to take further steps to mitigate their impact

Feature Reverse Step 2

0.45

inliner feature maps

0.44

outliner feature maps

0.43

0.42

0.41

0.40

0.39

0.41

0.42 Featur0e.43Reverse S0t.e44p 1

0.45

Fig. 3: Visulization of outlier detection results for feature map accuracy.

on the DNN model and ensure its security. Example 1. Here we use an example to illustrate how FMT detects backdoor feature maps. Specifically, as shown in Fig. 3, we can observe that the feature map accuracy for different feature maps is mainly located in 42% to 44%. However, we notice that a few feature maps are out of distribution. For example, two feature maps’ accuracy is lower than 41%, which indicates these feature maps may be backdoor feature maps.

F. Remove Backdoor from the DNN
After detecting the backdoor-related feature maps (anomaly feature maps), our next step is to remove the backdoor from the DNN. To achieve this, we follow a two-step process: initialization and fine-tuning.
a) Initialization: In this step, we initialize the weights corresponding to the anomaly feature maps identified in the previous stage. To perform the initialization, we first set the weights of the anomaly feature maps to zero. This process neutralizes the contribution of these feature maps to the overall model output. By resetting the weights associated with these feature maps, we effectively remove the backdoor information that the feature maps were designed to extract. In addition to resetting the weights, we also adjust the biases of the anomaly feature maps to ensure that the output of these feature maps remains close to zero during the forward pass. This initialization process helps to neutralize the impact of the malicious backdoor on the model’s predictions, thus reducing the risk of targeted misclassification.
b) Fine-Tuning: Once the anomaly feature maps have been initialized, we proceed to fine-tune the entire DNN model. Fine-tuning involves updating the model weights using a smaller learning rate and a subset of clean, accurately labeled samples. This step allows the model to adapt and learn from the new initialization while preserving its ability to perform well on clean samples. By fine-tuning the model, we ensure that it retains its predictive capabilities on legitimate inputs and remains robust against potential attacks.

IV. EVALUATION We evaluate FMT and answer the following questions: • RQ1 (Effectiveness) How effective of FMT compare to

existing defense strategies? • RQ2 (Efficiency) How efficient of FMT compare to ex-
isting defense strategies? • RQ3 (Coverage) How well does FMT coverage backdoor
feature maps within DNN models? • RQ4 (Sensitivity) Can FMT behave stably under differ-
ent settings?
A. Experimental Setting
In this work, we use BackdoorBench [14] as the benchmark to evaluate FMT’s performance. To the best of our knowledge, BackdoorBench is the most famous open-source and peerreviewed benchmark for backdoor attacks and defenses.
1) Datasets and Models: Since most of the existing backdoor-related literature focused on image classification tasks [11, 17, 2, 19, 23, 28, 29, 21], we following existing researches to choice CIFAR10, CIFAR100 [25], and GTSRB [26] datasets to evaluate FMT’s performance. These datasets are widely used by our baselines and have been systematically evaluated by BackdoorBench [14], so we believe evaluating FMT on these datasets can provide a comprehensive and meaningful assessment of its performance.
General Image classification (CIFAR10 and CIFAR100): CIFAR-10 and CIFAR-100 are widely-used datasets for evaluating DNN performance and vulnerability in image classification tasks. The CIFAR-10 dataset consists of 60,000 32x32 color images in 10 classes, with 6,000 images per class. The CIFAR-100 dataset is an extension of CIFAR-10, containing 100 classes with 600 images per class, and also has 60,000 images. German Traffic Sign Recognition Benchmark (GTSRB).: GTSRB is a widely-used dataset in computer vision for evaluating deep learning models’ performance in traffic sign recognition tasks. The dataset consists of over 50,000 images, representing 43 different traffic sign classes, collected from various real-world scenarios with diverse lighting conditions, viewpoints, and occlusions. Assessing backdoor defense techniques on the GTSRB dataset is crucial due to the safetycritical nature of traffic sign recognition in real-world applications, such as autonomous driving systems.
We chose PreActResNet18 [30] as our evaluation model since it has been systematically evaluated by BackdoorBench [14], widely used by baseline approaches, and it is also widely used by vision tasks (e.g., image classification [30], object detection [31]), so we believe evaluating FMT on these datasets can provide a comprehensive and meaningful assessment of its performance.
2) Attack setup: In our experiments, we selected five stateof-the-art (SOTA) backdoor attack strategies, which are systematically evaluated and implemented by BackdoorBench, as baselines to evaluate the effectiveness of our defense strategy. These attack strategies consist of BadNets [12], Blended [11], Low Frequency [32], SSBA [20], and WaNet [21]. BadNets [12] represents one of the earliest and most well-known backdoor attack strategies, which poisons the training dataset with images containing the trigger pattern and corresponding target labels. Blended attacks [11] introduce a more incon-

spicuous trigger by blending it with the input image, rendering it less noticeable. Low-Frequency attacks [32] exploit the frequency domain of images, creating triggers that are visually difficult to detect. SSBA [20], generate triggers informed by semantic segmentation to craft more natural and stealthy backdoor triggers. Lastly, WaNet attacks [21] leverage a watermarking technique to embed the trigger into the input, making it less perceptible.
We train the CIFAR10 and CIFAR100 datasets with 100 epochs, SGD momentum of 0.9, learning rate of 0.01, and batch size of 128, using the CosineAnnealingLR scheduler. The GTSRB dataset is trained with 50 epochs. We set the poisoning rate to 10% by default. To ensure fair comparisons, we adhere to the default configuration in the original papers, including trigger patterns, trigger sizes, and target labels.
3) Defense setup: A lot of effective defense strategies [23, 22, 13, 33, 29] are implemented by BackdoorBench, we believe that taking these defense strategies as our baseline can demonstrate FMT’s effectiveness. However, some of these defense strategies require DNN developers to have access to the backdoor trigger (e.g., AC [16], Spectral [29], ABL [34], D-BR [17] and DDE [19]), or to modify the model training process (e.g., ABL [34], DBD [35]) which is not possible in our defense scenarios. Therefore, we exclude these strategies from our baseline comparison.
Finally, we select six widely used defense strategies that align with our defense goals: Fine-tuning (FT) retrains the backdoor model with a subset clean dataset to remove the backdoor trigger’s effects. Fine-pruning (FP) [22] prunes backdoor feature maps to remove backdoors from the model. Adversarial Neuron Pruning (ANP) [36] selectively prunes neurons associated with the backdoor trigger while maintaining performance. Channel Lipschitz Pruning (CLP) [23] is a data-free strategy that analyzes feature maps’ Lipschitz constants to identify potential backdoor-related feature maps. Neural Attention Distillation (NAD) [33] leverages attention transfer to erase backdoor triggers from deep neural networks, ensuring that the model focuses on the relevant neurons for classification. Neural Cleanse (NC) identifies and mitigates backdoor attacks in neural networks by analyzing the model’s internal structure and identifying patterns that are significantly different from the norm, which may indicate the presence of a backdoor.
In order to repair the model, we adopt a configuration consisting of 10 epochs, a batch size of 256, and a learning rate of 0.01. The CosineAnnealingLR scheduler is employed alongside the Stochastic Gradient Descent (SGD) optimizer with a momentum of 0.9 for the client optimizer. We set the default retraining dataset ratio to 10% to ensure a fair and consistent evaluation of the defense strategies. For the CLP, we configure the Lipschitz Constant threshold (u) to be 3, the pruning step to 0.05, and the maximum pruning rate to 0.9, which is consistent with BackdoorBench and its original paper default settings. In the case of ANP, we optimize all masks using Stochastic Gradient Descent (SGD) with a perturbation budget (i.e., ϵ) of 0.4. All other configurations are maintained

as specified in the respective original publications to guarantee a rigorous comparison of the defense strategies.
4) Evaluation Metrics: To assess the effectiveness of our proposed defense strategy and compare it with the SOTA defense baselines, we employ three key evaluation metrics: clean accuracy (Acc), attack success rate (ASR), and Robust Accuracy (RA). Acc is used to measure the performance of the defense strategy on clean inputs. A high clean accuracy indicates that the model’s performance on its original task has not been compromised due to the defense. ASR evaluates the effectiveness of the defense strategy in mitigating backdoor attacks by measuring the rate at which the backdoor triggers succeed in causing misclassification. A lower ASR signifies a more effective defense, as it indicates that the backdoor triggers are less successful in fooling the model. RA quantifies the ability of a defense strategy to maintain its prediction accuracy on poisoned samples with respect to their original class labels. Unlike ASR, which measures the success rate of backdoor attacks, RA provides a complementary evaluation metric that focuses on the model’s performance in correctly classifying poisoned samples despite the presence of backdoor triggers.
B. Effectiveness
In this section, we evaluate the effectiveness of our proposed defense method and compare its performance with six existing state-of-the-art defense strategies as shown in Tab. I. The evaluation is conducted across three different datasets: CIFAR-10, CIFAR-100, and GTSRB. All experiments are repeated over five runs with different random seeds to ensure the consistency and reliability of the results. Due to space constraints, we report only the average performance without standard deviation.
For the CIFAR10 dataset, FMT consistently achieves low Attack Success Rate (ASR) values while maintaining high Robust Accuracy (RA). Specifically, FMT successfully cuts the average ASR down to 2.86% on average, which decreases the 19.2% ASR in CIFAR10 dataset, and the RA is also higher than baseline strategies. For instance, the proposed method’s RA reaches 87.40%, which is around 16.92% higher than the average RA achieved by baseline approaches. This demonstrates that our method can effectively mitigate backdoor attacks while increasing the model’s performance on poison data, highlighting its practical utility in real-world scenarios.
We can also observe that standard fine-tuning (FT) shows promising defense results against several attacks, such as BadNets, where it achieves an ASR of 1.51% and an RA of 92.46%. However, it fails to generalize to more complex attacks, like the Blended attack, where the ASR increases to 95.63%, and the RA drops to 4.16%. Neural Attention Distillation (NAD) exhibits similar behavior in terms of generalization, with its performance varying considerably across different attacks. For example, NAD achieves an ASR of 1.10% and an RA of 92.39% against BadNets, but its performance drops significantly when faced with the Blended attack, resulting in an ASR of 44.98% and an RA of 42.60%.
For pruning-based methods, e.g., FP, Adversarial Neuron Pruning (ANP), and Channel Lipschitz Pruning (CLP), these

Backdoor

BadNets

Blended

Low Frequency

SSBA

WaNet

AVG

Attack Acc ASR RA Acc ASR RA Acc ASR RA Acc ASR RA Acc ASR RA Acc↑ ASR↓ RA↑

CIFAR10

Benign FT FP CLP ANP NC NAD Our

91.94 97.21 - 93.44 99.95 - 93.44 99.39 - 92.94 98.80 - 91.53 98.83 - 92.65 98.84 93.35 1.51 92.46 93.54 95.63 4.16 93.41 77.89 20.67 93.28 71.57 26.52 94.12 22.30 74.53 93.54 53.78 43.67 91.62 16.06 79.24 93.21 96.84 3.04 93.24 96.73 2.79 92.88 82.92 16.00 91.30 0.62 84.91 92.45 58.63 37.20 91.20 94.77 5.01 93.29 99.83 0.17 92.47 99.10 0.82 88.27 9.96 76.22 89.03 53.84 41.88 90.85 71.5 24.82 91.22 73.36 26.16 93.25 99.44 0.56 93.19 98.03 1.88 92.92 68.59 29.13 90.81 1.93 88.98 92.28 68.27 29.34 89.11 1.32 89.17 93.23 99.90 0.10 90.67 2.26 86.24 90.33 0.53 86.72 90.54 86.91 12.38 90.78 38.18 54.92 92.97 1.10 92.39 92.18 44.98 42.60 92.32 13.43 77.03 92.22 38.18 57.18 94.16 12.61 83.22 92.77 22.06 70.48 91.67 1.67 91.71 91.85 6.44 74.43 91.77 1.90 90.52 91.92 2.89 88.59 93.42 1.38 92.16 92.13 2.86 87.40

CIFAR100

Benign FT FP CLP ANP NC NAD Our

67.33 93.61 - 70.19 99.84 - 69.56 97.47 - 69.24 98.12 - 65.67 97.50 - 68.39 97.31 69.90 1.38 67.18 70.32 89.36 6.57 69.93 45.90 35.95 69.30 58.14 28.45 71.16 6.41 63.64 70.12 40.23 40.35 67.59 23.09 54.04 69.44 93.56 4.03 68.73 82.60 10.84 68.25 76.46 14.52 66.33 81.47 11.26 68.06 71.44 18.94 59.74 74.79 19.42 68.56 99.52 0.37 65.46 92.88 5.18 68.40 89.57 7.44 60.06 6.93 54.53 64.44 72.74 17.38 66.95 15.36 58.05 70.05 97.70 1.70 69.47 55.00 32.00 68.92 89.87 7.54 64.23 0.23 60.55 67.92 51.63 31.97 64.67 0.10 64.38 64.16 1.14 34.63 65.25 2.28 53.91 65.16 2.17 56.86 67.09 1.29 62.92 65.27 1.39 54.54 68.84 0.31 67.63 69.24 81.07 10.16 69.33 31.78 44.41 68.39 30.82 42.24 71.57 17.41 57.72 69.47 32.27 44.43 66.25 0.09 67.23 67.27 0.41 40.49 66.32 0.18 65.83 66.77 0.39 60.61 68.79 0.18 66.84 67.08 0.25 60.20

GTSRB

Benign FT FP CLP ANP NC NAD Our

98.04 96.38 - 98.19 100.00 - 98.21 99.96 - 97.73 99.72 - 98.66 97.59 - 98.17 98.72 98.60 0.49 98.11 98.19 99.61 0.34 98.39 92.94 5.43 97.63 96.75 2.99 99.20 0.73 98.31 98.40 58.10 41.03 97.79 0.04 65.20 98.04 100.00 0.00 97.81 99.25 0.45 96.90 99.64 0.34 98.80 0.05 13.23 97.87 59.79 15.84 96.42 88.11 11.67 97.68 100.00 0.00 97.09 97.74 1.82 97.13 99.42 0.57 62.17 99.91 0.08 90.10 97.03 2.83 96.55 15.71 82.06 98.22 99.96 0.02 98.27 69.20 27.66 97.10 99.51 0.48 98.67 1.65 96.32 97.76 57.20 41.30 97.75 0.00 97.75 96.34 2.47 53.58 97.72 7.29 81.19 96.94 3.70 88.40 98.39 0.00 97.29 97.43 2.69 83.64 98.66 0.03 98.64 98.39 96.98 2.86 98.54 80.88 14.91 97.72 94.70 4.93 98.98 0.16 98.66 98.45 54.55 44.00 98.60 0.00 98.66 90.36 1.07 64.17 90.01 0.02 95.58 97.41 0.51 89.87 99.05 0.00 98.93 95.08 0.32 89.44

TABLE I: Performance comparison (%) of backdoor defense methods on CIFAR10, CIFAR100, and GTSRB datasets under PreActResNet18, under different attack strategies with a poison rate of 10% and retraining data ratio of 100%. The table presents the clean accuracy (Acc) and attack success rate (ASR) for each dataset, attack, and defense method. The defense methods include Fine-tuning (FT), Fine-pruning (FP), Activation Clustering (AC), Adversarial Neuron Pruning (ANP), Neural Cleanse (NC), Spectral Signatures (Spectral), Neural Attention Distillation (NAD), and our proposed method (Our).

approaches demonstrate varying levels of effectiveness against different attacks. However, none consistently outperform our method FMT across all attack types. For instance, FP achieves an ASR of 16.06% and an RA of 79.24% against BadNets, whereas ANP attains an ASR of 2.10% and an RA of 86.11% for the same attack. In comparison, our method FMT consistently delivers superior results, as shown in Tab. I.
For the reverse engineering-based method (i.e., NC), we can observe that when the backdoor triggers are simple and easy to reproduce, NC will have high performance, e.g., NC achieves an ASR of 1.32% and an RA of 89.17% against BadNets. However, when the backdoor triggers are complex and invisible, the performance of NC significantly deteriorates, indicating its limited effectiveness in handling sophisticated backdoor attacks. For instance, when faced with the Blended attack, where the backdoor trigger is more intricate and harder to detect, NC’s ASR increases to an alarming 99.90%, and its RA plummets to a mere 0.10%.
For the CIFAR100 and GTSRB datasets, we can observe that first, FMT is also better than baseline defense strategies. Specifically, FMT obtain 0.25% average ASR and 60.20%

average RA in CIFAR100, which decreases 1.14% ASR compared with baseline approaches, and FMT also obtain 0.32% average ASR and 89.44% average RA in GTSRB, which increases 5.80% RA compared with baseline approaches. Second, we can also observe that baseline approaches can not have effective performances. For example, NC achieves an ASR of 0.10%, and an RA of 64.38% against BadNets, but its effectiveness decreases against more sophisticated attacks (e.g., Blended), while FMT can obtain consistently low ASR (e.g., 0.25% average ASR in CIFAR100 and 0.32% average ASR in GTSRB) and higher RA (e.g., 60.20% average RA in CIFAR100 and 89.44% average RA in GTSRB), which means baseline approaches can not stable in all attack scenarios.
Answer to RQ1: FMT is an effective defense strategy to mitigate backdoor from the model.
C. Efficiency
DNN developers often have limited computational resources, making the overhead of defense methods an essential factor affecting their viability. To evaluate the overhead of

FMT and baseline methods, we recorded the time consumed by each defense strategy across different datasets and attack methods. Plots are shown in Fig. 4. We can observe that FMT is only slower than CLP, which does not involve retraining the DNN model and only prunes some feature maps in the DNN. However, as shown in Tab. I, CLP does not have state-of-theart effectiveness compared with other baselines. We believe that the extra time overhead of FMT compared with CLP is worthwhile. We can also observe that NC and FP have larger overhead compared to other approaches. This is because NC evaluates whether a class in the dataset is the target label, increasing time consumption as the number of classes increases. FP has a larger overhead because it repairs the model by pruning one feature map and then testing it, meaning the testing step causes a more significant overhead compared to other strategies, which is also consistent with prior evaluations [19].
In summary, we can observe that FMT’s overhead is lower than most defense strategies, making it a more practical choice for developers with limited computational resources. Additionally, it is essential to note that while some methods like NC and FP might offer good protection, their high overhead can threaten the validity for use in real-world scenarios. Since FMT demonstrates strong defense capabilities while maintaining relatively low overhead highlighting its potential usefulness in a wide range of applications and settings, making it a more practical choice for developers with limited computational resources.

Defense Strategy BadNets Blended LowFreq SSBA WaNet

CIFAR10-FP CIFAR10-CLP CIFAR10-Our

43.18 16.67 83.33

31.37 0.00 41.18

23.52 0.00 66.67

35.29 4.00 66.67

54.90 25.00 66.67

CIFAR100-FP CIFAR100-CLP CIFAR100-Our

19.60 0.00 50.00

33.33 0.00 66.67

35.29 0.00 58.33

31.37 8.33 50.00

25.49 33.33 100.00

GTSRB-FP GTSRB-CLP GTSRB-Our

21.57 16.67 66.67

29.41 3.92 66.67

27.45 0.00 66.67

41.17 1.96 83.33

39.21 0.00 91.67

TABLE II: Validity and coverage results (%).
serve that compared with existing feature map pruning techniques (i.e., FP, CLP)1, FMT has higher coverage, which explains why FMT can decrease more ASR than baseline approaches. For example, FMT obtain 83.33% coverage in cifar10 & BadNets attack, while FP and CLP only obtain 43.18% and 16.67% backdoor feature map coverage. In CIFAR100 & BadNets, FMT obtain 50.00% coverage while FP and CLP only obtain 19.60% and 0.00% coverage. In GTSRB & BadNets combination, FMT obtain 66.67% coverage while FP and CLP only obtain 21.57% and 16.67% coverage, and other dataset and attack combinations also prove that FMT can obtain higher backdoor feature map coverage compared with baseline approaches.

Answer to RQ3: FMT can detect more backdoor feature maps from the model than baseline approaches.

Answer to RQ2: FMT can efficiently mitigate backdoor from the model.

E. Sensitivity

D. Coverage

In this section, we investigate the coverage of the backdoor feature map which is detected by FMT and baseline feature map pruning approaches. In particular, we follow the existing work [23, 22] and investigate the number of backdoor triggers detected by FMT. Since only FP, and CLP are focused on feature map level, in this section, we will only use these baselines to evaluate FMT’ backdoor feature map detection coverage. To measure the coverage of FMT, we define the backdoor feature map coverage as follows:

Cov =

n i=1

I(f i

∈

T ACT opτ%(f ))

B

Here, n is the total number of feature maps detected by FMT, the indicator function returns 1 if the i-th detected feature map is backdoor (i.e., belongs to the top τ %, 10% by default, of feature maps with the highest sensitivity) and 0 otherwise, and B is the total number of backdoor feature maps detected by feeding clean samples and poison samples into the model and analyzing feature map sensitivity (also named TAC) [23]. Since higher TAC indicates higher sensitivity of the channel to the trigger, for ease of discussion, we label 10% that have the highest TAC.
The experiment results are shown in Tab. II, we can ob-

1) Effect of Poison Data Rate: The poison rate, referring to the proportion of poisoned samples in the training dataset, plays a crucial role in backdoor trigger injection results. We conducted experiments with different poison rates (from 0.1% to 10%) to explore their impact on FMT’s effectiveness. The results, shown in Table III, indicate that FMT demonstrates consistent performance across different poison rates and effectively mitigates backdoor attacks. For example, considering the BadNets attack, the ASR increases slightly from 1.71% to 1.67% as the poisoning rate increases from 0.1% to 10%. This trend is observed for other attack strategies as well.
Although it may be expected that a higher poison rate would lead to a higher ASR, our experimental results show that this is not true. When the poisoning rate is very low, it becomes more challenging for defense strategies to detect the backdoor trigger from the model due to its subtle influence. As the poison rate increases, the backdoor trigger has a more noticeable impact on the model, which can be more easily detected and mitigated by the defense strategy. Our experimental results emphasize the importance of designing defense strategies capable of detecting and mitigating backdoor attacks, even when dealing with subtle influences caused by low poison rates.
1NAD is a distillation defense strategy, which is not performed in the feature map level.

Overhead (/s) Overhead (/s) Overhead (/s)

Feature

1750

Fine-tuning Fine-pruning

CLP

1500

ANP NC

NAD

1250

1000

200 750 150

500

100 50

250 Ba0dNets

Blended

x

0

BadNets

Blended

Low Freq

SSBA

3500

8000

200

150

6000 100

50

Ba0dNets

Blended

4000

x

2000

Feature Fine-tuning Fine-pruning CLP ANP NC NAD

3000

200

2500 150

100

2000 50

1500

Ba0dNets

x

Blended

1000

500

0

WaNet

BadNets

Blended

Low Freq

SSBA

0

WaNet

BadNets

Blended

Low Freq

SSBA

Fig. 4: Overhead of FMT and baseline approaches.

Poison

BadNets

Blended

Low Frequency

SSBA

WaNet

Rate (%) Acc ASR RA Acc ASR RA Acc ASR RA Acc ASR RA Acc ASR RA

0.1 92.09 1.71 91.72 91.83 8.53 75.90 91.74 2.09 91.00 92.17 2.23 89.60 93.19 1.48 91.91 0.5 92.01 0.99 91.74 91.95 8.31 75.30 91.92 2.14 91.21 91.90 1.42 91.92 93.32 1.45 91.94 1 92.22 1.78 91.48 91.66 8.49 74.49 92.06 2.08 91.00 91.85 2.08 89.61 93.46 1.32 92.22 5 92.59 1.24 90.39 93.45 8.31 74.47 93.32 2.23 90.07 93.10 2.93 88.49 91.86 1.09 91.90 10 91.67 1.67 91.71 91.85 6.44 74.43 91.77 1.90 90.52 91.92 2.89 88.59 93.42 1.38 92.16

Feature Fine-tuning Fine-pruning CLP ANP NC NAD
WaNet

TABLE III: FMT’s effectiveness under different poison rate.

2) Effectiveness under Different Percentages of Clean Data: We are also interested in studying the correlation between the performance of FMT and the amount of available training data, which will be used to repair the model to mitigate backdoor triggers. We compare four different retraining data ratios:5%, 10%, 15%, 20%, and 100%, and the results of our FMT are demonstrated in Tab. IV. We observe that the performance of our defense strategy improves as the amount of clean training data increases. For instance, when the retraining ratio is increased from 5% to 100%, the Attack Success Rate (ASR) for BadNets decreases from 1.77% to 0.9%, while the model accuracy (Acc) improves from 86.57% to 92.02% and the Retained Accuracy (RA) increases from 86.08% to 91.04%. Similar trends can be observed for other attack strategies such as Blended, Low Frequency, SSBA, and WaNet. This indicates that our defense strategy becomes more effective in mitigating backdoor attacks as more clean data is available for retraining the model. However, it is worth noting that even with a small amount of clean data (e.g., 5%), our defense strategy still exhibits competitively good performance in mitigating backdoor attacks. For example, with a 5% retraining ratio, the ASR for WaNet is 1.54%, while the Acc and RA are 89.31% and 88.07%, respectively.

Answer to RQ4: FMT work stably under different attack settings.
F. Threats to Validity
Parameters settings. One potential threat to validity is the parameter settings in the experiments. Backdoor attack and defense strategies have several parameter settings, which can affect the experimental results. To alleviate potential bias, we follow the authors’ suggested settings or employ the default settings from the original papers. However, there is still a possibility that different parameter configurations could yield varying results, making it essential to consider the potential impact of parameter tuning on the effectiveness of the proposed defense method. Dataset and model. Our evaluation is mainly focused on image classification tasks using benchmark datasets such as CIFAR-10, CIFAR-100, and GTSRB, as well as the PreActResNet18 model. The performance of our proposed defense method might be different when applied to other datasets, tasks, or models. To generalize our findings and ensure the robustness of our defense strategy, further investigations on a wider range of datasets, tasks, and models are necessary.

In practice, data collection can be time-consuming and expensive, especially when dealing with large-scale datasets. Thus, the ability of our defense strategy to perform effectively with a limited amount of clean data (e.g., 5%, 10% retraining ratio) demonstrates its practicality in real-world scenarios where DNN developers may not have access to extensive clean datasets for model retraining. This finding highlights the importance of having access to clean data for defending against backdoor attacks while also emphasizing the cost-effective nature of our proposed defense strategy in improving the overall security of deep learning models.

Transferability of attacks. We have evaluated FMT against a range of SOTA backdoor attack strategies. However, it is worth noting that novel backdoor attack strategies might be developed in the future, which could potentially bypass our defense mechanism. To address this issue, continuous evaluation and adaptation of defense strategies are crucial to ensure their effectiveness against emerging attack methods.
V. RELATED WORK
This section discusses the related work in two groups: backdoor attack and backdoor defense.

Retraining BadNets

Blended

Low Frequency

SSBA

WaNet

ratio (%) Acc ASR RA Acc ASR RA Acc ASR RA Acc ASR RA Acc ASR RA

5 86.57 1.77 86.08 90.59 7.63 63.40 88.07 4.60 81.00 87.26 3.50 79.77 89.31 1.54 88.07 10 91.67 1.67 91.71 91.85 6.44 74.43 91.77 1.90 90.52 91.92 2.89 88.59 93.42 1.38 92.16 15 91.71 1.66 91.30 91.88 6.37 74.61 91.60 1.88 90.63 91.95 2.87 88.64 92.47 1.42 91.32 20 91.83 1.47 91.91 91.92 6.08 74.87 91.74 1.90 90.73 91.73 2.88 88.91 92.54 1.44 91.33 100 92.02 0.9 91.04 92.12 5.31 75.77 91.24 1.71 91.07 92.31 2.67 89.37 92.95 1.37 91.42

TABLE IV: FMT’s effectiveness under different retraining data ratio.

A. Backdoor Attacks
According to the threat model, existing backdoor attack methods can be partitioned into two general categories, including data poisoning and training controllable.
Data poisoning attacks involve an attacker manipulating the training data. Existing methods in this category focus on designing different types of triggers to improve imperceptibility and attack effectiveness. These triggers can be classified based on various characteristics, such as visibility, locality, additivity, and sample specificity. Some attacks use visible triggers, such as BadNets [12], which inject a square box into the inputs, while other methods use invisible triggers, like Blended [11], to remain stealthier and harder to detect. Local triggers, as used in Label Consistent Attack [37], only affect a small region of the input, while global triggers, like those in SIG [38], have a more widespread impact. Additive triggers, such as those in Blended [11], modify the input by adding a pattern, while non-additive triggers, like in LowFreq [32], involve more complex manipulations. Some attacks use sample-agnostic triggers, such as BadNets [12], which apply the same trigger to all poisoned samples, whereas sample-specific triggers, like in SSBA [20], tailor the trigger to each input.
Training controllable attacks involve an attacker having control over both the training process and the training data simultaneously, allowing them to learn the trigger and the model weights jointly for potentially more effective and stealthy backdoor attacks. Examples include Input-Aware [39], where the attacker adapts the backdoor trigger during the training process by considering the input distribution and the model’s internal representation, making the backdoor trigger more difficult to detect and more robust against defense techniques, and WaNet [21], which learns the trigger and model weights jointly by optimizing a weighted combination of clean and poisoned samples, enabling the attacker to implant a more stealthy backdoor while maintaining a high attack success rate.
The above-mentioned backdoor attack poses a significant threat to the validity of DNN models, undermining their integrity and potentially causing misclassifications that can have severe consequences. To address this problem, we propose FMT, a novel defense mechanism designed to detect and mitigate the effects of backdoor attacks by identifying and removing backdoor feature maps in the DNN model.
B. Prior Work on Backdoor Defenses.
The research community has proposed a variety of defense mechanisms to detect and mitigate backdoor attacks in deep neural networks. These defenses can be broadly categorized

into two groups: data-centric and model-centric approaches. Data-centric defenses primarily focus on detecting and
cleansing poisoned data points in the training dataset to prevent the model from learning the backdoor trigger. One such technique is the AC proposed by Chen et al. [16], which identifies and removes poisoned data points by clustering activations of the penultimate layer. However, once the DNN developers rely on the third party to train the DNN model, they can not use these strategies to remove backdoor samples since they can not interfere with the third-party platforms.
Model-centric defenses, on the other hand, focus on analyzing and modifying the trained model itself to remove the backdoor behavior. NC [13] is a model-centric defense that identifies and neutralizes backdoor triggers by computing anomaly scores based on the L1-norm of adversarial perturbations required to reverse engineer the triggers. However, once the trigger become complex and invisible, DNN developers can not use NC to reproduce the triggers and can not remove the trigger from the model successfully. The Fine-Pruning [22] approach, as mentioned earlier, removes backdoors by pruning redundant feature maps less useful for normal classification. Another model-centric defense is NAD [33], which identifies backdoor neurons by analyzing the attribution of each neuron’s output with respect to the input features. However, without the backdoor samples, or poison samples from the injected datasets, they can not correctly analyze the backdoor model’s internal information, which causes them to have lower effectiveness to remove the backdoor trigger from the model.
While these defenses have shown promise in detecting and mitigating backdoor attacks, they face challenges when dealing with sophisticated triggers or trigger inaccessible scenarios. For instance, AC is less effective when the attacker uses complex triggers or distributes poisoned data points uniformly across the dataset. Similarly, NC and Fine-Pruning may struggle with advanced backdoor attacks that involve multiple triggers or stealthy triggers that blend seamlessly with the input data. In this work, we address these problems by proposing FMT, which mitigates backdoor triggers from the model by identifying and removing backdoor feature maps in the model. Our experiment results prove that FMT can remove the sophisticated triggers and increase the model’s RA.
VI. CONCLUSION
In this paper, we presented a novel defense strategy, FMT, to effectively detect and mitigate backdoor attacks in DNNs. Our approach focuses on identifying and eliminating backdoor feature maps within the DNN model. Through extensive exper-

iments, we demonstrated the effectiveness of our FMT defense strategy against a variety of backdoor attack methods, outperforming existing state-of-the-art defense techniques in terms of attack success rate reduction and computational overhead. Furthermore, our approach demonstrated its potential for use in situations when developers have constrained computational resources.
REFERENCES
[1] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma, Z. Huang, A. Karpathy, A. Khosla, M. S. Bernstein, A. C. Berg, and L. Fei-Fei, “Imagenet large scale visual recognition challenge,” International Journal of Computer Vision, vol. 115, pp. 211–252, 2014.
[2] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, “Bert: Pre-training of deep bidirectional transformers for language understanding,” ArXiv, vol. abs/1810.04805, 2019.
[3] D. S. Park, W. Chan, Y. Zhang, C.-C. Chiu, B. Zoph, E. D. Cubuk, and Q. V. Le, “Specaugment: A simple data augmentation method for automatic speech recognition,” ArXiv, vol. abs/1904.08779, 2019.
[4] M. Bojarski, D. W. del Testa, D. Dworakowski, B. Firner, B. Flepp, P. Goyal, L. D. Jackel, M. Monfort, U. Muller, J. Zhang, X. Zhang, J. Zhao, and K. Zieba, “End to end learning for self-driving cars,” ArXiv, vol. abs/1604.07316, 2016.
[5] P. Rajpurkar, J. A. Irvin, K. Zhu, B. Yang, H. Mehta, T. Duan, D. Y. Ding, A. Bagul, C. Langlotz, K. S. Shpanskaya, M. P. Lungren, and A. Ng, “Chexnet: Radiologistlevel pneumonia detection on chest x-rays with deep learning,” ArXiv, vol. abs/1711.05225, 2017.
[6] Google, “Google cloud,” 2023, accessed: 2023-05-02. [Online]. Available: https://cloud.google.com/
[7] I. Amazon Web Services, “Amazon web services (aws),” 2023, accessed: 2023-05-02. [Online]. Available: https://aws.amazon.com/
[8] L. Huawei Technologies Co., “Huawei cloud,” 2023, accessed: 2023-05-02. [Online]. Available: https://www. huaweicloud.com/
[9] DataTurks, “Dataturks: Data annotations made super easy,” 2023, accessed: 2023-05-02. [Online]. Available: https://dataturks.com/
[10] H. Face, “Hugging face: The ai community building the future,” 2023, accessed: 2023-05-02. [Online]. Available: https://huggingface.co/
[11] X. Chen, C. Liu, B. Li, K. Lu, and D. X. Song, “Targeted backdoor attacks on deep learning systems using data poisoning,” ArXiv, vol. abs/1712.05526, 2017.
[12] T. Gu, B. Dolan-Gavitt, and S. Garg, “Badnets: Identifying vulnerabilities in the machine learning model supply chain,” ArXiv, vol. abs/1708.06733, 2017.
[13] B. Wang, Y. Yao, S. Shan, H. Li, B. Viswanath, H. Zheng, and B. Y. Zhao, “Neural cleanse: Identifying and mitigating backdoor attacks in neural networks,” 2019 IEEE

Symposium on Security and Privacy (SP), pp. 707–723, 2019. [14] B. Wu, H. Chen, M. Zhang, Z. Zhu, S. Wei, D. Yuan, C. Shen, and H. Zha, “Backdoorbench: A comprehensive benchmark of backdoor learning,” ArXiv, vol. abs/2206.12654, 2022. [15] Y. Li, B. Wu, Y. Jiang, Z. Li, and S. Xia, “Backdoor learning: A survey,” IEEE transactions on neural networks and learning systems, vol. PP, 2020. [16] B. Chen, W. Carvalho, N. Baracaldo, H. Ludwig, B. Edwards, T. Lee, I. Molloy, and B. Srivastava, “Detecting backdoor attacks on deep neural networks by activation clustering,” ArXiv, vol. abs/1811.03728, 2018. [17] W. Chen, B. Wu, and H. Wang, “Effective backdoor defense by exploiting sensitivity of poisoned samples,” Advances in Neural Information Processing Systems, vol. 35, pp. 9727–9737, 2022. [18] R. Zheng, R. Tang, J. Li, and L. Liu, “Data-free backdoor removal based on channel lipschitzness,” in European Conference on Computer Vision, 2022. [19] ——, “Pre-activation distributions expose backdoor neurons,” Advances in Neural Information Processing Systems, vol. 35, pp. 18 667–18 680, 2022. [20] Y. Li, Y. Li, B. Wu, L. Li, R. He, and S. Lyu, “Invisible backdoor attack with sample-specific triggers,” 2021 IEEE/CVF International Conference on Computer Vision (ICCV), pp. 16 443–16 452, 2020. [21] A. Nguyen and A. Tran, “Wanet - imperceptible warpingbased backdoor attack,” ArXiv, vol. abs/2102.10369, 2021. [22] K. Liu, B. Dolan-Gavitt, and S. Garg, “Fine-pruning: Defending against backdooring attacks on deep neural networks,” in International Symposium on Recent Advances in Intrusion Detection, 2018. [23] B. Zhao and Y. Lao, “Clpa: Clean-label poisoning availability attacks using generative adversarial nets,” in AAAI Conference on Artificial Intelligence, 2022. [24] M. D. Zeiler and R. Fergus, “Visualizing and understanding convolutional networks,” in European Conference on Computer Vision, 2013. [25] A. Krizhevsky and G. Hinton, “Cifar-10 and cifar100 (canadian institute for advanced research),” 2009, accessed: 2023-05-02. [Online]. Available: https://www. cs.toronto.edu/∼kriz/cifar.html [26] J. Stallkamp, M. Schlipsing, J. Salmen, and C. Igel, “The german traffic sign recognition benchmark: A multi-class classification competition,” In Proceedings of the IEEE International Joint Conference on Neural Networks (IJCNN), 2012, accessed: 202305-02. [Online]. Available: http://benchmark.ini.rub.de/ ?section=gtsrb&subsection=dataset [27] ASE2023Paper, “Fmt soursecode,” 2023, accessed: 2023-05-02. [Online]. Available: https://github.com/ ASE2023Paper/FMT/ [28] B. Wang, Y. Yao, S. Shan, H. Li, B. Viswanath, H. Zheng, and B. Y. Zhao, “Neural cleanse: Identifying and miti-

gating backdoor attacks in neural networks,” 2019 IEEE Symposium on Security and Privacy (SP), pp. 707–723, 2019. [29] B. Tran, J. Li, and A. Madry, “Spectral signatures in backdoor attacks,” in Neural Information Processing Systems, 2018. [30] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image recognition,” 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 770–778, 2015. [31] S. Ren, K. He, R. Girshick, and J. Sun, “Faster r-cnn: Towards real-time object detection with region proposal networks,” in Advances in Neural Information Processing Systems, 2015, pp. 91–99. [32] Y. Zeng, W. Park, Z. M. Mao, and R. Jia, “Rethinking the backdoor attacks’ triggers: A frequency perspective,” 2021 IEEE/CVF International Conference on Computer Vision (ICCV), pp. 16 453–16 461, 2021. [33] Y. Li, N. Koren, L. Lyu, X. Lyu, B. Li, and X. Ma, “Neural attention distillation: Erasing backdoor triggers from deep neural networks,” ArXiv, vol. abs/2101.05930, 2021. [34] Y. Li, X. Lyu, N. Koren, L. Lyu, B. Li, and X. Ma, “Antibackdoor learning: Training clean models on poisoned data,” in Neural Information Processing Systems, 2021. [35] K. Huang, Y. Li, B. Wu, Z. Qin, and K. Ren, “Backdoor defense via decoupling the training process,” ArXiv, vol. abs/2202.03423, 2022. [36] D. Wu and Y. Wang, “Adversarial neuron pruning purifies backdoored deep models,” ArXiv, vol. abs/2110.14430, 2021. [37] A. Turner, D. Tsipras, and A. Madry, “Label-consistent backdoor attacks,” ArXiv, vol. abs/1912.02771, 2019. [38] M. Barni, K. Kallas, and B. Tondi, “A new backdoor attack in cnns by training set corruption without label poisoning,” 2019 IEEE International Conference on Image Processing (ICIP), pp. 101–105, 2019. [39] A. Nguyen and A. Tran, “Input-aware dynamic backdoor attack,” ArXiv, vol. abs/2010.08138, 2020.

