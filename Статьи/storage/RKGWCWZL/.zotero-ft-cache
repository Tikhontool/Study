1

Enhancing Backdoor Attacks with Multi-Level MMD Regularization

Pengfei Xia, Hongjing Niu, Ziqiang Li, and Bin Li, Member, IEEE

arXiv:2111.05077v2 [cs.LG] 13 Mar 2022

Abstract—While Deep Neural Networks (DNNs) excel in many tasks, the huge training resources they require become an obstacle for practitioners to develop their own models. It has become common to collect data from the Internet or hire a third party to train models. Unfortunately, recent studies have shown that these operations provide a viable pathway for maliciously injecting hidden backdoors into DNNs. Several defense methods have been developed to detect malicious samples, with the common assumption that the latent representations of benign and malicious samples extracted by the infected model exhibit different distributions. However, a comprehensive study on the distributional differences is missing. In this paper, we investigate such differences thoroughly via answering three questions: 1) What are the characteristics of the distributional differences? 2) How can they be effectively reduced? 3) What impact does this reduction have on difference-based defense methods? First, the distributional differences of multi-level representations on the regularly trained backdoored models are veriﬁed to be signiﬁcant by introducing Maximum Mean Discrepancy (MMD), Energy Distance (ED), and Sliced Wasserstein Distance (SWD) as the metrics. Then, ML-MMDR, a difference reduction method that adds multi-level MMD regularization into the loss, is proposed, and its effectiveness is testiﬁed on three typical difference-based defense methods. Across all the experimental settings, the F1 scores of these methods drop from 90%-100% on the regularly trained backdoored models to 60%-70% on the models trained with ML-MMDR. These results indicate that the proposed MMD regularization can enhance the stealthiness of existing backdoor attack methods. The prototype code of our method is now available at https://github.com/xpf/Multi-Level-MMD-Regularization.

Index Terms—Deep Neural Networks, Backdoor Attacks, Distributional Differences, Maximum Mean Discrepancy.
!

1 INTRODUCTION

I N the past years, Deep Neural Networks (DNNs) have shown impressive achievements in computer vision [1], [2], [3], [4], [5], natural language processing [6], [7], [8], and some other ﬁelds [9], [10], [11]. It is widely believed that the success of DNNs is closely related to their largescale models, consumption of a huge amount of training data, and computational power. For example, GPT-3 [12], a deep model demonstrated to be effective in various tasks, comprises 175 billion parameters and is pretrained on 45 TB of text data. A training cycle for this model on a Tesla V100 GPU would require $4.6 million and 355 years1. The superior performance of DNNs usually comes at the cost of massive time and economic resources.
To save on training costs, it has become common for users and companies to collect data from the Internet, use pretrained parameters, or hire a third party to train models. Unfortunately, these operations pose security risks due to the possible presence of malicious sources and parties. One of the major threats is dubbed as backdoor attacks or Trojan attacks [15], [16], [17], [18], where a hidden backdoor is injected into the victim model during the training phase and can be activated by a predeﬁned trigger. The infected model would exhibit duel characteristics. When the backdoor is not activated, it behaves as normal as a benign model. But once
• P. Xia, Z. Li and B. Li are with the Department of Electronic Engineering and Information Science, University of Science and Technology of China, Hefei, China. E-mail: {xpengfei, iceli}@mail.ustc.edu.cn, binli@ustc.edu.cn.
• H. Niu is with the Department of Automation, University of Science and Technology of China, Hefei, China. E-mail: sasori@mail.ustc.edu.cn.
1. https://lambdalabs.com/blog/demystifying-gpt-3/

Benign Inputs

The Infected Model

0

Malicious Inputs

The Latent Representations

Activation Clustering

Spectral Signatures

Benign Malicious

Fig. 1. An illustrative example of difference-based defense methods. The infected model is trained to classify all inputs with a trigger, i.e., the white square in the upper left corner of the images, as the number 0. Some defense methods, such as activation clustering [13] and spectral signatures [14], use the latent representations extracted by the infected model to distinguish malicious samples from benign ones.
triggered, the predictions are forced to an attacker-speciﬁc target. Backdoor attacks have threatened the deployment of DNNs in security-sensitive scenarios, such as face recognition systems [15] and autonomous vehicles [16].
Several defense methods [13], [14], [19], [20] have been presented by distinguishing malicious samples from benign ones according to their latent representations extracted by the infected model. For example, Chen et al. [13] proposed activation clustering, which is based on the observation that the projected activations of the last hidden layer are

divided into two distinct clusters. Tran et al. [14] proposed calculating an outlier score for each sample by performing singular value decomposition on the representations. The inputs with the top scores would be removed as malicious samples. An illustrative example is shown in Fig. 1.
The previously presented defense methods all rely on a common assumption: the latent representations of benign and malicious samples exhibit different distributions. A naturally arisen question is whether this assumption would always be true. There is a shortage of a comprehensive investigation on this issue. Some reduction methods [21], [22], [23] have been proposed to reduce the distributional difference in the latent space. However, in Section 5 and Section 6, we will show that these methods are not sufﬁcient to provide a solid solution.
In this paper, we focus on this commonly adopted assumption and investigate the following questions:
• What are the characteristics of the distributional differences between benign and malicious samples in the latent space extracted by the infected model?
• How can they be effectively reduced when conducting backdoor attacks?
• What impact does this reduction have on differencebased defense methods?
The main contributions of this paper include:
• Three typical metrics, i.e., Maximum Mean Discrepancy (MMD) [24], Energy Distance (ED) [25], and Sliced Wasserstein Distance (SWD) [26], are introduced to explicitly quantify the distributional differences in the latent spaces. The differences between benign and malicious samples in multi-level representations are veriﬁed to be large. This motivates us to take the multiple levels of features into consideration when designing the reduction method, other than those of the last hidden layer.
• A new method, ML-MMDR, is proposed by introducing Multi-Level MMD Regularization into the loss when training a backdoored model. The experimental results indicate that the proposed method can fully reduce the differences without compromising the attack strength.
• The effectiveness of ML-MMDR is testiﬁed on three typical difference-based defense methods. The experimental results show that the performance of these methods is substantially degraded by the proposed regularization. This illustrates that ML-MMDR can enhance existing attack methods to escape detection.
The rest of this paper is organized as follows. In Section 2, the preliminaries are brieﬂy introduced. Section 3 provides the setup used in the following three sections. Section 4, Section 5, and Section 6 provide our detailed works addressing the three mentioned questions. Some open issues are discussed in Section 7, and the related works are reviewed in Section 8. Section 9 concludes this paper.
2 PRELIMINARIES
2.1 Deep Neural Networks
A DNN is composed of multiple layers and can be deﬁned as fθ = f1 ◦ f2 ◦ · · · ◦ fm, where θ denotes the parameters

2

of the model, f1, f2, · · · , fm−1 denote the m − 1 hidden layers, and fm denotes the output layer. For convenience, let zi = f1 ◦ f2 ◦ · · · ◦ fi denote the structure from the input to the i-th hidden layer and zi(x) denote the extracted representation of the input x. Given a training set D = {(x, y)},
the procedure of training a DNN can be formulated as:

min
θ

1 |D|

L(fθ(x), y),

(1)

(x,y)∈D

where (x, y) denote the input and its ground-truth label, and L denotes the loss function. The trained model is expected to perform well on a test set T , and D ∩ T = ∅ is required.

2.2 Backdoor Attacks

In this paper, the scenario of injecting a backdoor into a
DNN by dataset poisoning [15], [16], [17], [27] is considered. Let U and V denote the malicious training and test sets. The
generation of malicious samples is associated with a target class t, a trigger b, and a fusion function G. For a benign pair (x, y), the corresponding malicious pair is (x , t), where x = G(x, b). To train a DNN with a backdoor, one can
optimize the equation:

min
θ

1 |D|

L(fθ (x),

y)

+

1 |U |

L(fθ(x ), t), (2)

(x,y)∈D

(x ,t)∈U

and we suppose the trained model can generalize to the benign and malicious test sets T and V . The ratio of malicious
sample volume to benign sample volume in training data, r = |U |/|D|, is an important hyperparameter.

2.3 Statistical Distance

Let X ∼ p and Y ∼ q denote two sample sets drawn

from the distributions p and q. A statistical distance takes

the two sets as its input and returns a real number as the

distance between p and q. In this paper, since the latent

representations to be measured are high-dimensional data

and their distribution functions are difﬁcult to solve, MMD,

ED, SWD are chosen as the metrics.

Maximum Mean Discrepancy. Gretton et al. [24] introduced

a kernel-based metric, MMD, to quantify the distance be-

tween two distributions. It can be calculated as:

MMD2(X, Y

)

=

1 m2

m
k(xi, xj)
i,j=1

+

1 n2

n
k(yi, yj)
i,j=1

−

2 mn

m,n
k(xi, yj)
i,j=1

, (3)

where m and n denote the sizes of X and Y , respectively. k is a kernel function and k(x, y) = φ(x), φ(y) , where φ denotes a feature mapping. Energy Distance. Szekely and Rizzo [25] ﬁrst introduced this metric. It is a speciﬁc case of MMD, where no kernel is applied. The equation is:

ED2(X,

Y

)

=

1 m2

m i,j=1

||xi

−

xj ||

+

1 n2

n i,j=1

||yi

−

yj ||

−

2 mn

m,n
||xi
i,j=1

−

yj ||

. (4)

Sliced Wasserstein Distance. SWD is a potential alternative to Wasserstein distance [28], which can be be approximated more easily. The underlying idea is to decompose highdimensional data into one-dimensional data via random linear projection. SWD with l1 cost can be computed as:

m

SWD1(X, Y ) = E
R

min
σ∈Sm

i=1

|R(xσi )

−

R(yi)|

,

(5)

where R denotes a linear projection, and σ ∈ Sm denotes a possible sample sequence [26].

3

Clean

Patched Blended

SIG

Warped

Images

Triggers

3 SETUP
3.1 Threat Model
Our threat model considers that the user needs to use a DNN trained in an untrusted environment. This model is prevalent among companies, such as car manufacturers and autonomous driving solution providers. We assume that the attacker has complete control over the training data and the training procedure. The defender’s goal is to distinguish malicious samples from benign ones. The defender does not have any prior knowledge about the attack but has access to the parameters of the DNN and keeps a few (about 200) benign validation data.

3.2 Attack Methods
Four backdoor attack methods are considered, including patch-based attack (Patched) [16], blending-based attack (Blended) [15], SIG [27], and warping-based attack (Warped) [29]. The main differences between these methods are the trigger b and the fusion function G. The forms of generating malicious samples are shown in TABLE 1, and some examples are shown in Fig. 2. The other settings are kept the same for these methods. The target t is set to the 0th category, and the poisoning data ratio, r, is set to 0.1.

TABLE 1 Forms of generating malicious samples. m: a 2D mask. α: a
hyperparameter between 0 to 1. W: the warp function.

Attack
Patched Blended SIG Warped

Fusion Function x = G(x, b)
x = (1 − m) x + m b x = (1 − α) · x + α · b x =x+b x = W(x, b)

3.3 Defense Methods
Four defense methods are adopted to test the effect of attack methods, three of which are difference-based detection methods. Note that some of these methods were proposed for ﬁltering malicious samples in the training phase. We believe that it is also reasonable to apply them in the test phase, and the experimental results show that they perform well. Besides, the original detection methods mainly utilize the outputs of the last hidden layer. We extend them to multi-level representations for a more thorough analysis. Activation Clustering. Chen et al. [13] proposed activation clustering to detect malicious samples, which does not require veriﬁed data. The author ﬁrst observed that the

Fig. 2. Malicious examples from CelebA [30] generated by four backdoor attack methods.
features of benign and malicious inputs, which are classiﬁed into the same category by the backdoored model, visually show two distinct clusters. Then, they proposed ﬁltering malicious samples with three steps: 1) Perform independent component analysis on the latent representations to reduce the dimensionality. 2) Use k-means to cluster the data represented by the reduced features into two clusters. 3) Analyze whether each cluster is benign or malicious by exclusionary reclassiﬁcation, relative size comparison, or silhouette score. Spectral Signatures. Tran et al. [14] identiﬁed a characteristic of backdoor attacks called spectral signatures. A spectral signature is a detectable trace in the spectrum of the representations’ covariance that these attacks tend to leave behind. They then proposed an algorithm to show that one could use this trace to remove malicious samples with three steps: 1) Perform singular vector decomposition on the centered features. 2) Calculate the outlier scores with the top right singular vector. 3) Remove data with the top-k scores. Subspace Reconstruction. The core idea of subspace reconstruction for backdoor detection is to construct a subspace learned from a small number of benign representations and assume that projecting malicious representations into this space would cause considerable information to be lost. As a result, the reconstruction loss of these features is higher than that of benign features. Javaheripi et al. [31] ﬁrst introduced this idea into their Trojan detection framework. Neural Cleanse. Wang et al. [32] proposed neural cleanse, a detection and mitigation system for backdoor attacks. In the detection step, the method ﬁrst uses reverse engineering to obtain potential triggers towards every category and then determines the ﬁnal synthetic trigger according to its l1 norm. The authors introduced three techniques, i.e., input ﬁltering, neuron pruning, and unlearning, in the mitigation step to remove the backdoor with the synthetic trigger. Since input ﬁltering is similar to the three detection methods described above, unlearning needs to retrain the model, we apply neuron pruning in this paper.
3.4 Experimental Settings
Datasets. Two image datasets, CIFAR-10 [33] and CelebA [30], are selected, which are often used for backdoor learning tasks. For CelebA, following the conﬁguration in [29], [34], we use the three most balanced attributes, i.e., “Heavy Makeup”, “Mouth Slightly Open”, and “Smiling”, to create eight categories for image classiﬁcation.

4

DNN Architectures. Four DNN architectures, VGG-11 (V11) [35], VGG-16 (V-16) [35], ResNet-18 (R-18) [36], and PreActResNet-18 (P-18) [37], are used in our experiments. Since the activations of the hidden layers need to be extracted as the latent representations, we deﬁne s1, s2, and s3 to represent three locations in the network structure. zs1 (x), zs2 (x), and zs3 (x) denote the extracted features of x from the corresponding levels. Speciﬁcally, we set s1, s2, and s3 to the 14th, 21st, and 28th layers for V-11, the 23rd, 33rd, and 43rd layers for V-16, and the 27th, 39th, and 51st layers for R-18 and P-18. More details of the DNN architectures can be seen in Appendix A. Evaluation Metrics. Benign Accuracy (BA) and Attack Success Rate (ASR) are adopted as the metrics for measuring the performance of an infected model on the test sets T and V . For a difference-based defense method, we use F1 score to evaluate its performance. Implementation Details. We adopt stochastic gradient descent with a momentum of 0.9 and a weight decay of 5e-4 as the optimizer for all experiments. The batch size is set to 256, and the total training duration is 100 epochs for CIFAR10 and 40 epochs for CelebA. The initial learning rate is set to 0.01 and it is dropped by 10 after 50 and 70 epochs for CIFAR-10 and 20 and 30 epochs for CelebA. All images are resized to 32×32 and normalized between 0 and 1. Our code2 is implemented with PyTorch [38].
4 DISTRIBUTIONAL DIFFERENCES BETWEEN BE-
NIGN AND MALICIOUS SAMPLES
What are the characteristics of the distributional differences between benign and malicious samples in the latent spaces learned by an infected model? In this section, we attempt to answer this question. We ﬁrst need to quantify the differences. Given an infected DNN fθ, let ZiTj = {zi(x)|(x, y) ∈ T ∧ fθ(x) == j} and ZiVj = {zi(x )|(x , t) ∈ V ∧ fθ(x ) == j} denote the extracted representations of benign and malicious samples, which are classiﬁed into the same category j by fθ. A distributional difference refers to the dissimilarity between the distributions of ZiTj and ZiVj . In particular, since a well-trained backdoored model classiﬁes almost all malicious samples to the target t, we mainly focus on the difference between ZiTt and ZiVt . Given that the latent representations are high-dimensional, and their distributions are difﬁcult to estimate explicitly, in this paper, MMD, ED, and SWD are adopted to measure the distributional differences between benign and malicious samples.
To comprehensively analyze the differences, we calculate the three metrics on the test data at multiple levels, i.e., M(ZiTt , ZiVt ), where M is one of the three measures and i = {s1, s2, s3}. For MMD, we use a Gaussian mixture kernel. To give intuitive comparisons, for each type of attack, we provide the intra-class distance and the minimal interclass distance among clean samples as two baselines, that is, M(ZiTt , ZiTt ) and minj=t M(ZiTt , ZiTj ). To avoid being inﬂuenced by the numerical scale, we use the relative ordering (ascending) of M(ZiTt , ZiVt ) in M(ZiTt , ZiVt ), M(ZiTt , ZiTt ), and minj=t M(ZiTt , ZiTj) as the indicator. TABLE 2 presents Average Relative Ordering (ARO) over the three metrics, and the speciﬁc distance values are shown in Appendix B.
2. https://github.com/xpf/Multi-Level-MMD-Regularization

TABLE 2 ARO over MMD, ED and SWD at three levels.

Method Model

Patched Blended
SIG Warped

V-11 V-16 R-18 P-18
V-11 V-16 R-18 P-18
V-11 V-16 R-18 P-18
V-11 V-16 R-18 P-18

CIFAR-10

s1

s2

s3

2.67 3.00 2.67 2.67 3.00 2.33 2.67 3.00 2.67 2.67 2.67 2.00

2.67 2.67 2.67 2.67 2.67 2.33 2.67 2.67 2.00 3.00 2.67 3.00

2.67 3.00 3.00 3.00 3.00 2.33 2.67 2.67 3.00 3.00 3.00 2.00

3.00 3.00 3.00 3.00 3.00 2.67 2.67 2.67 2.33 3.00 3.00 2.00

CelebA

s1

s2

s3

3.00 3.00 3.00 3.00 3.00 3.00 3.00 3.00 3.00 3.00 3.00 3.00

3.00 3.00 3.00 3.00 3.00 3.00 3.00 3.00 3.00 3.00 3.00 3.00

3.00 3.00 3.00 3.00 3.00 3.00 3.00 3.00 3.00 3.00 3.00 3.00

3.00 3.00 3.00 3.00 3.00 3.00 3.00 3.00 3.00 3.00 3.00 3.00

Some characteristics can be identiﬁed:
• The distributional differences of multi-level representations are all large. We observe that for all types of backdoor attacks, all DNN architectures, and all the two datasets, there are substantial increases in the MMD, ED, and SWD values compared to the corresponding intra-class values at all three levels (see in Appendix B). The results of ARO are more intuitive, where 69.8% of the values are 3.0 and 95.8% of the values are greater than 2.0. This means that in most cases, M(ZiTt , ZiVt ) is greater than minj=t M(ZiTt , ZiTj). Both of the two observations provide strong evidence of this characteristic.
• The distributional differences have some relationship to the dataset. In general, the infected models trained on CelebA have larger ARO values than the models trained on CIFAR-10.
The ﬁrst characteristic is more inspiring. Since the distributional differences are all signiﬁcant at multi-level representations, does it mean that the previous detection methods can be extended to multiple layers? We conduct experiments on this and ﬁnd that this is indeed the case. The results are shown in Fig. 5, Fig. 6, and Fig. 7, where the F1 scores of the three difference-based detection methods at three levels on the regularly trained backdoored models are all high. This provides guidance for us to design a proper reduction method.
5 METHOD FOR REDUCING THE DISTRIBUTIONAL DIFFERENCES
How to effectively reduce the distributional differences the infected models exhibit that defense methods could utilize? According to the analysis in Section 4, we know that the differences in multi-level representations are all large. Therefore, we propose a method named ML-MMDR to reduce the differences by adding multi-level MMD regularization

Algorithm 1: Mini-batch ML-MMDR
Input: Benign training set D; malicious training set U ; Learning rate η; Number of training epochs N ; Constraint strength λ; Representation level set I; Target label t
Output: Model parameters θ

1 Initialize parameters θ;

2 Create the concatenated training set C = D ∪ U ;

3 for n ← 1 to N do

4 Shufﬂe the training set C;

5 for each mini-batch (X, Y ) ⊂ C do

6

(X1, Y1) = {(x, y) | (x, y) ∈ (X, Y ) ∧ (x, y) ∈

D};

7

(X2, Y2) = {(x, y) | (x, y) ∈ (X, Y ) ∧ (x, y) ∈

U };

8

(X3, Y3) = {(x, y) | (x, y) ∈ (X, Y ) ∧ (x, y) ∈

D ∧ y == t};

9

L1

←

1 |X1 |

· L(fθ(X1), Y1);

10

L2

←

1 |X2 |

· L(fθ(X2), Y2);

11

L3

←

1 |I |

·

i∈I MMD2(ZiX2 , ZiX3 );

12

Lt ← L1 + L2 + λ · L3;

13

θ ← θ − η · ∇θLt;

14 end

15 end

5
to 1.979, 1.979, and 2.0 at s1, s2, and s3 for CIFAR-10, respectively, compared to the models trained with RBT (λ = 0.0). 2.0 is basically the minimum value that ARO can achieve. • Constraining only the features of the last hidden layer, i.e., SL-MMDR, causes the features of other intermediate layers to have large differences still. For example, for λ = 0.3, the average ARO over the models trained with SL-MMDR is 2.833, 2.25, and 2.0 at s1, s2, and s3 for CIFAR-10. It can be seen that the difference at the ﬁrst level is even increased by SL-MMDR, which preserves the possibility of using these representations for defense. These results demonstrate that the methods of constraining only the last hidden layer [21], [22], [23] are not sufﬁcient.
To show the differences more intuitively, the representations are visualized using the dimensionality reduction technique, as shown in Fig. 4. In the model trained with RBT, the representations of benign and malicious inputs can obviously be divided into two groups. For the model trained with ML-MMDR, these two types of representations almost overlap at all three levels. The ﬁrst two levels of representations are still distinguishable for the model trained with SLMMDR. The visualization results support our observations.

to the loss during the training of a backdoored model. The proposed method can be formulated as:

min
θ

1 |D|

L(fθ(x), y)

+

1 |U |

L(fθ(x ), t)

(x,y)∈D

(x ,t)∈U

+

λ

·

1 |I |

MMD2(ZiDt , ZiUt )

, (6)

i∈I

where I is a set of levels, and λ is the hyperparameter that controls the constraint strength. The optimization goal consists of three parts, the ﬁrst two of which are included in the regular backdoor training, as shown in 2. We add the third item to reduce the distributional differences. In practice, we adopt the mini-batch ML-MMDR, and the procedure is presented in Algorithm 1.
Two settings of I are considered, including I = {s3} and I = {s1, s2, s3}, which correspond to constraining the features of the last hidden layer (SL-MMDR) and all three levels (ML-MMDR), respectively. We set λ = {0.0, 0.1, 0.2, 0.3}, where λ = 0.0 stands for Regular Backdoor Training (RBT). For convenience, we use the {A,B,C} models to denote the infected models trained on dataset A with architecture B and attack method C. The experimental results on the {P-18} models are shown in Fig. 3. The results on other architectures are similar and are shown in Appendix C.
Some observations are summarized as follows:

• The proposed ML-MMDR can signiﬁcantly reduce the distributional differences at multi-level representations without harming the attack power. For example, when λ = 0.3, the average BA and ASR over the models trained with ML-MMDR decrease from 0.907 and 0.993 to 0.902 and 0.992, respectively, and the average ARO decreases from 2.792, 2.854, and 2.5

6 EMPIRICAL STUDY ON THE EFFECTIVENESS OF THE METHOD
Since the proposed ML-MMDR is feasible in reducing the distributional differences effectively without harming the attack intensity, in this section, we investigate what impact this reduction has on difference-based defense methods. Four typical methods, including Activation Clustering (AC) [13], Spectral Signatures (SS) [14], Subspace Reconstruction (SR) [31], and Neural Cleanse (NC) [32], are selected to testify our regularization. The ﬁrst three methods are difference-based detection methods, and the last method is not. The backdoored models trained with RBT, ML-MMDR, and SL-MMDR are tested, where λ = 0.3.
6.1 Results of AC, SS, and SR
AC, SS, and SR are all difference-based detection methods that distinguish malicious samples from benign ones by using the latent representations extracted by the infected model. In our experiments, these methods are performed on the representations of three levels, i.e., s1, s2, and s3. Because the effects of the three methods are affected by the number of samples N and the ratio of malicious inputs to benign inputs r , we set up four combinations for each defense method. The detection performance of AC, SS, and SR is measured by F1 score, and the results on the {P18} models are shown in this section. The results on other architectures are similar and are presented in Appendix D. Below, we ﬁrst show the results of each defense method separately before making a uniﬁed summary. Activation Clustering. As suggested in [13], we ﬁrst adopt independent component analysis to reduce the dimensionality of the latent representations of inputs to obtain the vectors of length 20. Then we use k-means to cluster the

6

ARO, s1

ARO, s2

ARO, s3

BA

3.0

3.0

3.0

1.0

ASR 1.1

2.5

2.5

2.5

0.9

1.0

2.0

2.0

2.0

1.5

1.5

1.5

0.8

0.9

0.0 0.1 0.2 0.3 0.0 0.1 0.2 0.3 0.0 0.1 0.2 0.3 0.0 0.1 0.2 0.3 0.0 0.1 0.2 0.3

(a) CIFAR-10, Patched

ARO, s1

ARO, s2

ARO, s3

BA

3.0

3.0

3.0

1.0

ASR 1.1

2.5

2.5

2.5

0.9

1.0

2.0

2.0

2.0

1.5

1.5

1.5

0.8

0.9

0.0 0.1 0.2 0.3 0.0 0.1 0.2 0.3 0.0 0.1 0.2 0.3 0.0 0.1 0.2 0.3 0.0 0.1 0.2 0.3

(c) CIFAR-10, SIG

ARO, s1

ARO, s2

ARO, s3

BA

3.0

3.0

3.0

0.9

ASR 1.1

2.5

2.5

2.5

0.8

1.0

2.0

2.0

2.0

1.5

1.5

1.5

0.7

0.9

0.0 0.1 0.2 0.3 0.0 0.1 0.2 0.3 0.0 0.1 0.2 0.3 0.0 0.1 0.2 0.3 0.0 0.1 0.2 0.3

(e) CelebA, Patched

ARO, s1

ARO, s2

ARO, s3

BA

3.0

3.0

3.0

0.9

ASR 1.1

2.5

2.5

2.5

0.8

1.0

2.0

2.0

2.0

1.5

1.5

1.5

0.7

0.9

0.0 0.1 0.2 0.3 0.0 0.1 0.2 0.3 0.0 0.1 0.2 0.3 0.0 0.1 0.2 0.3 0.0 0.1 0.2 0.3

(g) CelebA, SIG

ARO, s1

ARO, s2

ARO, s3

BA

3.0

3.0

3.0

1.0

ASR 1.1

2.5

2.5

2.5

0.9

1.0

2.0

2.0

2.0

1.5

1.5

1.5

0.8

0.9

0.0 0.1 0.2 0.3

0.0 0.1 0.2 0.3

0.0 0.1 0.2 0.3

0.0 0.1 0.2 0.3

0.0 0.1 0.2 0.3

(b) CIFAR-10, Blended

ARO, s1

ARO, s2

ARO, s3

BA

3.0

3.0

3.0

1.0

ASR 1.1

2.5

2.5

2.5

0.9

1.0

2.0

2.0

2.0

1.5

1.5

1.5

0.8

0.9

0.0 0.1 0.2 0.3

0.0 0.1 0.2 0.3

0.0 0.1 0.2 0.3

0.0 0.1 0.2 0.3

0.0 0.1 0.2 0.3

(d) CIFAR-10, Warped

ARO, s1

ARO, s2

ARO, s3

BA

3.0

3.0

3.0

0.9

ASR 1.1

2.5

2.5

2.5

0.8

1.0

2.0

2.0

2.0

1.5

1.5

1.5

0.7

0.9

0.0 0.1 0.2 0.3

0.0 0.1 0.2 0.3

0.0 0.1 0.2 0.3

0.0 0.1 0.2 0.3

0.0 0.1 0.2 0.3

(f) CelebA, Blended

ARO, s1

ARO, s2

ARO, s3

BA

3.0

3.0

3.0

0.9

ASR 1.1

2.5

2.5

2.5

0.8

1.0

2.0

2.0

2.0

1.5

1.5

1.5

0.7

0.9

0.0 0.1 0.2 0.3

0.0 0.1 0.2 0.3

0.0 0.1 0.2 0.3

0.0 0.1 0.2 0.3

0.0 0.1 0.2 0.3

(h) CelebA, Warped

Fig. 3. Results of SL-MMDR and ML-MMDR on the {P-18} models with different λ. X-axis: the value of λ. Y-axis: the value of each indicator.

(a) RBT

(b) ML-MMDR, λ = 0.3

(c) SL-MMDR, λ = 0.3

Fig. 4. Visualization examples of the features extracted from the {CIFAR-10, P-18, Patched} models.

reduced representations into two groups. The results of AC on the {P-18} models are shown in Fig. 5.
Spectral Signatures. Following the same steps in [14], we ﬁrst take singular value decomposition of the latent representations, and then use the top right vector to compute an outlier score for each input. The results of SS on the {P-18} models are shown in Fig. 6.
Subspace Reconstruction. We choose 200 validated data and construct a subspace that can restore 90% of the energy of these samples. Subsequently, for the representation of each input, we perform the projection and reconstruction

steps with the learned subspace and compute the l2 norm as the reconstruction loss for that sample. The max F1 score for each model is calculated by adjusting the threshold, and the results are shown in Fig. 7. Summary. The experimental results of the above three defense methods are similar and are summarized as follows:
• The representations at s1 and s2 levels can be used by these methods to detect malicious samples. For example, when N = 100 and r = 1.0, the average F1 scores of AC at the three levels for RBT are 0.978, 0.991, and 0.948 on the {CIFAR-10, P-18} models, and

N = 100, r = 0.1
1

N = 100, r = 1.0

N = 500, r = 0.1

N = 500, r = 1.0

7

N = 100, r = 0.1
1

N = 100, r = 1.0

N = 500, r = 0.1

N = 500, r = 1.0

0

s1

s2

s3

s1

s2

s3

s1

s2

s3

s1

s2

s3

(a) CIFAR-10, Patched

N = 100, r = 0.1
1

N = 100, r = 1.0

N = 500, r = 0.1

N = 500, r = 1.0

0

s1

s2

s3

s1

s2

s3

s1

s2

s3

s1

s2

s3

(b) CIFAR-10, Blended

N = 100, r = 0.1
1

N = 100, r = 1.0

N = 500, r = 0.1

N = 500, r = 1.0

0

s1

s2

s3

s1

s2

s3

s1

s2

s3

s1

s2

s3

(c) CIFAR-10, SIG

N = 100, r = 0.1
1

N = 100, r = 1.0

N = 500, r = 0.1

N = 500, r = 1.0

0

s1

s2

s3

s1

s2

s3

s1

s2

s3

s1

s2

s3

(d) CIFAR-10, Warped

N = 100, r = 0.1
1

N = 100, r = 1.0

N = 500, r = 0.1

N = 500, r = 1.0

0 s1

s2

s3

s1

s2

s3

s1

s2

s3

s1

s2

s3

(e) CelebA, Patched

N = 100, r = 0.1
1

N = 100, r = 1.0

N = 500, r = 0.1

N = 500, r = 1.0

0 s1

s2

s3

s1

s2

s3

s1

s2

s3

s1

s2

s3

(f) CelebA, Blended

N = 100, r = 0.1
1

N = 100, r = 1.0

N = 500, r = 0.1

N = 500, r = 1.0

0 s1

s2

s3

s1

s2

s3

s1

s2

s3

s1

s2

s3

(g) CelebA, SIG

0 s1

s2

s3

s1

s2

s3

s1

s2

s3

s1

s2

s3

(h) CelebA, Warped

Fig. 5. F1 scores of AC on the {P-18} models. X-axis: the level of features. Y-axis: the value of F 1.

N = 100, r = 0.1
1

N = 100, r = 1.0

N = 500, r = 0.1

N = 500, r = 1.0

N = 100, r = 0.1
1

N = 100, r = 1.0

N = 500, r = 0.1

N = 500, r = 1.0

0 s1

s2

s3

s1

s2

s3

s1

s2

s3

s1

s2

s3

(a) CIFAR-10, Patched

N = 100, r = 0.1
1

N = 100, r = 1.0

N = 500, r = 0.1

N = 500, r = 1.0

0 s1

s2

s3

s1

s2

s3

s1

s2

s3

s1

s2

s3

(b) CIFAR-10, Blended

N = 100, r = 0.1
1

N = 100, r = 1.0

N = 500, r = 0.1

N = 500, r = 1.0

0 s1

s2

s3

s1

s2

s3

s1

s2

s3

s1

s2

s3

(c) CIFAR-10, SIG

N = 100, r = 0.1
1

N = 100, r = 1.0

N = 500, r = 0.1

N = 500, r = 1.0

0 s1

s2

s3

s1

s2

s3

s1

s2

s3

s1

s2

s3

(d) CIFAR-10, Warped

N = 100, r = 0.1
1

N = 100, r = 1.0

N = 500, r = 0.1

N = 500, r = 1.0

0 s1

s2

s3

s1

s2

s3

s1

s2

s3

s1

s2

s3

(e) CelebA, Patched

N = 100, r = 0.1
1

N = 100, r = 1.0

N = 500, r = 0.1

N = 500, r = 1.0

0 s1

s2

s3

s1

s2

s3

s1

s2

s3

s1

s2

s3

(f) CelebA, Blended

N = 100, r = 0.1
1

N = 100, r = 1.0

N = 500, r = 0.1

N = 500, r = 1.0

0

s1

s2

s3

s1

s2

s3

s1

s2

s3

s1

s2

s3

(g) CelebA, SIG

0

s1

s2

s3

s1

s2

s3

s1

s2

s3

s1

s2

s3

(h) CelebA, Warped

Fig. 6. F1 scores of SS on the {P-18} models. X-axis: the level of features. Y-axis: the value of F 1.

N = 100, r = 0.1
1

N = 100, r = 1.0

N = 500, r = 0.1

N = 500, r = 1.0

8

N = 100, r = 0.1
1

N = 100, r = 1.0

N = 500, r = 0.1

N = 500, r = 1.0

0

s1

s2

s3

s1

s2

s3

s1

s2

s3

s1

s2

s3

(a) CIFAR-10, Patched

N = 100, r = 0.1
1

N = 100, r = 1.0

N = 500, r = 0.1

N = 500, r = 1.0

0

s1

s2

s3

s1

s2

s3

s1

s2

s3

s1

s2

s3

(b) CIFAR-10, Blended

N = 100, r = 0.1
1

N = 100, r = 1.0

N = 500, r = 0.1

N = 500, r = 1.0

0

s1

s2

s3

s1

s2

s3

s1

s2

s3

s1

s2

s3

(c) CIFAR-10, SIG

N = 100, r = 0.1
1

N = 100, r = 1.0

N = 500, r = 0.1

N = 500, r = 1.0

0

s1

s2

s3

s1

s2

s3

s1

s2

s3

s1

s2

s3

(d) CIFAR-10, Warped

N = 100, r = 0.1
1

N = 100, r = 1.0

N = 500, r = 0.1

N = 500, r = 1.0

0 s1

s2

s3

s1

s2

s3

s1

s2

s3

s1

s2

s3

(e) CelebA, Patched

N = 100, r = 0.1
1

N = 100, r = 1.0

N = 500, r = 0.1

N = 500, r = 1.0

0 s1

s2

s3

s1

s2

s3

s1

s2

s3

s1

s2

s3

(f) CelebA, Blended

N = 100, r = 0.1
1

N = 100, r = 1.0

N = 500, r = 0.1

N = 500, r = 1.0

0 s1

s2

s3

s1

s2

s3

s1

s2

s3

s1

s2

s3

(g) CelebA, SIG

0 s1

s2

s3

s1

s2

s3

s1

s2

s3

s1

s2

s3

(h) CelebA, Warped

Fig. 7. F1 scores of SR on the {P-18} models. X-axis: the level of features. Y-axis: the value of F 1.

0.898, 0.963, and 0.939 on the {CelebA, P-18} models. This conﬁrms the conclusion in Section 4. • The performance of difference-based detection methods is dropped on the infected models trained with ML-MMDR. For example, when N = 500 and r = 1.0, the average F1 values of AC at the three levels for ML-MMDR drop from 0.986, 0.997, 0.979 to 0.598, 0.546, 0.645 on the {CIFAR-10, P-18} models, and from 0.965, 0.975, 0.988 to 0.630, 0.618, 0.613 on the {CelebA, P-18} models. The results of SS and SR are similar. • The models trained with SL-MMDR can still be utilized by these defense methods to detect malicious samples. For example, when N = 500 and r = 1.0, the average F1 scores of AC, SS, and SR at s0 level for SL-MMDR are 0.962, 0.985, and 0.867 on the {CIFAR10, P-18} models, and 0.977, 0.998, and 0.856 on the {CelebA, P-18} models. Sometimes, the values are higher than on the models trained with RBT. This indicates that constraining only the activations of the last hidden layer, which is adopted in the previous reduction methods [21], [22], [23], is not enough for bypassing backdoor detection algorithms.
6.2 Results of NC
NC consists of two steps, i.e., trigger synthesis and neuron pruning, to complete the detection and mitigation of the backdoor. We conduct experiments on the above two steps separately.

Trigger Synthesis. Wang et al. [32] deﬁned a generic form of backdoor sample generation with a 3D trigger pattern and a 2D location mask. Then they formulated an optimization problem to reverse a pattern and a location for each category. The ﬁnal synthetic trigger is selected by calculating the l1 norm of all candidate masks. Because the form deﬁned above is mainly used to reverse the patching-based attack [16], we conduct experiments on the {Patched} models. The results are shown in Fig. 8, and some of the reversed triggers on the {CIFAR-10, Patched} models are shown in Fig. 9.
As see, whether it is from the distinguishability of the l1 norm or from the intuitive feeling, the infected models trained with ML-MMDR can still be used to reverse the triggers. This indicates that the trigger synthesis relies on a different principle from the difference-based methods to defend against backdoor attacks. However, this method seems to be easily affected by the dataset, such as the performance on the {CelebA, Patched} models is not as good as on the {CIFAR-10, Patched} models. Neuron Pruning. After obtaining the reversed trigger, Wang et al. [32] used it to patch the infected model by neuron pruning. To rigorously test the proposed method, we use the real trigger instead of the reversed one for pruning. The results are shown in Fig. 10.
It can be seen that the curves of BA and ASR are affected by ML-MMDR. For example, the two curves almost overlap on the {CIFAR-10, V-11, Warped} models, showing similar downward trends. On the {CelebA} models, in most cases, BA and ASR quickly drop to low values simultaneously. The results indicate that the performance of the neuron

40 30 20 10 V-11V, -R11B, TSDVR-M11-,ASDRMV-L-16V, -R16B, TSDVR-M16-,ASDRMR-L-18R, -R18B, TSDRR-M18-,ASDRMP-L-18P, -R18B, TSDPR-M18-,ASDRM-L
(a) CIFAR-10

9
20 18 16 14 12 V-11V, -R11B, TSDVR-M11-,ASDRMV-L-16V, -R16B, TSDVR-M16-,ASDRMR-L-18R, -R18B, TSDRR-M18-,ASDRMP-L-18P, -R18B, TSDPR-M18-,ASDRM-L
(b) CelebA

Fig. 8. l1 norm of triggers for infected and uninfected labels on the {Patched} models. X-axis: the model architecture and the backdoor training method. Y-axis: the l1 norm of trigger.

Original Trigger

V-11

Reversed Triggers

V-16

R-18

P-18 RBT

ML-MMDR

duct experiments on different kernels, including Gaussian Kernel (GK), Gaussian Mixture Kernel (GMK), and Linear Kernel (LK). The detailed settings are shown in TABLE 3. The results of three difference-based defense methods are shown in Fig. 11.
TABLE 3 Settings of different kernels. σ: the standard deviation.

SL-MMDR
Fig. 9. Reversed triggers on the {CIFAR-10, Patched} models.
pruning is reduced on the models trained with ML-MMDR compared to on the models trained with RBT. We think these results are reasonable because neuron pruning essentially assumes that the backdoor behavior is encoded into some neurons, which is similar to the assumption of the distributional differences.
Interestingly, we ﬁnd that in some cases, ASR declines at the beginning and rises later as the pruning ratio increases, such as on the {CelebA, R-18, Patched} model trained with RBT. We believe there are two possible reasons:
• Neuro pruning preferentially masks the neurons with large activation values on malicious samples, but this does not mean that the model would not classify the input into the target category t, especially when the pruning ratio is large.
• Backdoor attacks may leave a bias in the infected model during training. This bias would play a dominant role when the model’s functionality is greatly broken, causing the model to classify arbitrary inputs into the backdoor target t.
6.3 Ablation Study on Different Kernels Considering that the choice of a kernel in MMD may have an impact on the training of the backdoored model, we con-

Kernel
GK1 GK2 GK3 GMK1 GMK2 GMK3 GMK4 GMK5 GMK6 LK

Parameter
σ = 1/2 σ=1 σ=2 σ = [1/2, 1, 2] σ = [1/4, 1/2, 1, 2, 4] σ = [1/8, 1/4, 1/2, 1, 2, 4, 8] σ = [1/3, 1, 3] σ = [1/9, 1/3, 1, 3, 9] σ = [1/27, 1/9, 1/3, 1, 3, 9, 27] -

Two observations can be seen from the ﬁgure. First of all, no matter which kernel is used, ML-MMDR signiﬁcantly reduces the detection effects. Second, GK or GMK is a better choice than LK. We believe that the reason is that these two types of kernels map the original dimensions to inﬁnite dimensions, which makes the measurement of MMD more accurate.

7 DISCUSSION
One of the trends of backdoor attacks is to be more concealed, and this concealment is reﬂected in two aspects. On the one hand, the attacker hopes that the constructed malicious samples can evade human perception, so some researchers have proposed the invisible attacks [39], [40] and the label-consistent attack [41]. On the other hand, the attack method needs to be machine imperceptible, meaning that it needs to escape various backdoor defense methods. Some methods have been proposed to achieve this goal, including new trigger forms [29] or backdoor training approaches [21]. Our work also belongs to one of these. However, it

V-11
1

V-16

R-18

P-18

V-11
1

V-16

R-18

10 P-18

0

0.0

0.2 0.0

0.2 0.0

0.2 0.0

0.2

(a) CIFAR-10, Patched

V-11
1

V-16

R-18

P-18

0

0.0

0.2 0.0

0.2 0.0

0.2 0.0

0.2

(b) CIFAR-10, Blended

V-11

V-16

R-18

P-18

1

0

0.0

0.2 0.0

0.2 0.0

0.2 0.0

0.2

(c) CIFAR-10, SIG

V-11
1

V-16

R-18

P-18

0

0.0

0.2 0.0

0.2 0.0

0.2 0.0

0.2

(d) CIFAR-10, Warped

V-11

V-16

R-18

P-18

1

0

0.0

0.2 0.0

0.2 0.0

0.2 0.0

0.2

(e) CelebA, Patched

V-11
1

V-16

R-18

P-18

0

0.0

0.2 0.0

0.2 0.0

0.2 0.0

0.2

(f) CelebA, Blended

V-11

V-16

R-18

P-18

1

0

0

0.0

0.2 0.0

0.2 0.0

0.2 0.0

0.2

0.0

0.2 0.0

0.2 0.0

0.2 0.0

0.2

(g) CelebA, SIG

(h) CelebA, Warped

Fig. 10. BA and ASR of the infected models when pruning the trigger-related neurons. X-axis: the ratio of neurons pruned. Y-axis: the benign accuracy and the attack success rate.

N = 100, r = 0.1
1.0

N = 100, r = 1.0

N = 500, r = 0.1

N = 500, r = 1.0

0.5

0.0

s1

s2

s3

s1

s2

s3

s1

s2

s3

(a) AC

s1

s2

s3

N = 100, r = 0.1
1.0

N = 100, r = 1.0

N = 500, r = 0.1

N = 500, r = 1.0

0.5

0.0

s1

s2

s3

s1

s2

s3

s1

s2

s3

(b) SS

s1

s2

s3

N = 100, r = 0.1
1.0

N = 100, r = 1.0

N = 500, r = 0.1

N = 500, r = 1.0

0.5

0.0 s1

s2

s3

s1

s2

s3

s1

s2

s3

(c) SR

s1

s2

s3

Fig. 11. F1 scores of AC, SS, SR on the {CIFAR-10, P-18, Patched} models with different kernels.

is not sufﬁcient to consider machine perception or human perception alone, because the defender may set up multiple defense methods. Constructing an attack that can bypass defense methods with different principles requires further research.
The proposed method in this paper reduces the distributional differences by adding a constraint to the loss function during the training of a backdoored model. It requires the attacker to have permission to train the model. Another more relaxed setting is that the attacker only provides data, and

the user trains the model on the data. Can the distributional differences still be reduced in this case? We argue the answer is positive. However, by combining the optimized triggers [42] and MMD, we make a preliminary attempt and ﬁnd it is not easy. We optimize a trigger on a trained benign model, so that adding the trigger to a bengin sample can not only force the model’s output into a speciﬁed category, but also minimize the distributional differences of the latent representations. Subsequently, we use this optimized trigger to construct a poisoning training set and train an infected

11

model on these data with RBT. The new model can always optimize parameters that are easier for completing the task, but are less concealed. We plan it in future work.
8 RELATED WORK
8.1 Backdoor Attacks
Since Gu et al. [16] ﬁrst explored the backdoor vulnerability in deep learning models, many variants have been proposed. From the method of injecting the hidden threat, backdoor attacks can be roughly divided into two categories, i.e., poisoning-based attacks and non-poisoning-based attacks. Poisoning-based methods [15], [16], [17], [29], [40], [41], [43] execute the Trojan horse implantation by mixing a small number of malicious samples into the training data and learning an infected model from the mixed dataset. Constructing more concealed and effective malicious samples is the research focus of this type of attack. The primary method [16] uses a static local patch as the trigger. To obtain better attack performance, Liu et al. [17] built a Trojan method, in which the trigger is optimized rather than predeﬁned. Zhong et al. [39] argued that the triggers of the previous attacks [16], [17] are all visually visible, which undermines the concealment of the backdoor. Then, they proposed adding an imperceptible perturbation mask to the bengin image to generate its backdoor adversary. However, these attacks ignore that the inconsistency of the instance and its label can increase the risk of disclosure. Turner et al. [41] ﬁrst pointed out this problem and proposed the label-consistent backdoor attacks by leveraging adversarial examples and generative models. In addition to the above digital attacks, some studies focus on the physical world. Chen et al. [15] adopted a pair of glasses as the physical trigger to fool a face recognition system. Li et al. [44] suggested that physical transformations should be considered when training a backdoored model.
Non-poisoning-based methods [45], [46], [47], [48], [49] encode the backdoor functionality into deep models by transfer learning or weight perturbations. Dumford and Scheirer [45] ﬁrst explored the possibility of injecting the backdoor without poisoning, where they proposed to modify the model’s parameters directly. Unlike [45], Tang et al. [48] proposed a training-free method by inserting a malicious module into the target model instead of perturbing the parameters.
The method we proposed in this paper can be used to enhance the above attacks to escape difference-based backdoor detection. As shown in 6, ML-MMDR adds a constraint item to the loss function without affecting the regular backdoor training.
8.2 Backdoor Defenses
Several defense approaches have been proposed to defend against these attacks, such as backdoor detection [13], [50], trigger synthesis [32], [51], model reconstruction [42], [52], and model diagnosis [53], [54]. Among them, a vast number of defense methods [13], [14], [19], [20], [55], [56], [57] distinguish backdoor samples from the clean ones by exploiting the distributional differences between clean and adversarial representations. Three typical methods have been introduced above, we here add some others. Chou et al. [56]

leveraged the power of visualization tools, such as GradCAM [58], to inspect the backdoor behavior of malicious models. Soremekun et al. [57] proposed a detection method similar to the activation clustering [13], which uses t-SNE as the dimensionality reduction and the mean-shift as the clustering algorithm. Hayase et al. [20] argued that the previous defense [14] works only when the spectral signature is large and proposed a method, SPECTRE, using robust covariance estimation to amplify the signature of malicious data.
However, our work demonstrates that the performance of difference-based detection methods can be greatly reduced when using infected models trained with MLMMDR. This proves that there is a need to study more powerful defense methods.
9 CONCLUSION
Our work studies the distributional differences between the features of clean and adversarial inputs in the infected model, which is the hypothesis of many detection methods. We identify that the distributional differences of multiple levels are all large enough to be used to distinguish inputs. Therefore, we propose a multi-level MMD reduction method and demonstrate that the differences can be considerably reduced without harming the attack intensity. Finally, the experimental results of three difference-based defense methods, i.e., the activation clustering, the spectral signatures, and the subspace reconstruction, indicate that the defense effects decrease drastically as the differences reduce. The proposed method can enhance existing attacks to escape backdoor detection algorithms.
ACKNOWLEDGMENT
The work is partially supported by the National Natural Science Foundation of China under grant No.U19B2044 and No.61836011.
REFERENCES
[1] R. Girshick, “Fast r-cnn,” in Proceedings of the IEEE international conference on computer vision, 2015, pp. 1440–1448.
[2] J. Long, E. Shelhamer, and T. Darrell, “Fully convolutional networks for semantic segmentation,” in Proceedings of the IEEE conference on computer vision and pattern recognition, 2015, pp. 3431– 3440.
[3] J. Redmon and A. Farhadi, “Yolov3: An incremental improvement,” arXiv preprint arXiv:1804.02767, 2018.
[4] J. Yin, P. Xia, and J. He, “Online hard region mining for semantic segmentation,” Neural Processing Letters, vol. 50, no. 3, pp. 2665– 2679, 2019.
[5] P. Xia, J. He, and J. Yin, “Boosting image caption generation with feature fusion module,” Multimedia Tools and Applications, vol. 79, no. 33, pp. 24 225–24 239, 2020.
[6] I. Sutskever, O. Vinyals, and Q. V. Le, “Sequence to sequence learning with neural networks,” arXiv preprint arXiv:1409.3215, 2014.
[7] Q. Chen, X. Zhu, Z. Ling, S. Wei, H. Jiang, and D. Inkpen, “Enhanced lstm for natural language inference,” arXiv preprint arXiv:1609.06038, 2016.
[8] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, “Bert: Pretraining of deep bidirectional transformers for language understanding,” arXiv preprint arXiv:1810.04805, 2018.
[9] W. Chan, N. Jaitly, Q. Le, and O. Vinyals, “Listen, attend and spell: A neural network for large vocabulary conversational speech recognition,” in 2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2016, pp. 4960–4964.

12

[10] D. Silver, A. Huang, C. J. Maddison, A. Guez, L. Sifre, G. Van Den Driessche, J. Schrittwieser, I. Antonoglou, V. Panneershelvam, M. Lanctot et al., “Mastering the game of go with deep neural networks and tree search,” nature, vol. 529, no. 7587, pp. 484–489, 2016.
[11] J. Jumper, R. Evans, A. Pritzel, T. Green, M. Figurnov, O. Ronneberger, K. Tunyasuvunakool, R. Bates, A. Zˇ ´ıdek, A. Potapenko et al., “Highly accurate protein structure prediction with alphafold,” Nature, vol. 596, no. 7873, pp. 583–589, 2021.
[12] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell et al., “Language models are few-shot learners,” Advances in neural information processing systems, vol. 33, pp. 1877–1901, 2020.
[13] B. Chen, W. Carvalho, N. Baracaldo, H. Ludwig, B. Edwards, T. Lee, I. Molloy, and B. Srivastava, “Detecting backdoor attacks on deep neural networks by activation clustering,” in SafeAI@ AAAI, 2019.
[14] B. Tran, J. Li, and A. Madry, “Spectral signatures in backdoor attacks,” Advances in neural information processing systems, vol. 31, 2018.
[15] X. Chen, C. Liu, B. Li, K. Lu, and D. Song, “Targeted backdoor attacks on deep learning systems using data poisoning,” arXiv preprint arXiv:1712.05526, 2017.
[16] T. Gu, B. Dolan-Gavitt, and S. Garg, “Badnets: Identifying vulnerabilities in the machine learning model supply chain,” arXiv preprint arXiv:1708.06733, 2017.
[17] Y. Liu, S. Ma, Y. Aafer, W.-C. Lee, J. Zhai, W. Wang, and X. Zhang, “Trojaning attack on neural networks,” 2017.
[18] M. Xue, C. He, J. Wang, and W. Liu, “One-to-n & n-to-one: Two advanced backdoor attacks against deep learning models,” IEEE Transactions on Dependable and Secure Computing, 2020.
[19] K. Jin, T. Zhang, C. Shen, Y. Chen, M. Fan, C. Lin, and T. Liu, “A uniﬁed framework for analyzing and detecting malicious examples of dnn models,” arXiv preprint arXiv:2006.14871, 2020.
[20] J. Hayase, W. Kong, R. Somani, and S. Oh, “Spectre: Defending against backdoor attacks using robust statistics,” arXiv preprint arXiv:2104.11315, 2021.
[21] T. J. L. Tan and R. Shokri, “Bypassing backdoor detection algorithms in deep learning,” in 2020 IEEE European Symposium on Security and Privacy (EuroS&P). IEEE, 2020, pp. 175–183.
[22] K. Doan, Y. Lao, and P. Li, “Backdoor attack with imperceptible input and latent modiﬁcation,” Advances in Neural Information Processing Systems, vol. 34, 2021.
[23] Y. Ren, L. Li, and J. Zhou, “Simtrojan: Stealthy backdoor attack,” in 2021 IEEE International Conference on Image Processing (ICIP). IEEE, 2021, pp. 819–823.
[24] A. Gretton, K. M. Borgwardt, M. J. Rasch, B. Scho¨ lkopf, and A. Smola, “A kernel two-sample test,” The Journal of Machine Learning Research, vol. 13, no. 1, pp. 723–773, 2012.
[25] G. J. Sze´kely and M. L. Rizzo, “Energy statistics: A class of statistics based on distances,” Journal of statistical planning and inference, vol. 143, no. 8, pp. 1249–1272, 2013.
[26] S. Kolouri, K. Nadjahi, U. Simsekli, R. Badeau, and G. Rohde, “Generalized sliced wasserstein distances,” Advances in Neural Information Processing Systems, vol. 32, 2019.
[27] M. Barni, K. Kallas, and B. Tondi, “A new backdoor attack in cnns by training set corruption without label poisoning,” in 2019 IEEE International Conference on Image Processing (ICIP). IEEE, 2019, pp. 101–105.
[28] C. Villani, Optimal transport: old and new. Springer, 2009, vol. 338.
[29] A. Nguyen and A. Tran, “Wanet–imperceptible warping-based backdoor attack,” arXiv preprint arXiv:2102.10369, 2021.
[30] Z. Liu, P. Luo, X. Wang, and X. Tang, “Deep learning face attributes in the wild,” in Proceedings of the IEEE international conference on computer vision, 2015, pp. 3730–3738.
[31] M. Javaheripi, M. Samragh, G. Fields, T. Javidi, and F. Koushanfar, “Cleann: Accelerated trojan shield for embedded neural networks,” in 2020 IEEE/ACM International Conference On Computer Aided Design (ICCAD). IEEE, 2020, pp. 1–9.
[32] B. Wang, Y. Yao, S. Shan, H. Li, B. Viswanath, H. Zheng, and B. Y. Zhao, “Neural cleanse: Identifying and mitigating backdoor attacks in neural networks,” in 2019 IEEE Symposium on Security and Privacy (SP). IEEE, 2019, pp. 707–723.
[33] A. Krizhevsky, G. Hinton et al., “Learning multiple layers of features from tiny images,” 2009.

[34] A. Salem, R. Wen, M. Backes, S. Ma, and Y. Zhang, “Dynamic backdoor attacks against machine learning models,” arXiv preprint arXiv:2003.03675, 2020.
[35] K. Simonyan and A. Zisserman, “Very deep convolutional networks for large-scale image recognition,” arXiv preprint arXiv:1409.1556, 2014.
[36] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image recognition,” in Proceedings of the IEEE conference on computer vision and pattern recognition, 2016, pp. 770–778.
[37] ——, “Identity mappings in deep residual networks,” in European conference on computer vision. Springer, 2016, pp. 630–645.
[38] A. Paszke, S. Gross, S. Chintala, G. Chanan, E. Yang, Z. DeVito, Z. Lin, A. Desmaison, L. Antiga, and A. Lerer, “Automatic differentiation in pytorch,” 2017.
[39] H. Zhong, C. Liao, A. C. Squicciarini, S. Zhu, and D. Miller, “Backdoor embedding in convolutional neural network models via invisible perturbation,” in Proceedings of the Tenth ACM Conference on Data and Application Security and Privacy, 2020, pp. 97–108.
[40] S. Li, M. Xue, B. Z. H. Zhao, H. Zhu, and X. Zhang, “Invisible backdoor attacks on deep neural networks via steganography and regularization,” IEEE Transactions on Dependable and Secure Computing, vol. 18, no. 5, pp. 2088–2105, 2020.
[41] A. Turner, D. Tsipras, and A. Madry, “Label-consistent backdoor attacks,” arXiv preprint arXiv:1912.02771, 2019.
[42] Y. Liu, Y. Xie, and A. Srivastava, “Neural trojans,” in 2017 IEEE International Conference on Computer Design (ICCD). IEEE, 2017, pp. 45–48.
[43] Y. Liu, X. Ma, J. Bailey, and F. Lu, “Reﬂection backdoor: A natural backdoor attack on deep neural networks,” in European Conference on Computer Vision. Springer, 2020, pp. 182–199.
[44] Y. Li, T. Zhai, B. Wu, Y. Jiang, Z. Li, and S. Xia, “Rethinking the trigger of backdoor attack,” arXiv preprint arXiv:2004.04692, 2020.
[45] J. Dumford and W. Scheirer, “Backdooring convolutional neural networks via targeted weight perturbations,” in 2020 IEEE International Joint Conference on Biometrics (IJCB). IEEE, 2018, pp. 1–9.
[46] K. Kurita, P. Michel, and G. Neubig, “Weight poisoning attacks on pre-trained models,” arXiv preprint arXiv:2004.06660, 2020.
[47] A. S. Rakin, Z. He, and D. Fan, “Tbt: Targeted neural network attack with bit trojan,” in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2020, pp. 13 198–13 207.
[48] R. Tang, M. Du, N. Liu, F. Yang, and X. Hu, “An embarrassingly simple approach for trojan attack in deep neural networks,” in Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, 2020, pp. 218–228.
[49] S. Wang, S. Nepal, C. Rudolph, M. Grobler, S. Chen, and T. Chen, “Backdoor attacks against transfer learning with pre-trained deep learning models,” IEEE Transactions on Services Computing, 2020.
[50] M. Ficco, “Malware analysis by combining multiple detectors and observation windows,” IEEE Transactions on Computers, 2021.
[51] H. Chen, C. Fu, J. Zhao, and F. Koushanfar, “Deepinspect: A blackbox trojan detection and mitigation framework for deep neural networks.” in IJCAI, 2019, pp. 4658–4664.
[52] Y. Li, X. Lyu, N. Koren, L. Lyu, B. Li, and X. Ma, “Neural attention distillation: Erasing backdoor triggers from deep neural networks,” arXiv preprint arXiv:2101.05930, 2021.
[53] X. Xu, Q. Wang, H. Li, N. Borisov, C. A. Gunter, and B. Li, “Detecting ai trojans using meta neural analysis,” arXiv preprint arXiv:1910.03137, 2019.
[54] S. Kolouri, A. Saha, H. Pirsiavash, and H. Hoffmann, “Universal litmus patterns: Revealing backdoor attacks in cnns,” in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2020, pp. 301–310.
[55] K. Liu, B. Dolan-Gavitt, and S. Garg, “Fine-pruning: Defending against backdooring attacks on deep neural networks,” in International Symposium on Research in Attacks, Intrusions, and Defenses. Springer, 2018, pp. 273–294.
[56] E. Chou, F. Trame`r, and G. Pellegrino, “Sentinet: Detecting localized universal attacks against deep learning systems,” in 2020 IEEE Security and Privacy Workshops (SPW). IEEE, 2020, pp. 48–54.
[57] E. Soremekun, S. Udeshi, S. Chattopadhyay, and A. Zeller, “Exposing backdoors in robust machine learning models,” arXiv preprint arXiv:2003.00865, 2020.
[58] R. R. Selvaraju, M. Cogswell, A. Das, R. Vedantam, D. Parikh, and D. Batra, “Grad-cam: Visual explanations from deep networks via gradient-based localization,” in Proceedings of the IEEE international conference on computer vision, 2017, pp. 618–626.

APPENDIX A DNN ARCHITECTURES
The details of the DNN architectures and the selected layers are shown in TABLE 1. For V-11 and V-16, we always choose the ReLU layers to extract the latent representations. For R18 or P-18, we use the RBB or PBB blocks to extract the latent representations.

13
APPENDIX B QUANTIFICATION OF THE DISTRIBUTIONAL DIFFER-
ENCES
MMD, ED, and SWD are used to quantify the distributional differences between the latent representations of benign and malicious samples at the three levels. The speciﬁc distance values on CIFAR-10 and CelebA are shown in TABLE 2 and TABLE 3, respectively.

TABLE 1 DNN architectures used in this paper. Conv: a convolutional layer. BN:
a batch normalization layer. ReLU: a rectiﬁed linear activation layer. MP: a max-pooling layer. GAP: a global average pooling layer. FC: a fully connected layer. RBB: a basic block for ResNet. PBB: a basic block for PreActResNet. The underline indicates that the outputs of that
layer or block are used as the latent representations of inputs.

APPENDIX C RESULTS OF ML-MMDR
The results of SL-MMDR and ML-MMDR on the {V-11}, {V16}, and {R-18} models with different λ are shown in Fig. 1, Fig. 2, and Fig. 3, respectively.

Layer
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53

V-11
Conv BN ReLU MP Conv BN ReLU MP Conv BN ReLU Conv BN ReLU MP Conv BN ReLU Conv BN ReLU MP Conv BN ReLU Conv BN ReLU GAP FC

V-16
Conv BN ReLU Conv BN ReLU MP Conv BN ReLU Conv BN ReLU MP Conv BN ReLU Conv BN ReLU Conv BN ReLU MP Conv BN ReLU Conv BN ReLU Conv BN ReLU MP Conv BN ReLU Conv BN ReLU Conv BN ReLU GAP FC

R-18 Conv BN ReLU RBB
RBB
RBB
RBB
RBB
RBB
RBB
RBB GAP FC

P-18 Conv BN ReLU PBB
PBB
PBB
PBB
PBB
PBB
PBB
PBB GAP FC

APPENDIX D PERFORMANCE OF AC, SS, AND SR
The F1 scores of AC on the {V-11}, {V-16}, and {R-18} models are shown in Fig. 4, Fig. 5, and Fig. 6, respectively. The F1 scores of SS on the {V-11}, {V-16}, and {R-18} models are shown in Fig. 7, Fig. 8, and Fig. 9, respectively. The F1 scores of SR on the {V-11}, {V-16}, and {R-18} models are shown in Fig. 10, Fig. 11, and Fig. 12, respectively.

14

TABLE 2 MMD, ED, and SWD values on CIFAR-10 between the latent representations of benign and malicious samples at the three levels. All values are
averaged over sets of 500 inputs sampled randomly from the particular data.

Model Method

MMD2

s1

s2

s3

Patched

0.41

0.84

2.88

V-11 Intra-class

0.02

0.02

0.01

Inter-class

0.18

0.44

2.47

Patched

0.59

1.86

3.27

V-16 Intra-class

0.01

0.01

0.01

Inter-class

0.27

0.91

3.95

Patched

0.29

0.58

1.85

R-18 Intra-class

0.02

0.01

0.01

Inter-class

0.16

0.25

1.06

Patched

0.27

0.38

1.11

R-18 Intra-class

0.02

0.01

0.01

Inter-class

0.16

0.29

2.06

Blended

0.26

0.90

3.02

V-11 Intra-class

0.01

0.02

0.01

Inter-class

0.19

0.43

2.46

Blended

0.41

1.64

3.86

V-16 Intra-class

0.01

0.02

0.01

Inter-class

0.24

1.01

4.19

Blended

0.28

0.31

1.09

R-18 Intra-class

0.01

0.01

0.01

Inter-class

0.19

0.26

1.12

Blended

0.28

0.39

2.54

P-18 Intra-class

0.01

0.01

0.01

Inter-class

0.19

0.32

2.41

SIG

0.29

0.92

3.33

V-11 Intra-class

0.02

0.02

0.01

Inter-class

0.22

0.43

2.55

SIG

0.56

1.87

3.49

V-16 Intra-class

0.01

0.01

0.01

Inter-class

0.24

1.00

4.21

SIG

0.37

0.45

1.52

R-18 Intra-class

0.01

0.01

0.02

Inter-class

0.19

0.27

1.20

SIG

0.33

0.50

2.22

P-18 Intra-class

0.01

0.01

0.01

Inter-class

0.17

0.31

2.60

Warped

0.77

1.49

3.65

V-11 Intra-class

0.01

0.01

0.01

Inter-class

0.22

0.43

2.54

Warped

1.21

1.72

3.98

V-16 Intra-class

0.01

0.02

0.01

Inter-class

0.27

0.93

3.96

Warped

0.46

0.59

1.03

R-18 Intra-class

0.01

0.02

0.02

Inter-class

0.17

0.28

1.21

Warped

0.42

0.55

1.93

P-18 Intra-class

0.01

0.02

0.01

Inter-class

0.20

0.31

2.42

s1
778.82 11.17 280.59
1286.90 10.38 521.48
4898.08 110.74 2407.12
3541.53 92.33 1794.53
445.24 10.67 322.78
798.74 10.80 453.56
4251.33 90.34 2835.14
3402.49 79.71 2214.61
503.06 11.21 352.16
1219.64 10.99 460.41
5910.28 94.78 2777.41
4421.40 71.52 2017.53
1659.84 9.80 341.63
3114.98 9.19 443.33
7807.47 79.31 2535.64
5818.77 72.00 2346.60

ED2 s2
973.58 6.45 460.08
2627.58 3.44
1017.54
4594.22 34.85 1726.27
1819.97 25.61 1292.70
994.06 5.26 429.84
1958.26 5.30
1156.00
2112.81 44.57 1802.27
1823.00 27.83 1548.64
1075.74 6.48 436.90
2656.29 4.38
1121.27
3190.55 38.04 1872.69
2556.99 26.03 1549.07
2215.77 3.95 380.34
2334.75 4.45 979.75
4156.52 43.34 1908.70
2655.92 34.00 1477.92

s3
606.35 1.00 622.32
384.35 0.28 861.37
7626.06 19.56 3794.22
3507.75 8.83
7961.95
573.97 0.91 631.29
445.55 1.15 887.87
3429.00 13.39 4079.09
12990.96 14.88
10264.65
719.73 0.98 657.50
308.19 0.56 867.11
4969.67 20.07 4622.49
10010.73 7.74
12497.88
720.80 1.07 664.04
439.79 0.78 799.54
2668.61 21.76 4732.28
6626.69 15.78 10500.92

s1
167.14 21.19 180.37
318.80 11.32 361.89
467.06 41.87 480.62
136.28 77.79 472.66
142.29 17.43 191.69
194.21 12.86 285.58
191.57 66.76 1135.98
708.26 34.93 309.86
172.31 13.37 202.55
483.49 12.49 310.03
774.35 113.43 1185.90
281.72 44.37 48.41
1160.18 17.51 210.73
2312.96 9.65 340.94
503.87 50.00 703.90
230.71 50.25 155.89

SWD1 s2
271.81 7.86 208.98
679.74 10.84 375.39
475.38 15.68 312.22
36.06 24.00 154.34
228.55 6.90 239.53
431.69 19.20 473.22
138.60 15.31 356.98
106.44 21.38 141.03
356.56 9.17 217.84
769.85 7.35 394.09
147.18 24.77 229.39
277.23 13.68 205.11
1037.38 5.25 203.33
927.90 12.80 368.47
125.95 27.71 435.46
319.87 32.63 88.97

s3
130.11 3.13 32.67
111.44 3.39 47.60
87.39 26.11 109.04
260.36 8.85 274.56
129.32 4.43 29.38
141.87 3.28 65.75
50.17 33.18 109.47
272.66 7.07 34.96
112.83 3.37 22.85
116.86 4.18 38.28
132.84 39.25 44.91
75.30 6.89 374.96
152.64 2.68 33.79
141.02 1.79 56.23
609.97 23.54 60.81
52.46 9.80 292.53

15

TABLE 3 MMD, ED, and SWD values on CelebA between the latent representations of benign and malicious samples at the three levels. All values are
averaged over sets of 500 inputs sampled randomly from the particular data.

Model Method

MMD2

s1

s2

s3

Patched

0.58

2.25

3.04

V-11 Intra-class

0.02

0.02

0.02

Inter-class

0.09

0.54

1.47

Patched

1.32

3.54

3.31

V-16 Intra-class

0.02

0.02

0.03

Inter-class

0.24

1.33

1.63

Patched

0.27

1.26

3.61

R-18 Intra-class

0.02

0.02

0.02

Inter-class

0.08

0.40

1.18

Patched

0.29

1.49

2.98

P-18 Intra-class

0.02

0.02

0.01

Inter-class

0.08

0.43

1.35

Blended

0.25

1.81

3.59

V-11 Intra-class

0.02

0.02

0.02

Inter-class

0.09

0.61

1.32

Blended

0.80

3.49

3.96

V-16 Intra-class

0.02

0.01

0.01

Inter-class

0.24

1.16

1.55

Blended

0.20

0.74

3.91

R-18 Intra-class

0.02

0.02

0.02

Inter-class

0.09

0.35

1.30

Blended

0.22

1.04

4.65

P-18 Intra-class

0.02

0.02

0.01

Inter-class

0.07

0.40

1.47

SIG

0.28

2.13

3.74

V-11 Intra-class

0.02

0.02

0.01

Inter-class

0.09

0.58

1.37

SIG

1.14

4.02

3.32

V-16 Intra-class

0.02

0.02

0.02

Inter-class

0.25

1.47

1.60

SIG

0.27

1.15

4.30

R-18 Intra-class

0.02

0.02

0.01

Inter-class

0.08

0.38

1.19

SIG

0.27

1.33

4.33

P-18 Intra-class

0.02

0.02

0.03

Inter-class

0.07

0.43

1.46

Warped

0.82

2.72

4.10

V-11 Intra-class

0.02

0.02

0.01

Inter-class

0.09

0.58

1.30

Warped

1.93

4.29

4.13

V-16 Intra-class

0.02

0.02

0.01

Inter-class

0.23

1.19

1.68

Warped

0.41

1.64

4.75

R-18 Intra-class

0.02

0.02

0.02

Inter-class

0.09

0.36

1.19

Warped

0.51

2.68

4.82

P-18 Intra-class

0.02

0.02

0.02

Inter-class

0.08

0.44

1.57

s1
932.95 9.64 117.69
2540.82 13.26 314.71
3117.11 83.23 794.15
1395.52 31.60 306.56
352.43 15.71 122.77
1279.55 10.05 323.90
2128.99 104.46 888.29
971.04 35.05 297.17
420.24 9.69 116.33
1986.21 9.30 314.24
2942.51 89.73 782.21
1380.08 34.52 302.50
1383.38 9.58 120.40
3259.40 8.17 297.12
4863.88 89.28 845.40
2687.21 37.62 313.12

ED2 s2
2066.36 2.59 275.41
3320.80 3.57 516.73
7926.51 31.93 1783.98
3931.52 13.52 732.31
1332.43 2.47 320.88
2406.84 1.33 397.31
4083.16 28.57 1635.75
2226.99 9.81 684.72
1876.40 2.15 297.75
3170.06 2.33 464.11
7034.58 39.39 1712.78
3530.54 15.87 801.63
2240.92 2.80 297.94
2034.82 1.50 445.75
10773.26 30.22 1642.50
9386.98 10.47 760.52

s3
888.92 1.18 190.44
806.02 1.40 226.58
19089.21 8.35
1881.09
10782.47 3.61
1853.09
1123.55 0.93 173.47
912.84 0.28 212.21
21899.05 9.34
2082.18
27127.98 1.63
1883.19
1398.77 0.16 182.32
625.41 0.88 230.23
31957.46 4.98
1790.68
26558.91 14.26 1681.01
1138.15 0.31 168.84
555.86 0.39 255.00
27248.32 12.61 1870.43
20266.48 4.46
1873.21

s1
169.11 15.27 8.92
450.21 9.77 44.44
371.52 20.14 29.32
196.65 23.65 84.68
131.30 19.18 9.76
255.41 13.99 25.64
126.18 67.40 61.34
93.00 33.99 31.88
81.00 9.24 9.71
493.58 10.58 27.35
320.26 35.29 48.84
243.13 48.92 68.84
630.73 17.76 11.43
743.10 8.53 18.21
415.60 38.00 31.34
259.45 44.28 48.90

SWD1 s2
293.66 12.72 52.58
460.55 17.00 112.95
809.13 26.54 141.38
335.78 21.54 87.12
150.84 7.81 46.22
315.77 8.63 117.19
463.06 28.77 118.59
265.71 8.50 64.16
336.93 8.82 40.21
468.55 11.34 121.42
766.79 31.60 200.99
480.76 10.75 37.40
357.68 11.73 30.95
117.84 8.18 54.99
1075.88 23.76 144.72
545.32 8.19 58.20

s3
205.00 8.68 22.93
207.84 12.25 26.27
3412.75 34.68 43.78
121.51 9.48 22.68
247.53 8.74 15.29
203.79 5.08 16.69
4165.82 27.17 29.75
410.14 5.28 52.16
305.82 3.33 16.81
149.46 7.65 23.92
5457.98 33.97 69.90
485.03 5.45 108.35
241.28 4.04 6.32
114.63 5.87 8.92
4853.27 67.28 69.24
225.31 9.45 71.26

16

ARO, s1

ARO, s2

ARO, s3

BA

3.0

3.0

3.0

1.0

ASR 1.1

2.5

2.5

2.5

0.9

1.0

2.0

2.0

2.0

1.5

1.5

1.5

0.8

0.9

0.0 0.1 0.2 0.3 0.0 0.1 0.2 0.3 0.0 0.1 0.2 0.3 0.0 0.1 0.2 0.3 0.0 0.1 0.2 0.3

(a) CIFAR-10, Patched

ARO, s1

ARO, s2

ARO, s3

BA

3.0

3.0

3.0

1.0

ASR 1.1

2.5

2.5

2.5

0.9

1.0

2.0

2.0

2.0

1.5

1.5

1.5

0.8

0.9

0.0 0.1 0.2 0.3 0.0 0.1 0.2 0.3 0.0 0.1 0.2 0.3 0.0 0.1 0.2 0.3 0.0 0.1 0.2 0.3

(c) CIFAR-10, SIG

ARO, s1

ARO, s2

ARO, s3

BA

3.0

3.0

3.0

0.9

ASR 1.1

2.5

2.5

2.5

0.8

1.0

2.0

2.0

2.0

1.5

1.5

1.5

0.7

0.9

0.0 0.1 0.2 0.3 0.0 0.1 0.2 0.3 0.0 0.1 0.2 0.3 0.0 0.1 0.2 0.3 0.0 0.1 0.2 0.3

(e) CelebA, Patched

ARO, s1

ARO, s2

ARO, s3

BA

3.0

3.0

3.0

0.9

ASR 1.1

2.5

2.5

2.5

0.8

1.0

2.0

2.0

2.0

1.5

1.5

1.5

0.7

0.9

0.0 0.1 0.2 0.3 0.0 0.1 0.2 0.3 0.0 0.1 0.2 0.3 0.0 0.1 0.2 0.3 0.0 0.1 0.2 0.3

(g) CelebA, SIG

ARO, s1

ARO, s2

ARO, s3

BA

3.0

3.0

3.0

1.0

ASR 1.1

2.5

2.5

2.5

0.9

1.0

2.0

2.0

2.0

1.5

1.5

1.5

0.8

0.9

0.0 0.1 0.2 0.3

0.0 0.1 0.2 0.3

0.0 0.1 0.2 0.3

0.0 0.1 0.2 0.3

0.0 0.1 0.2 0.3

(b) CIFAR-10, Blended

ARO, s1

ARO, s2

ARO, s3

BA

3.0

3.0

3.0

1.0

ASR 1.1

2.5

2.5

2.5

0.9

1.0

2.0

2.0

2.0

1.5

1.5

1.5

0.8

0.9

0.0 0.1 0.2 0.3

0.0 0.1 0.2 0.3

0.0 0.1 0.2 0.3

0.0 0.1 0.2 0.3

0.0 0.1 0.2 0.3

(d) CIFAR-10, Warped

ARO, s1

ARO, s2

ARO, s3

BA

3.0

3.0

3.0

0.9

ASR 1.1

2.5

2.5

2.5

0.8

1.0

2.0

2.0

2.0

1.5

1.5

1.5

0.7

0.9

0.0 0.1 0.2 0.3

0.0 0.1 0.2 0.3

0.0 0.1 0.2 0.3

0.0 0.1 0.2 0.3

0.0 0.1 0.2 0.3

(f) CelebA, Blended

ARO, s1

ARO, s2

ARO, s3

BA

3.0

3.0

3.0

0.9

ASR 1.1

2.5

2.5

2.5

0.8

1.0

2.0

2.0

2.0

1.5

1.5

1.5

0.7

0.9

0.0 0.1 0.2 0.3

0.0 0.1 0.2 0.3

0.0 0.1 0.2 0.3

0.0 0.1 0.2 0.3

0.0 0.1 0.2 0.3

(h) CelebA, Warped

Fig. 1. Results of SL-MMDR and ML-MMDR on the {V-11} models with different λ. X-axis: the value of λ. Y-axis: the value of each indicator.

ARO, s1

ARO, s2

ARO, s3

BA

3.0

3.0

3.0

1.0

ASR 1.1

2.5

2.5

2.5

0.9

1.0

2.0

2.0

2.0

1.5

1.5

1.5

0.8

0.9

0.0 0.1 0.2 0.3 0.0 0.1 0.2 0.3 0.0 0.1 0.2 0.3 0.0 0.1 0.2 0.3 0.0 0.1 0.2 0.3

(a) CIFAR-10, Patched

ARO, s1

ARO, s2

ARO, s3

BA

3.0

3.0

3.0

1.0

ASR 1.1

2.5

2.5

2.5

0.9

1.0

2.0

2.0

2.0

1.5

1.5

1.5

0.8

0.9

0.0 0.1 0.2 0.3 0.0 0.1 0.2 0.3 0.0 0.1 0.2 0.3 0.0 0.1 0.2 0.3 0.0 0.1 0.2 0.3

(c) CIFAR-10, SIG

ARO, s1

ARO, s2

ARO, s3

BA

3.0

3.0

3.0

0.9

ASR 1.1

2.5

2.5

2.5

0.8

1.0

2.0

2.0

2.0

1.5

1.5

1.5

0.7

0.9

0.0 0.1 0.2 0.3 0.0 0.1 0.2 0.3 0.0 0.1 0.2 0.3 0.0 0.1 0.2 0.3 0.0 0.1 0.2 0.3

(e) CelebA, Patched

ARO, s1

ARO, s2

ARO, s3

BA

3.0

3.0

3.0

0.9

ASR 1.1

2.5

2.5

2.5

0.8

1.0

2.0

2.0

2.0

1.5

1.5

1.5

0.7

0.9

0.0 0.1 0.2 0.3 0.0 0.1 0.2 0.3 0.0 0.1 0.2 0.3 0.0 0.1 0.2 0.3 0.0 0.1 0.2 0.3

(g) CelebA, SIG

ARO, s1

ARO, s2

ARO, s3

BA

3.0

3.0

3.0

1.0

ASR 1.1

2.5

2.5

2.5

0.9

1.0

2.0

2.0

2.0

1.5

1.5

1.5

0.8

0.9

0.0 0.1 0.2 0.3

0.0 0.1 0.2 0.3

0.0 0.1 0.2 0.3

0.0 0.1 0.2 0.3

0.0 0.1 0.2 0.3

(b) CIFAR-10, Blended

ARO, s1

ARO, s2

ARO, s3

BA

3.0

3.0

3.0

1.0

ASR 1.1

2.5

2.5

2.5

0.9

1.0

2.0

2.0

2.0

1.5

1.5

1.5

0.8

0.9

0.0 0.1 0.2 0.3

0.0 0.1 0.2 0.3

0.0 0.1 0.2 0.3

0.0 0.1 0.2 0.3

0.0 0.1 0.2 0.3

(d) CIFAR-10, Warped

ARO, s1

ARO, s2

ARO, s3

BA

3.0

3.0

3.0

0.9

ASR 1.1

2.5

2.5

2.5

0.8

1.0

2.0

2.0

2.0

1.5

1.5

1.5

0.7

0.9

0.0 0.1 0.2 0.3

0.0 0.1 0.2 0.3

0.0 0.1 0.2 0.3

0.0 0.1 0.2 0.3

0.0 0.1 0.2 0.3

(f) CelebA, Blended

ARO, s1

ARO, s2

ARO, s3

BA

3.0

3.0

3.0

0.9

ASR 1.1

2.5

2.5

2.5

0.8

1.0

2.0

2.0

2.0

1.5

1.5

1.5

0.7

0.9

0.0 0.1 0.2 0.3

0.0 0.1 0.2 0.3

0.0 0.1 0.2 0.3

0.0 0.1 0.2 0.3

0.0 0.1 0.2 0.3

(h) CelebA, Warped

Fig. 2. Results of SL-MMDR and ML-MMDR on the {V-16} models with different λ. X-axis: the value of λ. Y-axis: the value of each indicator.

17

ARO, s1

ARO, s2

ARO, s3

BA

3.0

3.0

3.0

1.0

ASR 1.1

2.5

2.5

2.5

0.9

1.0

2.0

2.0

2.0

1.5

1.5

1.5

0.8

0.9

0.0 0.1 0.2 0.3 0.0 0.1 0.2 0.3 0.0 0.1 0.2 0.3 0.0 0.1 0.2 0.3 0.0 0.1 0.2 0.3

(a) CIFAR-10, Patched

ARO, s1

ARO, s2

ARO, s3

BA

3.0

3.0

3.0

1.0

ASR 1.1

2.5

2.5

2.5

0.9

1.0

2.0

2.0

2.0

1.5

1.5

1.5

0.8

0.9

0.0 0.1 0.2 0.3 0.0 0.1 0.2 0.3 0.0 0.1 0.2 0.3 0.0 0.1 0.2 0.3 0.0 0.1 0.2 0.3

(c) CIFAR-10, SIG

ARO, s1

ARO, s2

ARO, s3

BA

3.0

3.0

3.0

0.9

ASR 1.1

2.5

2.5

2.5

0.8

1.0

2.0

2.0

2.0

1.5

1.5

1.5

0.7

0.9

0.0 0.1 0.2 0.3 0.0 0.1 0.2 0.3 0.0 0.1 0.2 0.3 0.0 0.1 0.2 0.3 0.0 0.1 0.2 0.3

(e) CelebA, Patched

ARO, s1

ARO, s2

ARO, s3

BA

3.0

3.0

3.0

0.9

ASR 1.1

2.5

2.5

2.5

0.8

1.0

2.0

2.0

2.0

1.5

1.5

1.5

0.7

0.9

0.0 0.1 0.2 0.3 0.0 0.1 0.2 0.3 0.0 0.1 0.2 0.3 0.0 0.1 0.2 0.3 0.0 0.1 0.2 0.3

(g) CelebA, SIG

ARO, s1

ARO, s2

ARO, s3

BA

3.0

3.0

3.0

1.0

ASR 1.1

2.5

2.5

2.5

0.9

1.0

2.0

2.0

2.0

1.5

1.5

1.5

0.8

0.9

0.0 0.1 0.2 0.3

0.0 0.1 0.2 0.3

0.0 0.1 0.2 0.3

0.0 0.1 0.2 0.3

0.0 0.1 0.2 0.3

(b) CIFAR-10, Blended

ARO, s1

ARO, s2

ARO, s3

BA

3.0

3.0

3.0

1.0

ASR 1.1

2.5

2.5

2.5

0.9

1.0

2.0

2.0

2.0

1.5

1.5

1.5

0.8

0.9

0.0 0.1 0.2 0.3

0.0 0.1 0.2 0.3

0.0 0.1 0.2 0.3

0.0 0.1 0.2 0.3

0.0 0.1 0.2 0.3

(d) CIFAR-10, Warped

ARO, s1

ARO, s2

ARO, s3

BA

3.0

3.0

3.0

0.9

ASR 1.1

2.5

2.5

2.5

0.8

1.0

2.0

2.0

2.0

1.5

1.5

1.5

0.7

0.9

0.0 0.1 0.2 0.3

0.0 0.1 0.2 0.3

0.0 0.1 0.2 0.3

0.0 0.1 0.2 0.3

0.0 0.1 0.2 0.3

(f) CelebA, Blended

ARO, s1

ARO, s2

ARO, s3

BA

3.0

3.0

3.0

0.9

ASR 1.1

2.5

2.5

2.5

0.8

1.0

2.0

2.0

2.0

1.5

1.5

1.5

0.7

0.9

0.0 0.1 0.2 0.3

0.0 0.1 0.2 0.3

0.0 0.1 0.2 0.3

0.0 0.1 0.2 0.3

0.0 0.1 0.2 0.3

(h) CelebA, Warped

Fig. 3. Results of SL-MMDR and ML-MMDR on the {R-18} models with different λ. X-axis: the value of λ. Y-axis: the value of each indicator.

N = 100, r = 0.1
1

N = 100, r = 1.0

N = 500, r = 0.1

N = 500, r = 1.0

N = 100, r = 0.1
1

N = 100, r = 1.0

N = 500, r = 0.1

N = 500, r = 1.0

0 s1

s2

s3

s1

s2

s3

s1

s2

s3

s1

s2

s3

(a) CIFAR-10, Patched

N = 100, r = 0.1
1

N = 100, r = 1.0

N = 500, r = 0.1

N = 500, r = 1.0

0 s1

s2

s3

s1

s2

s3

s1

s2

s3

s1

s2

s3

(b) CIFAR-10, Blended

N = 100, r = 0.1
1

N = 100, r = 1.0

N = 500, r = 0.1

N = 500, r = 1.0

0 s1

s2

s3

s1

s2

s3

s1

s2

s3

s1

s2

s3

(c) CIFAR-10, SIG

N = 100, r = 0.1
1

N = 100, r = 1.0

N = 500, r = 0.1

N = 500, r = 1.0

0 s1

s2

s3

s1

s2

s3

s1

s2

s3

s1

s2

s3

(d) CIFAR-10, Warped

N = 100, r = 0.1
1

N = 100, r = 1.0

N = 500, r = 0.1

N = 500, r = 1.0

0

s1

s2

s3

s1

s2

s3

s1

s2

s3

s1

s2

s3

(e) CelebA, Patched

N = 100, r = 0.1
1

N = 100, r = 1.0

N = 500, r = 0.1

N = 500, r = 1.0

0

s1

s2

s3

s1

s2

s3

s1

s2

s3

s1

s2

s3

(f) CelebA, Blended

N = 100, r = 0.1
1

N = 100, r = 1.0

N = 500, r = 0.1

N = 500, r = 1.0

0

s1

s2

s3

s1

s2

s3

s1

s2

s3

s1

s2

s3

(g) CelebA, SIG

0

s1

s2

s3

s1

s2

s3

s1

s2

s3

s1

s2

s3

(h) CelebA, Warped

Fig. 4. F1 scores of AC on the {V-11} models. X-axis: the level of features. Y-axis: the value of F 1.

18

N = 100, r = 0.1
1

N = 100, r = 1.0

N = 500, r = 0.1

N = 500, r = 1.0

N = 100, r = 0.1
1

N = 100, r = 1.0

N = 500, r = 0.1

N = 500, r = 1.0

0

s1

s2

s3

s1

s2

s3

s1

s2

s3

s1

s2

s3

(a) CIFAR-10, Patched

N = 100, r = 0.1
1

N = 100, r = 1.0

N = 500, r = 0.1

N = 500, r = 1.0

0

s1

s2

s3

s1

s2

s3

s1

s2

s3

s1

s2

s3

(b) CIFAR-10, Blended

N = 100, r = 0.1
1

N = 100, r = 1.0

N = 500, r = 0.1

N = 500, r = 1.0

0

s1

s2

s3

s1

s2

s3

s1

s2

s3

s1

s2

s3

(c) CIFAR-10, SIG

N = 100, r = 0.1
1

N = 100, r = 1.0

N = 500, r = 0.1

N = 500, r = 1.0

0

s1

s2

s3

s1

s2

s3

s1

s2

s3

s1

s2

s3

(d) CIFAR-10, Warped

N = 100, r = 0.1
1

N = 100, r = 1.0

N = 500, r = 0.1

N = 500, r = 1.0

0 s1

s2

s3

s1

s2

s3

s1

s2

s3

s1

s2

s3

(e) CelebA, Patched

N = 100, r = 0.1
1

N = 100, r = 1.0

N = 500, r = 0.1

N = 500, r = 1.0

0 s1

s2

s3

s1

s2

s3

s1

s2

s3

s1

s2

s3

(f) CelebA, Blended

N = 100, r = 0.1
1

N = 100, r = 1.0

N = 500, r = 0.1

N = 500, r = 1.0

0 s1

s2

s3

s1

s2

s3

s1

s2

s3

s1

s2

s3

(g) CelebA, SIG

0 s1

s2

s3

s1

s2

s3

s1

s2

s3

s1

s2

s3

(h) CelebA, Warped

Fig. 5. F1 scores of AC on the {V-16} models. X-axis: the level of features. Y-axis: the value of F 1.

N = 100, r = 0.1
1

N = 100, r = 1.0

N = 500, r = 0.1

N = 500, r = 1.0

N = 100, r = 0.1
1

N = 100, r = 1.0

N = 500, r = 0.1

N = 500, r = 1.0

0

s1

s2

s3

s1

s2

s3

s1

s2

s3

s1

s2

s3

(a) CIFAR-10, Patched

N = 100, r = 0.1
1

N = 100, r = 1.0

N = 500, r = 0.1

N = 500, r = 1.0

0

s1

s2

s3

s1

s2

s3

s1

s2

s3

s1

s2

s3

(b) CIFAR-10, Blended

N = 100, r = 0.1
1

N = 100, r = 1.0

N = 500, r = 0.1

N = 500, r = 1.0

0 s1

s2

s3

s1

s2

s3

s1

s2

s3

s1

s2

s3

(c) CIFAR-10, SIG

N = 100, r = 0.1
1

N = 100, r = 1.0

N = 500, r = 0.1

N = 500, r = 1.0

0 s1

s2

s3

s1

s2

s3

s1

s2

s3

s1

s2

s3

(d) CIFAR-10, Warped

N = 100, r = 0.1
1

N = 100, r = 1.0

N = 500, r = 0.1

N = 500, r = 1.0

0 s1

s2

s3

s1

s2

s3

s1

s2

s3

s1

s2

s3

(e) CelebA, Patched

N = 100, r = 0.1
1

N = 100, r = 1.0

N = 500, r = 0.1

N = 500, r = 1.0

0 s1

s2

s3

s1

s2

s3

s1

s2

s3

s1

s2

s3

(f) CelebA, Blended

N = 100, r = 0.1
1

N = 100, r = 1.0

N = 500, r = 0.1

N = 500, r = 1.0

0 s1

s2

s3

s1

s2

s3

s1

s2

s3

s1

s2

s3

(g) CelebA, SIG

0 s1

s2

s3

s1

s2

s3

s1

s2

s3

s1

s2

s3

(h) CelebA, Warped

Fig. 6. F1 scores of AC on the {R-18} models. X-axis: the level of features. Y-axis: the value of F 1.

19

N = 100, r = 0.1
1

N = 100, r = 1.0

N = 500, r = 0.1

N = 500, r = 1.0

N = 100, r = 0.1
1

N = 100, r = 1.0

N = 500, r = 0.1

N = 500, r = 1.0

0

s1

s2

s3

s1

s2

s3

s1

s2

s3

s1

s2

s3

(a) CIFAR-10, Patched

N = 100, r = 0.1
1

N = 100, r = 1.0

N = 500, r = 0.1

N = 500, r = 1.0

0

s1

s2

s3

s1

s2

s3

s1

s2

s3

s1

s2

s3

(b) CIFAR-10, Blended

N = 100, r = 0.1
1

N = 100, r = 1.0

N = 500, r = 0.1

N = 500, r = 1.0

0

s1

s2

s3

s1

s2

s3

s1

s2

s3

s1

s2

s3

(c) CIFAR-10, SIG

N = 100, r = 0.1
1

N = 100, r = 1.0

N = 500, r = 0.1

N = 500, r = 1.0

0

s1

s2

s3

s1

s2

s3

s1

s2

s3

s1

s2

s3

(d) CIFAR-10, Warped

N = 100, r = 0.1
1

N = 100, r = 1.0

N = 500, r = 0.1

N = 500, r = 1.0

0 s1

s2

s3

s1

s2

s3

s1

s2

s3

s1

s2

s3

(e) CelebA, Patched

N = 100, r = 0.1
1

N = 100, r = 1.0

N = 500, r = 0.1

N = 500, r = 1.0

0 s1

s2

s3

s1

s2

s3

s1

s2

s3

s1

s2

s3

(f) CelebA, Blended

N = 100, r = 0.1
1

N = 100, r = 1.0

N = 500, r = 0.1

N = 500, r = 1.0

0 s1

s2

s3

s1

s2

s3

s1

s2

s3

s1

s2

s3

(g) CelebA, SIG

0 s1

s2

s3

s1

s2

s3

s1

s2

s3

s1

s2

s3

(h) CelebA, Warped

Fig. 7. F1 scores of SS on the {V-11} models. X-axis: the level of features. Y-axis: the value of F 1.

N = 100, r = 0.1
1

N = 100, r = 1.0

N = 500, r = 0.1

N = 500, r = 1.0

N = 100, r = 0.1
1

N = 100, r = 1.0

N = 500, r = 0.1

N = 500, r = 1.0

0

s1

s2

s3

s1

s2

s3

s1

s2

s3

s1

s2

s3

(a) CIFAR-10, Patched

N = 100, r = 0.1
1

N = 100, r = 1.0

N = 500, r = 0.1

N = 500, r = 1.0

0

s1

s2

s3

s1

s2

s3

s1

s2

s3

s1

s2

s3

(b) CIFAR-10, Blended

N = 100, r = 0.1
1

N = 100, r = 1.0

N = 500, r = 0.1

N = 500, r = 1.0

0 s1

s2

s3

s1

s2

s3

s1

s2

s3

s1

s2

s3

(c) CIFAR-10, SIG

N = 100, r = 0.1
1

N = 100, r = 1.0

N = 500, r = 0.1

N = 500, r = 1.0

0 s1

s2

s3

s1

s2

s3

s1

s2

s3

s1

s2

s3

(d) CIFAR-10, Warped

N = 100, r = 0.1
1

N = 100, r = 1.0

N = 500, r = 0.1

N = 500, r = 1.0

0 s1

s2

s3

s1

s2

s3

s1

s2

s3

s1

s2

s3

(e) CelebA, Patched

N = 100, r = 0.1
1

N = 100, r = 1.0

N = 500, r = 0.1

N = 500, r = 1.0

0 s1

s2

s3

s1

s2

s3

s1

s2

s3

s1

s2

s3

(f) CelebA, Blended

N = 100, r = 0.1
1

N = 100, r = 1.0

N = 500, r = 0.1

N = 500, r = 1.0

0 s1

s2

s3

s1

s2

s3

s1

s2

s3

s1

s2

s3

(g) CelebA, SIG

0 s1

s2

s3

s1

s2

s3

s1

s2

s3

s1

s2

s3

(h) CelebA, Warped

Fig. 8. F1 scores of SS on the {V-16} models. X-axis: the level of features. Y-axis: the value of F 1.

20

N = 100, r = 0.1
1

N = 100, r = 1.0

N = 500, r = 0.1

N = 500, r = 1.0

N = 100, r = 0.1
1

N = 100, r = 1.0

N = 500, r = 0.1

N = 500, r = 1.0

0

s1

s2

s3

s1

s2

s3

s1

s2

s3

s1

s2

s3

(a) CIFAR-10, Patched

N = 100, r = 0.1
1

N = 100, r = 1.0

N = 500, r = 0.1

N = 500, r = 1.0

0

s1

s2

s3

s1

s2

s3

s1

s2

s3

s1

s2

s3

(b) CIFAR-10, Blended

N = 100, r = 0.1
1

N = 100, r = 1.0

N = 500, r = 0.1

N = 500, r = 1.0

0

s1

s2

s3

s1

s2

s3

s1

s2

s3

s1

s2

s3

(c) CIFAR-10, SIG

N = 100, r = 0.1
1

N = 100, r = 1.0

N = 500, r = 0.1

N = 500, r = 1.0

0

s1

s2

s3

s1

s2

s3

s1

s2

s3

s1

s2

s3

(d) CIFAR-10, Warped

N = 100, r = 0.1
1

N = 100, r = 1.0

N = 500, r = 0.1

N = 500, r = 1.0

0 s1

s2

s3

s1

s2

s3

s1

s2

s3

s1

s2

s3

(e) CelebA, Patched

N = 100, r = 0.1
1

N = 100, r = 1.0

N = 500, r = 0.1

N = 500, r = 1.0

0 s1

s2

s3

s1

s2

s3

s1

s2

s3

s1

s2

s3

(f) CelebA, Blended

N = 100, r = 0.1
1

N = 100, r = 1.0

N = 500, r = 0.1

N = 500, r = 1.0

0 s1

s2

s3

s1

s2

s3

s1

s2

s3

s1

s2

s3

(g) CelebA, SIG

0 s1

s2

s3

s1

s2

s3

s1

s2

s3

s1

s2

s3

(h) CelebA, Warped

Fig. 9. F1 scores of SS on the {R-18} models. X-axis: the level of features. Y-axis: the value of F 1.

N = 100, r = 0.1
1

N = 100, r = 1.0

N = 500, r = 0.1

N = 500, r = 1.0

N = 100, r = 0.1
1

N = 100, r = 1.0

N = 500, r = 0.1

N = 500, r = 1.0

0

s1

s2

s3

s1

s2

s3

s1

s2

s3

s1

s2

s3

(a) CIFAR-10, Patched

N = 100, r = 0.1
1

N = 100, r = 1.0

N = 500, r = 0.1

N = 500, r = 1.0

0

s1

s2

s3

s1

s2

s3

s1

s2

s3

s1

s2

s3

(b) CIFAR-10, Blended

N = 100, r = 0.1
1

N = 100, r = 1.0

N = 500, r = 0.1

N = 500, r = 1.0

0 s1

s2

s3

s1

s2

s3

s1

s2

s3

s1

s2

s3

(c) CIFAR-10, SIG

N = 100, r = 0.1
1

N = 100, r = 1.0

N = 500, r = 0.1

N = 500, r = 1.0

0 s1

s2

s3

s1

s2

s3

s1

s2

s3

s1

s2

s3

(d) CIFAR-10, Warped

N = 100, r = 0.1
1

N = 100, r = 1.0

N = 500, r = 0.1

N = 500, r = 1.0

0 s1

s2

s3

s1

s2

s3

s1

s2

s3

s1

s2

s3

(e) CelebA, Patched

N = 100, r = 0.1
1

N = 100, r = 1.0

N = 500, r = 0.1

N = 500, r = 1.0

0 s1

s2

s3

s1

s2

s3

s1

s2

s3

s1

s2

s3

(f) CelebA, Blended

N = 100, r = 0.1
1

N = 100, r = 1.0

N = 500, r = 0.1

N = 500, r = 1.0

0 s1

s2

s3

s1

s2

s3

s1

s2

s3

s1

s2

s3

(g) CelebA, SIG

0 s1

s2

s3

s1

s2

s3

s1

s2

s3

s1

s2

s3

(h) CelebA, Warped

Fig. 10. F1 scores of SR on the {V-11} models. X-axis: the level of features. Y-axis: the value of F 1.

21

N = 100, r = 0.1
1

N = 100, r = 1.0

N = 500, r = 0.1

N = 500, r = 1.0

N = 100, r = 0.1
1

N = 100, r = 1.0

N = 500, r = 0.1

N = 500, r = 1.0

0

s1

s2

s3

s1

s2

s3

s1

s2

s3

s1

s2

s3

(a) CIFAR-10, Patched

N = 100, r = 0.1
1

N = 100, r = 1.0

N = 500, r = 0.1

N = 500, r = 1.0

0

s1

s2

s3

s1

s2

s3

s1

s2

s3

s1

s2

s3

(b) CIFAR-10, Blended

N = 100, r = 0.1
1

N = 100, r = 1.0

N = 500, r = 0.1

N = 500, r = 1.0

0

s1

s2

s3

s1

s2

s3

s1

s2

s3

s1

s2

s3

(c) CIFAR-10, SIG

N = 100, r = 0.1
1

N = 100, r = 1.0

N = 500, r = 0.1

N = 500, r = 1.0

0

s1

s2

s3

s1

s2

s3

s1

s2

s3

s1

s2

s3

(d) CIFAR-10, Warped

N = 100, r = 0.1
1

N = 100, r = 1.0

N = 500, r = 0.1

N = 500, r = 1.0

0 s1

s2

s3

s1

s2

s3

s1

s2

s3

s1

s2

s3

(e) CelebA, Patched

N = 100, r = 0.1
1

N = 100, r = 1.0

N = 500, r = 0.1

N = 500, r = 1.0

0 s1

s2

s3

s1

s2

s3

s1

s2

s3

s1

s2

s3

(f) CelebA, Blended

N = 100, r = 0.1
1

N = 100, r = 1.0

N = 500, r = 0.1

N = 500, r = 1.0

0 s1

s2

s3

s1

s2

s3

s1

s2

s3

s1

s2

s3

(g) CelebA, SIG

0 s1

s2

s3

s1

s2

s3

s1

s2

s3

s1

s2

s3

(h) CelebA, Warped

Fig. 11. F1 scores of SR on the {V-16} models. X-axis: the level of features. Y-axis: the value of F 1.

N = 100, r = 0.1
1

N = 100, r = 1.0

N = 500, r = 0.1

N = 500, r = 1.0

N = 100, r = 0.1
1

N = 100, r = 1.0

N = 500, r = 0.1

N = 500, r = 1.0

0

s1

s2

s3

s1

s2

s3

s1

s2

s3

s1

s2

s3

(a) CIFAR-10, Patched

N = 100, r = 0.1
1

N = 100, r = 1.0

N = 500, r = 0.1

N = 500, r = 1.0

0

s1

s2

s3

s1

s2

s3

s1

s2

s3

s1

s2

s3

(b) CIFAR-10, Blended

N = 100, r = 0.1
1

N = 100, r = 1.0

N = 500, r = 0.1

N = 500, r = 1.0

0 s1

s2

s3

s1

s2

s3

s1

s2

s3

s1

s2

s3

(c) CIFAR-10, SIG

N = 100, r = 0.1
1

N = 100, r = 1.0

N = 500, r = 0.1

N = 500, r = 1.0

0 s1

s2

s3

s1

s2

s3

s1

s2

s3

s1

s2

s3

(d) CIFAR-10, Warped

N = 100, r = 0.1
1

N = 100, r = 1.0

N = 500, r = 0.1

N = 500, r = 1.0

0 s1

s2

s3

s1

s2

s3

s1

s2

s3

s1

s2

s3

(e) CelebA, Patched

N = 100, r = 0.1
1

N = 100, r = 1.0

N = 500, r = 0.1

N = 500, r = 1.0

0 s1

s2

s3

s1

s2

s3

s1

s2

s3

s1

s2

s3

(f) CelebA, Blended

N = 100, r = 0.1
1

N = 100, r = 1.0

N = 500, r = 0.1

N = 500, r = 1.0

0 s1

s2

s3

s1

s2

s3

s1

s2

s3

s1

s2

s3

(g) CelebA, SIG

0 s1

s2

s3

s1

s2

s3

s1

s2

s3

s1

s2

s3

(h) CelebA, Warped

Fig. 12. F1 scores of SR on the {R-18} models. X-axis: the level of features. Y-axis: the value of F 1.

