BACKDOOR DEFENSE VIA SUPPRESSING MODEL SHORTCUTS
Sheng Yang1 Yiming Li1 Yong Jiang1,2 Shu-Tao Xia1,2
1Tsinghua Shenzhen International Graduate School, Tsinghua University 2Research Center of Artiﬁcial Intelligence, Peng Cheng Laboratory
{yangs22, li-ym18}@mails.tsinghua.edu.cn; {jiangy, xiast}@sz.tsinghua.edu.cn

arXiv:2211.05631v2 [cs.CV] 6 Mar 2023

ABSTRACT
Recent studies have demonstrated that deep neural networks (DNNs) are vulnerable to backdoor attacks during the training process. Speciﬁcally, the adversaries intend to embed hidden backdoors in DNNs so that malicious model predictions can be activated through pre-deﬁned trigger patterns. In this paper, we explore the backdoor mechanism from the angle of the model structure. We select the skip connection for discussions, inspired by the understanding that it helps the learning of model ‘shortcuts’ where backdoor triggers are usually easier to be learned. Speciﬁcally, we demonstrate that the attack success rate (ASR) decreases signiﬁcantly when reducing the outputs of some key skip connections. Based on this observation, we design a simple yet effective backdoor removal method by suppressing the skip connections in critical layers selected by our method. We also implement ﬁne-tuning on these layers to recover high benign accuracy and to further reduce ASR. Extensive experiments on benchmark datasets verify the effectiveness of our method.
Index Terms— Backdoor Defense, Backdoor Learning, AI Security, Trustworthy ML, Deep Learning
1. INTRODUCTION
With high effectiveness and efﬁciency, deep neural networks (DNNs) are widely used in many areas, such as face recognition [1, 2, 3]. In general, training a well-performed DNN usually needs a lot of training samples and computational resources. Therefore, third-party resources (e.g., third-party training data) are usually involved in the training process.
However, recent studies demonstrated that using thirdparty resources also introduced a new security threat. This threat was called backdoor attack [4, 5, 6]. Different from adversarial attacks [7, 8, 9], the backdoor adversaries can modify some benign training samples or directly control the training loss to implant malicious model behaviors. The backdoor is a latent connection between adversary-speciﬁed trigger patterns and the target label. In the inference process, the adversaries can activate embedded backdoors with trigger patterns.
Corresponding author: Yiming Li (li-ym18@mails.tsinghua.edu.cn).

Backdoor attacks are stealthy since attacked models behave normally on predicting benign data, posing huge threats to the applications of DNNs [10].
Currently, there are many defenses to reduce backdoor threats. For example, model-repairing-based defenses [11] intended to directly remove backdoors of attacked models; Detection-based methods [12, 13, 14] identiﬁed whether the suspicious samples or models are backdoored; Preprocessing-based ones [15] perturbed input samples before feeding them into the deployed model for predictions. We noticed that almost all existing defenses treated different model structures equally. At most, they treat all models as two separate parts, including the feature extractor and the fullyconnected layers, and analyzed backdoor behaviors in the feature space. It raises an intriguing and important question: Are backdoor attacks independent of model structure?
In this paper, we explore the backdoor mechanism from the aspect of the model structure. We select one of the most successful and widely adopted structure components, i.e., the skip connection [16] for the analysis. We believe that this structure is highly correlated to backdoors since it helps the learning of ‘shortcuts’ [17] where backdoor triggers are usually easier to be learned. Speciﬁcally, we gradually reduce the outputs of skip connection (by multiplying a parameter smaller than 1) in different layers of backdoored models to study their effects. We observe that the attack success rate (ASR) drops signiﬁcantly more quickly than benign accuracy (BA), especially on critical layers. Motivated by this ﬁnding, we propose a simple yet effective backdoor defense by suppressing the skip connections in critical layers selected by our method. To reduce the side effects of shortcut suppression, we also implement ﬁne-tuning on these layers to recover high BA and to further reduce ASR.
In conclusion, our main contributions are three-fold: 1) We reveal that backdoor threats have an underlying correlation to the skip connection. To the best of our knowledge, this is the ﬁrst attempt trying to analyze backdoor mechanisms from the aspect of model structure. 2) Based on our observations, we propose a simple yet effective backdoor defense, dubbed shortcut suppression with ﬁne-tuning (SSFT). 3) We conduct extensive experiments on benchmark datasets, verifying the effectiveness of our defense method.

Accuracy (%)

BadNets-BA

BadNets-ASR

WaNet-BA

WaNet-ASR

100 Layer1.0 100 Layer1.1 100 Layer2.0 100 Layer2.1

80

80

80

80

60

60

60

60

40

40

40

40

20

20

20

20

0 0 0D.2ec0a.4y F0a.6ct0o.r8 1 0 0 0D.2ec0a.4y F0a.6ct0o.r8 1 0 0 0D.2ec0a.4y F0a.6ct0o.r8 1 0 0 0D.2ec0a.4y F0a.6ct0o.r8 1

BadNets-BA

BadNets-ASR

WaNet-BA

WaNet-ASR

100 Layer3.0 100 Layer3.1 100 Layer4.0 100 Layer4.1

80

80

80

80

60

60

60

60

40

40

40

40

20

20

20

20

0 0 0D.2ec0a.4y F0a.6ct0o.r8 1 0 0 0D.2ec0a.4y F0a.6ct0o.r8 1 0 0 0D.2ec0a.4y F0a.6ct0o.r8 1 0 0 0D.2ec0a.4y F0a.6ct0o.r8 1

Accuracy (%)

Fig. 1: The benign accuracy (BA) and attack success rate (ASR) w.r.t. the introduced decay factor γ of skip connections in different layers of models attacked by BadNets and WaNet on the CIFAR10 dataset.

2. THE EFFECTS OF SKIP CONNECTIONS
In this section, we analyze how skip connections inﬂuence backdoor attacks from both training and inference stages. We adopt ResNet-18 [16] on the CIFAR-10 dataset as an example for our discussions, as follows:
Training Stage. We introduce a decay factor γ ∈ [0, 1] to reduce gradients from the skip connections to suppress their effects. Our goal is to explore whether the suppression can alleviate the backdoor attack. The gradient suppression is conducted on all skip connections, as denoted in E.q.(1).

zi = f (zi−1) + γ · zi−1

⇒ ∂zi = ∂f (zi−1) · ∂zi−1 + γ · ∂zi−1 , (1)

∂wi−1

∂zi−1 ∂wi−1

∂wi−1

where zi is the output of i-th layer, wi is its weights, and f represents the convolution part of the layer.
However, we ﬁnd that assigning small γ has almost no inﬂuence on attack success rate (ASR) and benign accuracy (BA). It indicates that backdoors can also be embedded into structures other than skip connections. It is consistent with the fact that existing methods are effective in attacking DNNs having no skip connection (e.g., VGG [18]).
Inference Stage. Similar to the procedures in the training stage, we also introduce the decay factor γ in skip connections during the inference process. Speciﬁcally, we gradually reduce γ in each layer of models attacked by BadNets [4] and WaNet [19] to observe its inﬂuence on BA and ASR. As

shown in Figure 1, we have three key observations. Firstly, the attack success rate (ASR) may drop more quickly than benign accuracy (BA), especially on critical layers (e.g., layer 2.1&3.1). Secondly, critical layers do not necessarily contain the ﬁrst or the last layer that was typically used in existing defenses [20, 21, 22]. Thirdly, different attacks correlate to similar critical layers, which are most probably related to the model structure and training data.
3. THE PROPOSED METHOD
3.1. Preliminaries
Threat Model. In this paper, we consider the scenarios that defenders (i.e., model users) obtain a third-party model that could be backdoored. They have no information about the attack while having a few local benign samples. Defender’s Goals. Defenders intend to have low attack success rate on poisoned testing samples while having high benign accuracy on predicting benign testing samples.
3.2. Shortcut Suppression with Fine-Tuning (SSFT)
Overview. In general, our SSFT consists of three main stages (as shown in Figure 2), including 1) selecting critical layer(s) 2) suppressing shortcut(s), and 3) ﬁne-tuning critical layer(s). Selecting Critical Layer. As demonstrated in the previous section, different attacks tend to adopt similar critical layers for backdoor injection. Motivated by this observation, we propose to adopt a surrogate poisoned dataset to select the

Table 1: The main results (%) on the CIFAR-10 and the GTSRB dataset. We mark failed cases (ASR> 80% or BA < 80%) in red and the best result among all defenses in boldface. Underlined values are the second-best results.

Dataset→ Attack→ Metric→ Defense↓ No Defense
FT-FC FT-All MCR NAD SSFT SSFT*

BadNets

BA ASR

92.02 92.08 91.66 84.00 90.66 89.08 90.04

96.96 97.41 96.31 0.80 0.94 0.31 0.37

CIFAR-10 Blended Label-consistent

BA ASR BA ASR

90.89 91.34 91.76 82.51 90.00 88.64 89.22

83.11 83.17 0.81 0.71 0.86 0.31 0.32

92.17 92.42 91.80 84.13 90.20 89.86 90.26

100 100 86.79 6.15 17.72 4.63 6.44

WaNet

BA ASR

91.48 91.26 90.98 87.77 90.09 88.76 88.80

96.82 98.26 62.18 14.28 11.34 0.40 1.38

BadNets

BA ASR

98.25 98.62 99.78 98.50 96.91 99.62 99.68

95.38 95.63 22.11 0.16 0.46 0.33 0.82

GTSRB Blended Label-consistent

BA ASR BA ASR

98.65 91.34 99.62 98.38 97.09 99.46 99.45

87.48 83.17 2.74 0.11 0.21 0.24 0.41

96.83 97.62 99.68 95.85 96.58 99.60 99.60

66.40 77.19 57.22 0.03 0.00 6.00 6.06

1 SSFT: shortcut suppression with ﬁne-tuning under the standard mode (i.e., the decay factor γ is set to 0). 2 SSFT*: shortcut suppression with ﬁne-tuning under the optimized mode (i.e., with optimal decay factor).

WaNet

BA ASR

96.37 97.35 99.60 96.78 97.12 99.43 99.44

77.53 67.93 11.32 0.00 37.88 9.69 12.83

γ∙𝒙

4. EXPERIMENTS

softmax
maxpool conv

layer1 layer2 Frozen Parameters

block1 block1

Layer3.0

Layer3.1

(Critical Layer)

Layer3

layer4
Frozen Parameters

Fine-tuning
Fig. 2: The main pipeline of our method. In the ﬁrst stage, we select the critical layer based on surrogate poisoned dataset. After that, we suppress its skip connection by multiplying a small decay factor. We ﬁne-tune the critical layer at the end.

critical layer. Speciﬁcally, we retain the obtained third-party model with the surrogate dataset, based on which to select the layer by gradually decreasing the introduced decay factor of the skip connection in each layer. Layers having the sharpest ASR decreases are our critical layers.
Shortcut Suppression. Once critical layers are selected, we suppress its skip connection of the given third-party suspicious model by using a small decay factor γ during the inference process. Speciﬁcally, we introduce two suppression modes, including standard mode and optimized mode. In the standard mode, we simply assign γ as 0, i.e., remove the skip connection. We adopt the optimal γ selected in the process of critical layer selection in our optimal mode. We notice that our standard mode is different from using models without skip connections since we only remove the skip connection contained in the critical layer (instead of in all layers).
Fine-tuning Critical Layer. Shortcut suppression can signiﬁcantly reduce attack success rate. However, as shown in Figure 1, this process also has negative effects on benign accuracy (BA). Especially under the standard mode, the BA may even drop to 0. This phenomenon is expected since the prediction of benign samples also relies heavily on the skip connection. Accordingly, we propose to ﬁne-tune selected critical layer(s) based on local benign samples. This stage can signiﬁcantly increase BA and further reduce ASR.

4.1. Settings
Dataset and Model. We conduct experiments on two classical benchmark datasets, including CIFAR-10 and GTSRB datasets, with ResNet-18 [16].
Attack Baselines. We choose BadNets [4], backdoor attack with blended strategy (dubbed ‘Blended’) [23], labelconsistent attack with adversarial perturbations (dubbed ‘Label-Consistent’) [24], and WaNet [19] as our attack baselines. They are the representatives of patch-based visible attacks and patch-based invisible attacks, clean-label attacks, and non-patch-based invisible attacks, respectively.
Defense Baselines. We compare our methods with two stateof-the-art model-repairing-based backdoor defenses, including neural attention distillation (NAD) [25] and mode connectivity repair (MCR) [26]. We also adopt ﬁne-tuning on fullyconnected layers (dubbed ‘FT-FC’), all layers (dubbed ‘FTAll’), and standard model training (dubbed ‘No Defense’) as other important baselines for our discussions.
Attack Setup. For BadNets and Blended, we set a 3×3 blackwhite patch as the trigger pattern on both CIFAR-10 and GTSRB. We implement label-consistent attack and WaNet based on the BackdoorBox [27] with their default settings. We set the poisoning rate as 5% for all attacks to generate the poisoned datasets. The target label is set to 1 for all attacks.
Defense Setup. In all ﬁne-tuning-based methods, we set the learning rate as 0.01 and use 10% benign samples. We ﬁne-tune the model 10 epochs in total. For NAD and MCR, we also use 10% benign training samples which are also exploited in our defense. All baseline defenses are also implemented based on the BackdoorBox.
Evaluation Metric. Following the classical settings used in existing backdoor defenses, we adopt the attack success rate (ASR) and benign accuracy (BA) to measure the effectiveness of all methods. In general, the lower the ASR and the higher the BA, the better the defense.

Table 2: The ablation study (%) of our method.

Dataset→ Attack→ Metric→ Defense↓ No Defense
SS FT-Critical
SSFT

BadNets

BA ASR

92.02 14.30 91.82 89.08

96.96 0.00 96.73 0.31

CIFAR-10 Blended Label-consistent

BA ASR BA ASR

90.89 21.57 91.74 88.64

83.11 0.00 55.86 0.31

92.17 18.65 92.12 89.86

100 0.00 99.30 4.63

WaNet

BA ASR

91.48 12.78 91.06 88.76

96.82 0.00 93.46 0.40

BadNets

BA ASR

98.25 97.61 98.32 99.62

95.38 0.65 95.10 0.33

GTSRB Blended Label-consistent

BA ASR BA ASR

98.65 87.99 99.47 99.46

87.48 25.13 59.68 0.24

96.83 88.44 99.53 99.60

66.40 6.39 88.42 6.00

WaNet

BA ASR

96.37 22.41 99.23 99.43

77.53 0.00 45.09 9.69

Accuracy (%) Accuracy (%) Accuracy (%)

80 60 40 20 0 0.02

BA

ASR

0.06 De0c.1ay0 Facto0r.14 0.18

Fig. 3: Effects of the decay factors.

80

60

40

BA

ASR

20

00

2000 S4a0m0p0le Siz6e000 8000

Fig. 4: Effects of the sample size.

80

60

40

BA

ASR

20

0 0 2 4 6 E8poc1h0s 12 14 16 18

Fig. 5: Effects of the tuning epoch.

4.2. Main Results
As shown in Table 1, both FT-FC and FT-All fail in many cases with a high attack success rate (ASR), although they usually preserve the highest benign accuracy (BA). In contrast, both MCR and NAD reach relatively low ASR values in general, whereas having low BA values. Different from previous baseline methods, our SSFT yields a more balanced performance. In other words, our defense signiﬁcantly reduces ASR while preserving high BA. We notice that SSFT may also have better performance compared to SSFT*. It is mostly because the optimal decay factor is selected based on surrogate poisoned samples which may not be consistent with those used by the adversaries. We will further explore how to better select the decay factor in our future work.
4.3. Discussion
Ablation Study. There are two key strategies in our SSFT defense, including shortcut suppression and model ﬁne-tuning. In this part, we discuss their effects. As shown in Table 2, shortcut suppression (SS) signiﬁcantly reduces the attack success rate (ASR). However, it also results in low benign accuracy (BA). Fine-tuning the critical layer preserves high BA while having minor effects in decreasing ASR. In contrast, our SSFT is effective in maintaining BA and removing hidden backdoors. These results verify that both of these strategies are indispensable parts of our method.
Effects of the Decay Factor. In this part, we change the decay factor γ used in our SSFT while preserving other settings. As is shown in Figure 3, both benign accuracy and attack success rate are stable when using relatively small γ (i.e., < 0.1). In other words, our method is not very sensitive to the selection of γ to some extent. However, we need to notice that using a large decay factor will reduce defense effectiveness as it will degenerate our SSFT back to the model ﬁne-tuning.

Effects of the Sample Size. Recall that our SSFT method needs some benign local samples for ﬁne-tuning. Here we discuss how the sample size inﬂuences our defense. As shown in Figure 4, the BA increases while the ASR decreases with the increase in sample size. In particular, the SSFT could achieve promising performance even with a small sample size. These results reﬂect the effectiveness of our defense. Effects of the Tuning Epoch. Here we explore the inﬂuence of tuning epoch on our SSFT defense. As shown in Figure 5, similar to the effects of the sample size, the BA increases while the ASR decreases with the increase in the epoch. Our method can achieve good performance with a few epochs (e.g., 4), showing high efﬁciency.
5. CONCLUSIONS
In this paper, we explored the backdoor mechanism from the aspect of model structures. We revealed that backdoor threats have an underlying correlation to the skip connection. Specifically, we demonstrated that suppressing the skip connection in critical layers can signiﬁcantly reduce the attack success rate. Based on these ﬁndings, we propose a simple yet effective backdoor defense, dubbed shortcut suppression with ﬁne-tuning (SSFT), to remove hidden backdoors contained in the given suspicious model. Extensive experiments on benchmark datasets veriﬁed the effectiveness of our defense.
Acknowledgement
This work is supported in part by the National Natural Science Foundation of China under Grant 62771248, Shenzhen Science and Technology Program (JCYJ20220818101012025), the PCNL KEY project (PCL2021A07), and Research Center for Computer Network (Shenzhen) Ministry of Education.

6. REFERENCES
[1] Xiaoou Tang and Zhifeng Li, “Video based face recognition using multiple classiﬁers,” in IEEE FG, 2004.
[2] Hao Wang, Yitong Wang, Zheng Zhou, Xing Ji, Dihong Gong, Jingchao Zhou, Zhifeng Li, and Wei Liu, “Cosface: Large margin cosine loss for deep face recognition,” in CVPR, 2018.
[3] Haibo Qiu, Dihong Gong, Zhifeng Li, Wei Liu, and Dacheng Tao, “End2end occluded face recognition by masking corrupted features,” IEEE Transactions on Pattern Analysis and Machine Intelligence, 2021.
[4] Tianyu Gu, Kang Liu, Brendan Dolan-Gavitt, and Siddharth Garg, “Badnets: Evaluating backdooring attacks on deep neural networks,” IEEE Access, vol. 7, pp. 47230–47244, 2019.
[5] Yiming Li, Haoxiang Zhong, Xingjun Ma, Yong Jiang, and Shu-Tao Xia, “Few-shot backdoor attacks on visual object tracking,” in ICLR, 2022.
[6] Xiangyu Qi, Tinghao Xie, Yiming Li, Saeed Mahloujifar, and Prateek Mittal, “Revisiting the assumption of latent separability for backdoor defenses,” in ICLR, 2023.
[7] Jiawang Bai, Bin Chen, Yiming Li, Dongxian Wu, Weiwei Guo, Shu-tao Xia, and En-hui Yang, “Targeted attack for deep hashing based retrieval,” in ECCV, 2020.
[8] Xinwei Liu, Jian Liu, Yang Bai, Jindong Gu, Tao Chen, Xiaojun Jia, and Xiaochun Cao, “Watermark vaccine: Adversarial attacks to prevent watermark removal,” in ECCV, 2022.
[9] Jindong Gu, Hengshuang Zhao, Volker Tresp, and Philip HS Torr, “Segpgd: An effective and efﬁcient adversarial attack for evaluating and boosting segmentation robustness,” in ECCV, 2022.
[10] Yiming Li, Yong Jiang, Zhifeng Li, and Shu-Tao Xia, “Backdoor learning: A survey,” IEEE Transactions on Neural Networks and Learning Systems, 2022.
[11] Kang Liu, Brendan Dolan-Gavitt, and Siddharth Garg, “Fine-pruning: Defending against backdooring attacks on deep neural networks,” in RAID, 2018.
[12] Xi Li, Zhen Xiang, David J Miller, and George Kesidis, “Test-time detection of backdoor triggers for poisoned deep neural networks,” in ICASSP, 2022.
[13] Junfeng Guo, Ang Li, and Cong Liu, “Aeva: Blackbox backdoor detection using adversarial extreme value analysis,” in ICLR, 2022.

[14] Junfeng Guo, Yiming Li, Xun Chen, Hanqing Guo, Lichao Sun, and Cong Liu, “Scale-up: An efﬁcient black-box input-level backdoor detection via analyzing scaled prediction consistency,” in ICLR, 2023.
[15] Yiming Li, Tongqing Zhai, Yong Jiang, Zhifeng Li, and Shu-Tao Xia, “Backdoor attack in the physical world,” in ICLR Workshop, 2021.
[16] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun, “Deep residual learning for image recognition,” in CVPR, 2016.
[17] Dongxian Wu, Yisen Wang, Shu-Tao Xia, James Bailey, and Xingjun Ma, “Skip connections matter: On the transferability of adversarial examples generated with resnets,” in ICLR, 2020.
[18] Karen Simonyan and Andrew Zisserman, “Very deep convolutional networks for large-scale image recognition,” in ICLR, 2015.
[19] Anh Nguyen and Anh Tran, “Wanet–imperceptible warping-based backdoor attack,” in ICLR, 2021.
[20] Brandon Tran, Jerry Li, and Aleksander Madry, “Spectral signatures in backdoor attacks,” in NeurIPS, 2018.
[21] Jonathan Hayase and Weihao Kong, “Spectre: Defending against backdoor attacks using robust covariance estimation,” in ICML, 2021.
[22] Kunzhe Huang, Yiming Li, Baoyuan Wu, Zhan Qin, and Kui Ren, “Backdoor defense via decoupling the training process,” in ICLR, 2022.
[23] Xinyun Chen, Chang Liu, Bo Li, Kimberly Lu, and Dawn Song, “Targeted backdoor attacks on deep learning systems using data poisoning,” arXiv preprint arXiv:1712.05526, 2017.
[24] Alexander Turner, Dimitris Tsipras, and Aleksander Madry, “Label-consistent backdoor attacks,” arXiv preprint arXiv:1912.02771, 2019.
[25] Yige Li, Xixiang Lyu, Nodens Koren, Lingjuan Lyu, Bo Li, and Xingjun Ma, “Neural attention distillation: Erasing backdoor triggers from deep neural networks,” in ICLR, 2020.
[26] Pu Zhao, Pin-Yu Chen, Payel Das, Karthikeyan Natesan Ramamurthy, and Xue Lin, “Bridging mode connectivity in loss landscapes and adversarial robustness,” in ICLR, 2020.
[27] Yiming Li, Mengxi Ya, Yang Bai, Yong Jiang, and ShuTao Xia, “BackdoorBox: A python toolbox for backdoor learning,” in ICLR Workshop, 2023.

