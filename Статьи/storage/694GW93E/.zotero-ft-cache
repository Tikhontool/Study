arXiv:2306.16697v1 [cs.AI] 29 Jun 2023

Neural Polarizer: A Lightweight and Effective Backdoor Defense via Purifying Poisoned Features
Mingli Zhu1,2∗ Shaokui Wei1,2∗ Hongyuan Zha1 Baoyuan Wu1,2† 1School of Data Science, The Chinese University of Hong Kong, Shenzhen
(CUHK-Shenzhen), China 2Shenzhen Research Institute of Big Data
Abstract
Recent studies have demonstrated the susceptibility of deep neural networks to backdoor attacks. Given a backdoored model, its prediction of a poisoned sample with trigger will be dominated by the trigger information, though trigger information and benign information coexist. Inspired by the mechanism of the optical polarizer that a polarizer could pass light waves with particular polarizations while filtering light waves with other polarizations, we propose a novel backdoor defense method by inserting a learnable neural polarizer into the backdoored model as an intermediate layer, in order to purify the poisoned sample via filtering trigger information while maintaining benign information. The neural polarizer is instantiated as one lightweight linear transformation layer, which is learned through solving a well designed bi-level optimization problem, based on a limited clean dataset. Compared to other fine-tuning-based defense methods which often adjust all parameters of the backdoored model, the proposed method only needs to learn one additional layer, such that it is more efficient and requires less clean data. Extensive experiments demonstrate the effectiveness and efficiency of our method in removing backdoors across various neural network architectures and datasets, especially in the case of very limited clean data.
1 Introduction
Several studies have revealed the vulnerabilities of deep neural networks (DNNs) to various types of attacks [8, 14, 16, 26], of which backdoor attacks [7, 9, 34] are attracting increasing attention. In backdoor attacks, the adversary could produce a backdoored DNN model through manipulating the training dataset [6, 22] or the training process [19, 28], such that the backdoored model predicts any poisoned sample with particular triggers to the predetermined target label, while behaves normally on benign samples. Backdoor attacks can arise from various sources, such as training based on a poisoned dataset, or utilizing third-party platforms for model training, or downloading backdoored models from untrusted third-party providers. These scenarios significantly elevate the threat of backdoor attacks to DNNs’ applications, and meanwhile highlight the importance of defending against backdoor attacks.
Several seminal backdoor defense methods have been developed, mainly including 1) in-training approaches, which aim to train a secure model based on a poisoned dataset through well designed training algorithms or objective functions, such as DBD [13] and D-ST [5]. 2) post-training approaches, which aim to mitigate the backdoor effect from a backdoored model through adjusting the model parameters (e.g., fine-tuning or pruning), usually based on a limited subset of clean training
∗These authors contributed equally to this work. †Corresponds to Baoyuan Wu (wubaoyuan@cuhk.edu.cn).
Preprint. Under review.

Poisoned Benign Poisoned Benign Shared Shared

Backdoored Model (fixed)

Cat

…

…

Cat

Optical Polarizer

Cat

…

…

Dog

Neural Polarizer (learnable)

Figure 1: Left: Illustration of an optical polarizer [36]. Only light waves with specific polarizations can pass through the polarizer among the three incident light waves. Right: Defense against backdoors by integrating a trainable neural polarizer into the compromised model. The neural polarizer effectively filters out backdoor-related features, effectively eliminating the backdoor.

dataset, such as fine-pruning [21], ANP [38], or i-BAU [42]. This work focuses on the latter one. However, there are two limitations to existing post-training approaches. First, given very limited clean data, it is challenging to find a good checkpoint to simultaneously achieve backdoor mitigation and benign accuracy maintenance from the high-dimensional loss landscape of a complex model. Second, adjusting all parameters of a complex model is costly.
To tackle the above limitations, we propose a lightweight and effective post-training defense approach, which only learns one additional layer, while fixing all layers of the original backdoored model. It is inspired by the mechanism of the optical polarizer [41] that in a mixed light wave with diverse polarizations, only the light wave with some particular polarizations could pass the polarizer, while those with other polarizations are blocked (see Fig. 1-left). Correspondingly, by treating one poisoned sample as the mixture of trigger feature and benign feature, we define a neural polarizer and insert it into the backdoored model as one additional intermediate layer (see Fig. 1-right) in order to filter trigger feature and maintain benign feature, such that poisoned samples could be purified to mitigate backdoor effect, while benign samples are not significantly influenced.
In practice, to achieve an effective neural polarizer, it should be learned to weaken the correlation between the trigger and the target label while keeping the mapping from benign samples to their ground-truth labels. However, the defender only has a limited clean dataset, while neither trigger nor target label is accessible. To tackle it, we propose a bi-level optimization, where the target label is estimated by the output confidence, and the trigger is approximated by the targeted adversarial perturbation. Besides, in our experiments, the neural polarizer is implemented by a linear transformation (i.e., the combination of one 1 × 1 convolutional layer one batch normalization layer). Consequently, it can be efficiently and effectively learned with very limited clean data to achieve good defense performance, which is verified by extensive experiments on various model architectures and datasets.
Our main contributions are three-fold. 1) We propose an innovative backdoor defense approach that only learns one additional linear transformation called neural polarizer while fixing all parameters of the backdoored model, such that it just requires very low computational cost and very limited clean data. 2) A bi-level formulation and an effective learning algorithm are provided to optimize the parameter of the neural polarizer. 3) Extensive experimental results demonstrate the superiority of the proposed method on various networks and datasets, in terms of both efficiency and effectiveness.
2 Related work
Backdoor attack and defense. Traditional backdoor attacks are additive attacks that modify a small fraction of training samples by patching a pre-defined pattern and assigning them to targeted labels

2

[6, 9]. These modified samples, along with the unaffected samples, constitute a poisoned dataset [9]. The model trained with this dataset will be implanted with backdoors that predict the targeted label when triggered by the injected patterns, while maintaining normal behavior on clean samples [6, 9]. Recently, advanced attacks have considered more invisible trigger injection methods such as training an auto-encoder feature embedding or using a local transformation function [19, 28, 43]. To increase the stealthiness of attacks, clean-label attacks succeed by obfuscating image subject information and establishing a correlation between triggers and targeted labels without modifying the labels of poisoned samples [3, 29].
Backdoor defense methods can be broadly categorized into training-stage defenses and postprocessing defenses. Training-stage defenses assume access to a poisoned dataset for model training. The different behaviors between the poisoned and clean samples can be leveraged to identify suspicious instances, such as sensitivity to transformation [5] and clustering phenomenon in feature space [13]. Most defense methods belong to post-processing defenses, which assume that the defender only has access to a suspicious DNN model and a few clean samples. Therefore, they must remove the backdoor threat with limited resources. There are mainly three types of defense strategies: trigger reversion methods try to recover the most possible triggers and utilize the potential triggers to fine-tune the model [33]; pruning-based methods aim at locating the neurons that are most related to backdoors and pruning them [21, 38, 44, 45]; and fine-tuning based defenses leverage clean samples to rectify the model [18, 42]. I-BAU [42] is most closely related to our method, which formulates a minimax optimization framework to train the network with samples under universal adversarial perturbations. However, our method differs from I-BAU in that our approach does not require training the entire network, and our perturbation generation mechanism is distinct. Other methods proposed for backdoor detection include Beatrix [24], which uses Gram matrices to identify poisoned samples; and AEVA [10], which detects backdoor models by adversarial extreme value analysis. In this study, we focus on post-processing defenses and primarily compare our method with state-of-the-art post-processing defenses [18, 21, 33].
Adversarial training. Adversarial training (AT) [1, 2, 25] is an effective technique to improve the robustness of DNNs by incorporating adversarial examples during training. One of the most well-known methods is PGD-AT [25], which searches for adversarial examples by taking multiple iterative steps in the direction of maximizing the loss function. AWP [39] proposes a method to improve the robustness of models by introducing adversarial perturbations in the weights of the network. MART [35] proposes a new misclassification-aware adversarial training method by studying the impact of misclassified examples on the robustness of AT. Feature Denoising (FD) is an adversarial defense technique that further improves the robustness of adversarial training by applying denoisingbased operations. CIIDefence [11] presents a defense mechanism against adversarial attacks by searching for pixel-level perturbations around the original inputs that can cause high-confidence misclassifications. HGD [20] proposes an adversarial defense mechanism that uses a high-level representation-guided denoiser to remove perturbations from the input image. Xie et al.[40] introduce a denoising block that uses convolutions and residual connections to denoise feature maps using non-local means. In adversarial training literature, the most related work to ours is FD. However, we remark that our method differs from FD in two perspectives. Firstly, FD inserts multiple denoising layers with a residual structure to the network, which is different from ours. Given the limited training samples, multiple denoising layers will cause instability in the network. Secondly, FD primarily aims to enhance adversarial robustness through self-supervised learning, encoder-decoder reconstruction, or similar techniques [30], while we employ an explicit target label strategy to enhance the robustness against backdoors specifically.

3 Methodology

3.1 Basic settings

Notations. We consider a classification problem with K classes (K ≥ 2). Let x ∈ X ⊂ Rd be a d-dimensional input sample, and its ground truth label is denoted as y ∈ Y = {1, . . . , K}. Then, a L layers deep neural network f : X × W → RK parameterized by w ∈ W is defined as:

f (x; w) = fw(LL) ◦ fw(LL−−11) ◦ · · · ◦ fw(11)(x),

(1)

3

where fw(ll) is the function (e.g., convolution layer) with parameter wl in the lth layer with 1 ≤ l ≤ L. For simplicity, we denote f (x; w) as fw(x) or fw. Given input x, the predicted label of x is given by arg maxk fk(x; w), k = 1, . . . , K, where fk(x; w) is the logit of the kth class.
Threat model. We assume that the adversary could produce the backdoored model fw through manipulating training data or training process, such that fw performs well on benign sample x (i.e., fw(x) = y), and predicts poisoned sample x∆ = r(x, ∆) to the target label T , with ∆ indicating the trigger and r(·, ·) being the fusion function of x and ∆. Considering that the adversary may set multiple targets, we use Ti to denote the target label for xi.
Defender’s goal. Assume that the defender has access to the backdoored model fw and a limited set of benign training data, denoted as Dbn = {(xi, yi)}Ni=1. The defender’s goal is to obtain a new model fˆ based on fw and Dbn, such that the backdoor effect will be mitigated and the benign performance is maintained in fˆ.

3.2 Neural polarizer for DNNs

Neural polarizer. We propose the neural polarizer to purify poisoned sample in the feature space. Formally, it is instantiated as a lightweight linear transformation gθ, parameterized with θ. As shown in Fig. 1, gθ is inserted into the neural network fw at a specific immediate layer, to obtain a combined network fw,θ. For clarity, we denote fw,θ as fˆθ, since w is fixed. A desired neural polarizer should have the following three properties:
• Compatible with the neighboring layers: in other words, its input feature and its output activation must have the same shape. This requirement can be fulfilled through careful architectural design.
• Filtering trigger features in poisoned samples: after the neural polarizer, the trigger features should be filtered, such that the backdoor is deactivated, i.e., fˆθ(x∆) ̸= T .

Neural Polarizer

Previous

Layer

<latexit sha1_base64="MqDzBhyo7QHWlJSzd4etB6ZCgdk=">AAAB9HicbVC5TsNAEB2HK4QrQEljESElTWQjrjKChjJI5JASY60362SV9drsroMiyz9BQ0MBQrR0/AU0dPwNm6OAhCeN9PTejGbmeRGjUlnWt5FZWFxaXsmu5tbWNza38ts7dRnGApMaDlkomh6ShFFOaooqRpqRICjwGGl4/YuR3xgQIWnIr9UwIk6Aupz6FCOlJcd3kzu3n94kxX4pdfMFq2yNYc4Te0oKldLH+zHcf1bd/Fe7E+I4IFxhhqRs2VaknAQJRTEjaa4dSxIh3Edd0tKUo4BIJxkfnZoHWumYfih0cWWO1d8TCQqkHAae7gyQ6slZbyT+57Vi5Z85CeVRrAjHk0V+zEwVmqMEzA4VBCs21ARhQfWtJu4hgbDSOeV0CPbsy/Okfli2T8pHVzqNc5ggC3uwD0Ww4RQqcAlVqAGGW3iAJ3g2Bsaj8WK8TlozxnRmF/7AePsBa8CVjw==</latexit>
fw(kk)

C k <latexitsha1_base64="LynFaM6QXzAHa3W1pa3HRqT75xM=">AAACAnicbZDLSsNAFIZPvNZ6i7oSN4NFcFUSKeqy2E2XFewF2hAm00k7dDIJMxOhhOLGV3HjQhG3PoU738ZpG1Bbfxj4+M85nDl/kHCmtON8WSura+sbm4Wt4vbO7t6+fXDYUnEqCW2SmMeyE2BFORO0qZnmtJNIiqOA03Ywqk3r7XsqFYvFnR4n1IvwQLCQEayN5dvHNX+EeppFVKH6D7b9kW+XnLIzE1oGN4cS5Gr49mevH5M0okITjpXquk6ivQxLzQink2IvVTTBZIQHtGtQYLPIy2YnTNCZcfoojKV5QqOZ+3siw5FS4ygwnRHWQ7VYm5r/1bqpDq+9jIkk1VSQ+aIw5UjHaJoH6jNJieZjA5hIZv6KyBBLTLRJrWhCcBdPXobWRdm9LFduK6XqTR5HAU7gFM7BhSuoQh0a0AQCD/AEL/BqPVrP1pv1Pm9dsfKZI/gj6+MbyPSWag==</latexit>

⇥ Hk

⇥ Wk

1x1 Conv,

C k <latexitsha1_base64="zGI2D1RUz7TsEIwXCqgIyJXu8Tw=">AAAB6nicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE1GOxF48V7Qe0oWy2m3bpZhN2J0IJ/QlePCji1V/kzX/jts1BWx8MPN6bYWZekEhh0HW/ncLa+sbmVnG7tLO7t39QPjxqmTjVjDdZLGPdCajhUijeRIGSdxLNaRRI3g7G9ZnffuLaiFg94iThfkSHSoSCUbTSQ70/7pcrbtWdg6wSLycVyNHol796g5ilEVfIJDWm67kJ+hnVKJjk01IvNTyhbEyHvGupohE3fjY/dUrOrDIgYaxtKSRz9fdERiNjJlFgOyOKI7PszcT/vG6K4Y2fCZWkyBVbLApTSTAms7/JQGjOUE4soUwLeythI6opQ5tOyYbgLb+8SloXVe+qenl/Wand5nEU4QRO4Rw8uIYa3EEDmsBgCM/wCm+OdF6cd+dj0Vpw8plj+APn8wcVyI2u</latexit>

C k <latexitsha1_base64="LynFaM6QXzAHa3W1pa3HRqT75xM=">AAACAnicbZDLSsNAFIZPvNZ6i7oSN4NFcFUSKeqy2E2XFewF2hAm00k7dDIJMxOhhOLGV3HjQhG3PoU738ZpG1Bbfxj4+M85nDl/kHCmtON8WSura+sbm4Wt4vbO7t6+fXDYUnEqCW2SmMeyE2BFORO0qZnmtJNIiqOA03Ywqk3r7XsqFYvFnR4n1IvwQLCQEayN5dvHNX+EeppFVKH6D7b9kW+XnLIzE1oGN4cS5Gr49mevH5M0okITjpXquk6ivQxLzQink2IvVTTBZIQHtGtQYLPIy2YnTNCZcfoojKV5QqOZ+3siw5FS4ygwnRHWQ7VYm5r/1bqpDq+9jIkk1VSQ+aIw5UjHaJoH6jNJieZjA5hIZv6KyBBLTLRJrWhCcBdPXobWRdm9LFduK6XqTR5HAU7gFM7BhSuoQh0a0AQCD/AEL/BqPVrP1pv1Pm9dsfKZI/gj6+MbyPSWag==</latexit>

⇥ Hk

⇥ Wk

Batch Normalization

C k <latexitsha1_base64="LynFaM6QXzAHa3W1pa3HRqT75xM=">AAACAnicbZDLSsNAFIZPvNZ6i7oSN4NFcFUSKeqy2E2XFewF2hAm00k7dDIJMxOhhOLGV3HjQhG3PoU738ZpG1Bbfxj4+M85nDl/kHCmtON8WSura+sbm4Wt4vbO7t6+fXDYUnEqCW2SmMeyE2BFORO0qZnmtJNIiqOA03Ywqk3r7XsqFYvFnR4n1IvwQLCQEayN5dvHNX+EeppFVKH6D7b9kW+XnLIzE1oGN4cS5Gr49mevH5M0okITjpXquk6ivQxLzQink2IvVTTBZIQHtGtQYLPIy2YnTNCZcfoojKV5QqOZ+3siw5FS4ygwnRHWQ7VYm5r/1bqpDq+9jIkk1VSQ+aIw5UjHaJoH6jNJieZjA5hIZv6KyBBLTLRJrWhCcBdPXobWRdm9LFduK6XqTR5HAU7gFM7BhSuoQh0a0AQCD/AEL/BqPVrP1pv1Pm9dsfKZI/gj6+MbyPSWag==</latexit>

⇥ Hk

⇥ Wk

Posterior

<latexit sha1_base64="mVu+kHzbMhRh45ukmM2TipDLGIE=">AAAB/HicbVDLSsNAFL2pr1pf0S7dBItQEWoioi6LblxWsA9oY5hMJ+3QyYOZiRJC/BU3LhQRd+7d68adP+B3OH0stHrgcg/n3MvcOW7EqJCm+anlZmbn5hfyi4Wl5ZXVNX19oyHCmGNSxyELectFgjAakLqkkpFWxAnyXUaa7uB06DevCBc0DC5kEhHbR72AehQjqSRHL3pOeu2kg10ryy7Tsuo7maOXzIo5gvGXWBNSqu69vcLX83vN0T863RDHPgkkZkiItmVG0k4RlxQzkhU6sSARwgPUI21FA+QTYaej4zNjWyldwwu5qkAaI/XnRop8IRLfVZM+kn0x7Q3F/7x2LL1jO6VBFEsS4PFDXswMGRrDJIwu5QRLliiCMKfqVgP3EUdYqrwKKgRr+st/SWO/Yh1WDs5VGicwRh42YQvKYMERVOEMalAHDAncwj08aDfanfaoPY1Hc9pkpwi/oL18A563mIw=</latexit>
Layerfw(kk++11 )

Figure 2: An example of neural polarizer for a DNN.

• Preserving benign features in poisoned and benign
samples: the neural polarizer should preserve benign features, such that fˆθ performs well on both poisoned and benign samples, i.e., fˆθ(x∆) = fˆθ(x) = y.

The first property could be easily satisfied by designing neural polarizer’s architecture. For example, as illustrated in Fig. 2, given the input feature map with the shape Ck × Hk × Wk, the neural polarizer is implemented by a convolution layer (Conv) with Ck convolution filters of shape 1 × 1, followed by a Batch Normalization (BN) layer. The Conv-BN block can be seen as a linear transformation layer. To satisfy the latter two properties, θ should be learned by solving some well designed optimization,
of which the details is presented in Section 3.3.

3.3 Learning neural polarizer

Loss functions. To learn a good neural polarizer gθ, we firstly present some loss functions to encourage gθ to satisfy the latter two properties mentioned above, as follows:

• Loss for filtering trigger features in poisoned samples. Given trigger ∆ and target label T , filtering trigger features can be implemented by weakening the connection between ∆ and T with

the following loss:

Lasr(x, y, ∆, T ; θ) = − log(1 − sT (x∆; θ)),

(2)

where sT (x∆; θ) is the softmax probability of predicting x∆ to label T by fˆθ. By reducing Lasr, the attack success rate of the backdoor attack can be decreased.

4

• Loss for maintaining benign features in poisoned samples. To purify the poisoned sample such that it can be classified to the true label, we leverage the boosted cross entropy defined in [35]:

Lbce(x, y, ∆; θ) = − log(sy(x∆; θ)) − log

1

−

max
k̸=y

sk

(x∆;

θ)

.

(3)

• Loss for maintaining benign features in benign samples. To preserve benign features in benign sample, we adopt the widely used cross-entropy loss, i.e., Lbn(x, y; θ) = LCE(fˆθ(x), y).

Approximating ∆ and T . Note that as ∆ and T are inaccessible in both Lasr and Lbce, these

two losses cannot be directly optimized. Thus, we have to approximate ∆ and T . In terms of

approximating T , although some methods have been developed to detect the target class of a

backdoored model fw [10, 24], here we adopt a simple and dynamic strategy that T ≈ y′ =

arg maxk̸=y fˆk(x; θ). Then, the trigger is approximated by the targeted adversarial perturbation of fw, i.e.,

∆ ≈ δ∗ = arg min LCE fˆθ(x + δ), y′ ,

(4)

∥δ ∥p ≤ρ

where ∥ · ∥p is the Lp norm, ρ is the budget for perturbations.

Bi-level optimization. Combining the above loss functions and the approximations of ∆ and T , we introduce the following bi-level minimization problem to learn θ based on the clean data Dbn:

min
θ

1 N

N

λ1Lbn(xi, yi; θ) + λ2Lasr(xi, yi, δi∗, yi′; θ) + λ3Lbce(xi, yi, δi∗; θ),

i=1

(5)

s.t. δi∗ = arg min LCE fˆθ(xi + δi), yi′ , yi′ = arg max fˆki (xi; θ),

∥δi ∥p ≤ρ

ki ̸=yi

where λ1, λ2, λ3 > 0 are hyper-parameters to adjust the importance of each loss function.

To solve the above optimization problem, we proposed Algorithm 1, dubbed Neural Polarizer
based backdoor Defense (NPD). Specifically, NPD solves problem (5) by alternatively updating the surrogate target label y′, the perturbation δ and θ as follows:

• Inner minimization: Given parameter θ of the neural polarizer, we first estimate the target label

for sample (PGD) [25]

xi by yi′ = is employed

taorggemnaerxaktie̸=tyhiefˆpkei (rtxuir;bθa)ti.onTδhie∗nv,iathEeqt.a(r4g)e.ted

Project

Gradient

Descent

• Outer minimization: Given y′ and δ∗ for each sample in a batch, the θ can be updated by taking

one stochastic gradient descent [4] (SGD) step w.r.t. the outer minimization objective in Eq. (5).

Extension. To comprehensively evaluate the performance of NPD, we also provide two vari-
ants, with the relaxation that if the target label T is known. One is that approximating the trigger by the targeted adversarial perturbation for each benign sample in Eq. (5), i.e., ∆ ≈ δi∗ = arg min∥δi∥p≤ρ LCE (f (xi + δi), T ), dubbed NPD-TP. The other is approximating the trigger by the targeted universal adversarial perturbation (TUAP) [23] for all benign samples, dubbed NPD-TU.

4 Experiments
4.1 Implementation details
Attack settings. We evaluate the proposed method on eight famous backdoor attacks, including BadNets [9] (BadNets-A2O and BadNets-A2A refer to attacking one target label and all labels, respectively), Blended attack (Blended) [6], Input-aware dynamic backdoor attack (Input-aware)[27], Low frequency attack (LF) [43], Sample-specific backdoor attack (SSBA) [19], Trojan backdoor attack (Trojan) [22], and Warping-based poisoned networks (WaNet) [28]. We follow the default attack configuration as in BackdoorBench [37] for a fair comparison. The poisoning ratio is set to 10% in comparison with SOTA defenses. These attacks are conducted on three benchmark datasets: CIFAR-10 [15], Tiny ImageNet [17], and GTSRB [32]. We test all attacks on PreAct-ResNet18 [12] and VGG19-BN [31].

5

Algorithm 1 Neural Polarizer based Backdoor Defense (NPD)

1: Input: Training set Dbn, backdoored model fw, neural polarizer gθ, learning rate η > 0, perturbation bound ρ > 0, norm p, hyper-parameters λ1,λ2,λ3 > 0, warm-up epochs T0, training epochs T .

2: Output: Model fˆ(w, θ).

3: Initialize θ to be an identity function, fix w, and construct the composed network fˆ(w, θ).

4: Warm-up: Train fˆ(w, θ) with LCE(Dbn) for T0 epochs. 5: for t = 0, ..., T − 1 do

6: for mini-batch B = {(xi, yi)}bi=1 ⊂ Dbn do

7:

Compute {yi′}bi=1 and generate perturbations {(δi)}bi=1 with ∥δi∥p ≤ ρ and {yi′}bi=1 by

targeted PGD attack [25] via the inner minimization of Eq. (5);

8: Update θ via outer minimization of Eq. (5) by SGD.

9: end for

10: end for 11: return Model fˆ(w, θ).

Defense settings. We compare the proposed methods with six SOTA backdoor defense methods,
i.e., Fine-pruning (FP) [21], NAD [18], NC [33], ANP [38], i-BAU [42], and EP [45]. All these
defenses have access to 5% benign training samples. The training hyperparameters are adjusted
based on BackdoorBench [37]. We evaluate the proposed method under two proposed scenarios and
compare our NPD-TU, NPD-TP, and NPD with SOTA defenses. For the ablation study, we focus solely on NPD, which represents a more generalized scenario. We apply an l2 norm constraint to the adversarial perturbations, with a perturbation bound of 3 for CIFAR-10 and GTSRB datasets, and 6
for Tiny ImageNet. We train the neural polarizer for 50 epochs with batch size 128 and learning rate 0.01 on each dataset and the transformation block is inserted before the third convolution layer of the fourth layer for PreAct-ResNet18. The loss hyper-parameters λ1,λ2,λ3 are set to 1, 0.4, 0.4 for NPD, and 1, 0.1, 0.1 for NPD-TU and NPD-TP. More implementation details on SOTA attacks, defenses, and our methods can be found in Section B of supplementary materials.

Evaluation metric. In this work, we use clean ACCuracy (ACC), Attack Success Rate (ASR), and Defense Effectiveness Rating (DER) as evaluation metrics to assess the performance of different defenses. ACC represents the accuracy of clean samples while ASR measures the ratio of successfully misclassified backdoor samples to the target label. Defense Effectiveness Rating (DER ∈ [0, 1] [46]) is a comprehensive measure that considers both ACC and ASR:

DER = [max(0, ∆ASR) − max(0, ∆ACC) + 1]/2,

(6)

where ∆ASR denotes the decrease of ASR after applying defense, and ∆ACC denotes the drop in ACC following defense. Higher ACC, lower ASR and higher DER indicate better defense performance. Note that in comparison with SOTA defenses, the one achieving the best performance is highlighted in boldface, while the second-best result is indicated by underlining.

4.2 Main results
Table 1 and Table 2 showcase the defense performance of the proposed method in comparison to six SOTA defense methods on CIFAR-10 and Tiny ImageNet. The following observations can be made:
• Our methods show superior performance in terms of DER for almost all attacks compared to SOTA defenses. Conversely, FP and ANP excel in maintaining high ACC, but they struggle to eliminate backdoors in strong attacks like Blended and LF. NC’s emphasis on minimal universal adversarial perturbation renders it ineffective against sample-specific attacks and those utilizing large norm triggers. I-BAU shows similar performance in removing backdoors with an average DER of 92.42%, but it leads to a significant decrease in ACC, likely due to training the entire network by adversarial training.
• Defense performance of NPD-TU and NPD-TP are better than NPD. When the target label is known, the model only needs to find perturbations for that specific label, simplifying trigger identification and unlearning. These two methods outperform NPD, except for WaNet, which is a

6

Table 1: Comparison with the state-of-the-art defenses on CIFAR-10 dataset with 5% benign data and 10% poison ratio on PreAct-ResNet18 (%).

ATTACK
BadNets-A2O [9] BadNets-A2A [9]
Blended [6] Input-Aware [27]
LF [43] SSBA [19] Trojan [22] WaNet [28]
AVG

Backdoored ACC ASR DER
91.82 93.79 N/A 91.89 74.42 N/A 93.44 97.71 N/A 94.03 98.35 N/A 93.01 99.06 N/A 92.88 97.07 N/A 93.47 99.99 N/A 92.8 98.9 N/A
92.92 94.91 N/A

FP [21] ACC ASR DER

91.77 92.05 92.74 94.05 92.05 92.21 92.24 92.94

0.84 1.31 10.17 1.62 21.32 20.27 67.73 0.66

96.45 86.56 93.42 98.36 88.39 88.06 65.52 99.12

92.51 15.49 89.50

NAD [18] ACC ASR DER

88.82 90.73 92.25 94.08 91.72 92.15 92.18 93.07

1.96 1.61 47.64 0.92 75.47 70.77 5.77 0.73

94.42 85.82 74.44 98.72 61.15 62.78 96.46 99.08

91.88 25.61 84.13

NC [33] ACC ASR DER

90.27 89.79 93.69 93.84 93.01 92.88 91.85 92.80

1.62 1.11 99.76 10.48 99.06 97.07 51.03 98.90

95.31 85.60 50.00 93.84 50.00 50.00 73.67 50.00

92.27 57.38 68.44

ANP [38] ACC ASR DER

91.65 92.33 93.45 94.06 92.53 92.02 92.71 93.24

3.83 2.56 47.14 1.57 26.38 16.18 84.82 1.54

94.90 85.93 75.28 98.39 86.10 90.02 57.21 98.68

92.75 23.00 85.87

ATTACK
BadNets-A2O [9] BadNets-A2A [9]
Blended [6] Input-Aware [27]
LF [43] SSBA [19] Trojan [22] WaNet [28]
Avg

i-BAU [42] ACC ASR DER

87.43 89.39 89.43 89.91 88.92 86.53 89.29 90.70

4.48 1.29 26.82 0.02 11.99 2.89 0.54 0.88

92.46 85.32 83.44 97.10 91.49 93.92 97.64 97.96

88.95 6.11 92.42

EP [45] ACC ASR DER

89.80 88.72 91.94 93.68 91.97 91.67 92.32 90.47

1.26 3.00 48.22 2.88 84.73 4.33 2.49 96.52

95.26 84.12 74.00 97.56 56.64 95.76 98.18 50.02

91.32 30.43 81.44

NPD-TU (Ours) ACC ASR DER
90.81 1.44 95.67 91.66 0.82 86.68 91.88 0.03 98.06 92.01 0.14 98.10 91.42 0.01 98.73 91.61 2.46 96.67 92.59 0.04 99.54 92.18 3.24 97.52
91.77 1.02 96.37

NPD-TP (Ours) ACC ASR DER
90.90 0.62 96.12 92.54 0.04 87.19 91.33 0.83 97.38 93.24 0.00 98.78 91.92 0.08 98.94 91.82 0.83 97.59 92.19 0.00 99.36 92.57 7.47 95.60
92.06 1.23 96.41

NPD (Ours) ACC ASR DER
88.93 1.26 94.82 91.41 0.89 86.52 91.18 0.41 97.52 89.57 0.11 96.89 90.06 0.21 97.95 90.88 2.77 96.15 92.37 6.51 96.19 91.57 0.80 98.43
90.75 1.62 95.56

Table 2: Comparison with the state-of-the-art defenses on Tiny ImageNet dataset with 5% benign data and 10% poison ratio on PreAct-ResNet18 (%).

ATTACK
BadNets-A2O [9] BadNets-A2A [9]
Blended [6] Input-Aware [27]
LF [43] SSBA [19] Trojan [22] WaNet [28]
Avg

Backdoored ACC ASR DER
56.12 99.90 N/A 55.99 27.81 N/A 55.53 97.57 N/A 57.67 99.19 N/A 55.21 98.51 N/A 55.97 97.69 N/A 56.48 99.97 N/A 57.81 96.50 N/A
56.35 89.64 N/A

FP [21] ACC ASR DER

48.81 47.88 50.58 52.38 48.18 48.06 45.96 50.35

0.66 3.19 57.89 0.13 63.83 52.25 8.88 1.37

95.96 58.26 67.37 96.88 63.82 68.77 90.29 93.84

49.03 23.53 79.40

NAD [18] ACC ASR DER

48.35 48.29 55.22 57.42 49.61 47.67 48.83 50.02

0.27 2.30 98.88 0.07 58.01 69.47 1.01 0.87

95.93 58.91 49.84 99.44 67.45 59.96 95.66 93.92

50.68 28.86 77.55

NC [33] ACC ASR DER

56.12 54.12 54.50 53.46 53.08 53.30 54.43 57.81

99.90 18.72 96.07 2.48 90.48 0.26 1.54 96.50

50.00 53.61 50.24 96.25 52.95 97.38 98.19 50.00

54.60 50.74 68.58

ANP [38] ACC ASR DER

47.34 40.70 43.21 50.56 41.75 41.83 45.36 30.34

0.00 2.39 43.80 0.00 65.98 14.24 0.53 0.00

95.56 55.07 70.73 96.04 59.54 84.66 94.16 84.52

42.64 15.87 80.03

ATTACK
BadNets-A2O [9] BadNets-A2A [9]
Blended [6] Input-Aware [27]
LF [43] SSBA [19] Trojan [22] WaNet [28]
Avg

i-BAU [42] ACC ASR DER

51.63 53.52 50.76 55.49 53.65 52.39 51.85 53.04

95.92 12.89 95.58 0.46 94.27 84.64 99.15 69.82

49.74 56.22 48.61 98.28 51.34 54.74 48.10 60.96

52.79 69.09 58.50

EP [45] ACC ASR DER

54.00 54.79 56.32 57.33 54.86 55.56 54.47 57.06

0.02 1.28 88.88 0.03 93.20 66.67 0.12 0.20

98.88 62.67 54.34 99.41 52.48 65.31 98.92 97.78

55.55 31.30 78.77

NPD-TU (Ours) ACC ASR DER
47.23 0.01 95.50 46.81 1.96 58.34 46.24 0.00 94.14 49.54 0.27 95.40 46.04 0.00 94.67 46.56 0.00 94.14 48.56 0.00 96.02 48.52 0.01 93.60
47.44 0.28 90.22

NPD-TP (Ours) ACC ASR DER
49.89 1.28 96.20 49.79 3.31 59.15 49.72 0.18 95.79 53.88 0.04 97.68 49.20 0.30 96.10 49.04 0.00 95.38 49.61 0.05 96.52 51.88 0.82 94.88
50.38 0.75 91.46

NPD (Ours) ACC ASR DER
49.79 2.51 95.53 49.94 5.57 58.10 49.62 0.12 95.77 53.75 5.93 94.67 49.94 2.48 95.38 49.25 0.01 95.48 49.43 0.51 96.21 52.64 0.24 95.54
50.55 2.17 90.84

transformation-based attack without visible triggers. Fully perturbing the network proves more effective than solely unlearning targeted triggers in WaNet.
• NPD-TU is effective for trigger-additive attacks while NPD-TP is expert in defending against sample-specific attacks. It can be observed by comparing defense results on different attacks like Blended and SSBA on CIFAR-10. This demonstrates that the applicability of different strategies varies across different attack scenarios.
• Defense performance is robust across all attacks on Tiny ImageNet. Similar to the results on CIFAR-10, our method outperforms other methods in terms of ASR and DER for all backdoor attacks. Despite a slight decrease in ACC, NPD-TU achieving a remarkably good performance with ASR < 1.5% on average. NPD-TP and NPD perform best in removing backdoors while maintaining model utility.
In summary, our method outperforms other state-of-the-art approaches, showcasing the broad applicability of our proposed method across diverse datasets. Due to space limits, defending results on GTSRB dataset and VGG19-BN network can be found in Section C of supplementary materials.
7

Table 3: Defense performance under different Table 4: Defense results in comparison with NPD-

components of losses.

UU and NPD-UP.

ATTACK → Lbce1 Lbce2 Lasr
✓ ✓ ✓
✓✓ ✓ ✓✓

BadNets-A2O [9] ACC ASR
91.45 1.18 90.17 1.19 90.46 0.38 90.02 0.27 89.56 0.21 88.93 1.26

Blended [6] ACC ASR

92.47 91.51 91.68 91.31 91.09 91.18

99.63 2.01 18.28 98.32 1.73 0.41

LF [43] ACC ASR

92.00 90.91 91.19 91.07 90.47 90.06

95.90 9.60 1.06 0.80 7.63 0.21

ATTACK ↓
BadNets-A2O [9] Blended [6] LF [43] SSBA [19] Trojan [22] WaNet [28]

No defense ACC ASR

91.82 93.44 93.01 92.88 93.47 92.80

93.79 97.71 99.06 97.07 99.99 98.90

NPD-UU ACC ASR

79.35 86.35 82.95 84.31 89.81 84.70

0.10 10.77 75.42 52.36 38.11 5.98

NPD-UP ACC ASR

90.61 92.35 91.53 91.49 92.61 92.11

1.74 3.86 17.24 14.22 11.43 1.41

NPD (Ours) ACC ASR
88.93 1.26 91.18 0.41 90.06 0.21 90.88 2.77 92.37 6.51 91.57 0.80

Table 5: Defense results under different poisoning ratio on CIFAR-10 and PreAct-ResNet18(%).

Poisoning Ratio → ATTACK ↓

5%

10%

20%

30%

40%

No Defense Ours No Defense Ours No Defense Ours No Defense Ours No Defense Ours

BadNets-A2O [9]

ACC ASR

92.35 89.52

87.99 0.89

91.82 93.79

88.93 1.26

90.17 96.12

85.77 0.39

88.32 97.33

86.76 0.69

86.16 97.78

82.31 3.88

Blended [6]

ACC ASR

93.76 99.31

91.48 11.06

93.44 97.71

91.18 0.41

93.00 99.92

91.27 4.41

92.78 99.98

90.10 3.78

91.64 99.96

88.85 24.92

Input-Aware [27]

ACC ASR

90.92 94.19

90.31 0.30

94.03 98.35

89.57 0.11

89.18 97.66

88.60 3.51

89.63 97.62

89.41 3.43

90.12 98.57

88.57 0.98

WaNet [28]

ACC ASR

93.38 97.27

91.38 0.14

92.80 98.90

91.57 0.80

91.02 94.93

89.77 0.22

92.35 99.06

89.53 1.71

92.21 99.49

89.53 2.19

4.3 Analysis
Effectiveness of each loss term. We conduct an ablation study to evaluate the contribution of each component of the loss function towards the overall performance on CIFAR-10 dataset. We separately investigate the first and second terms of loss Lbce (see Eq. (3)), denoting them as Lbce1 and Lbce2, respectively. Throughout the study, we keep the loss Lbn consistent across all experiments and the result is shown in Table 3. Notably, the loss lbce1 plays a significant role in improving the overall performance, while removing each component leads to a significant drop in defense in certain cases. This ablation study underscores the importance of each loss component in effectively mitigating different types of attacks.

Effectiveness of the targeted adversarial perturbations in NPD. To show the efficacy of the targeted adversarial perturbations in NPD (see Eq. (5)), we compare NPD with its two variants using two types of untargeted perturbation. We refer to adversarial perturbations generated by UAP and standard PGD without a targeted label as NPD-UU (untargeted universal adversarial perturbation) and NPD-UP (untargeted PGD), respectively. As shown in Table 4, NPD-UU and NPD-UP fail to remove backdoors in certain cases although NPD-UP obtains a higher ACC. This result shows the superiority of NPD in removing backdoors.

Performance of choosing different layers to in-

sert the transformation layer. We evaluate the

influence of choosing different layers to insert the transformation layer by inserting it after each con- 80

volution layer of PreAct-ResNet18 network on 60 CIFAR-10 dataset. Figure 3 shows the defense performance under three attacks. The result shows that 40 inserting the transformation layer into the shallower

Percentage

BadNets Blended SSBA

ACC ASR

layers results in a decrease in accuracy. This is 20

because even a slight perturbation in the shallow

layers can cause significant instability in the final output. However, as the layer goes deeper, the features become more separable, resulting in better

0
0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15
Layers

defense performance.

Figure 3: Defense performance of inserting

linear transformation into different layers.

Defense effectiveness under different poisoning

ratios. To investigate the impact of poisoning ratios on defense performance, we conducted exper-

iments on NPD with different poisoning ratios on the CIFAR-10 dataset. As presented in Table 5,

8

there is a slight decrease in ACC as the poisoning ratio increases. Moreover, our approach exhibits a notably stable defense performance across a range of poisoning ratios.

Table 6: Results with different number of clean data on CIFAR-10 (%). The best DERs are highlighted in Boldface.

ATTACK BadNets-A2O [9]
SSBA [19] LF [43]

# Clean data → Defense ↓
i-BAU [42] ANP [38]
Ours
i-BAU [42] ANP [38]
Ours
i-BAU [42] ANP [38]
Ours

ACC
65.41 91.53 87.58
84.21 92.06 90.48
83.91 92.74 90.21

500 ASR
7.74 5.50 1.38
17.73 28.67 0.43
19.34 46.70 0.99

DER
79.82 94.00 94.09
85.33 83.79 97.12
85.31 76.04 97.63

ACC
66.05 90.81 87.80
76.84 92.13 90.80
70.12 92.28 89.28

250 ASR
2.02 2.03 0.27
51.90 27.61 10.03
99.42 18.02 0.50

DER
81.03 93.40 92.78
60.38 80.17 88.29
35.53 84.11 91.37

ACC
64.60 85.57 86.91
67.48 88.31 89.48
70.26 89.99 88.18

50 ASR
22.10 1.52 0.20
98.37 22.28 8.97
99.31 18.77 10.83

DER
70.27 91.04 92.37
35.21 80.92 88.16
35.60 82.59 85.65

Defense effectiveness under different clean ratios. We investigate the sensitivity of clean data on defense performance and compare our NPD with SOTA defenses. As shown in Table 6, NPD is less sensitive to the size of clean data among all the attacks and defenses. Even with only 50 samples, it still maintains acceptable performance. This result shows that our method exhibits minimal reliance on the number of training samples.

Table 7: Running time of different defense methods with 2500 CIFAR-10 images on PreActResNet18.

Defense

FP [21] NAD [18] NC [33] ANP [38] i-BAU [42] EP [45] NPD (Ours)

Runnign Time (sec.) 1169.01 74.39 896.45 58.75

57.23 131.84 55.16

Running time comparison. We measure the runtime of the defense methods on 2500 CIFAR-10 images with batch size 256 and PreAct-ResNet18. The experiments were conducted on a RTX 4090Ti GPU and the results are presented in Table 7. Among these methods, our proposed PND-UN achieves the fastest performance, requiring only 56 seconds. It should be noted that our method was trained for 50 epochs, while i-BAU was only trained for 5 epochs.

5 Conclusion
Inspired by the mechanism of optical polarizer, this work proposed a novel backdoor defense method by inserting a learnable neural polarizer as an intermediate layer of the backdoored model. We instantiated the neural polarizer as a lightweight linear transformation and it could be efficiently and effectively learned with limited clean samples to mitigate backdoor effect. To learn a desired neural polarizer, a bi-level optimization problem is proposed by filtering trigger features of poisoned samples while maintaining benign features of both poisoned and benign samples. Extensive experiments demonstrate the effectiveness of our method across all evaluated backdoor attacks and all other defense methods under various datasets and network architectures.
Limitations and future work. Although only limited clean data is needed for our method to achieve a remarkable defense performance, the accessibility of clean data is still an important limitation of the proposed method, which may restrict the application of our method. Therefore, a promising direction for future work is to further reduce the requirement of clean data by exploring data-free neural polarizer or learning neural polarizer based on poisoned training data.
Broader impacts. Backdoor attacks pose significant threats to the deployment of deep neural networks obtained from untrustworthy sources. This work has made a valuable contribution to the community with an efficient and effective backdoor defense strategy to ease the threat of existing backdoor attacks, even with a very limited set of clean samples, which ensures its practicality. Besides, the innovative defense strategy of learning additional lightweight layers, rather than adjusting the whole backdoored model, may inspire more researchers to develop more efficient and practical defense methods.

9

References
[1] Maksym Andriushchenko and Nicolas Flammarion. Understanding and improving fast adversarial training. Advances in Neural Information Processing Systems, 33:16048–16059, 2020. 3
[2] Tao Bai, Jinqi Luo, Jun Zhao, Bihan Wen, and Qian Wang. Recent advances in adversarial training for adversarial robustness. arXiv preprint arXiv:2102.01356, 2021. 3
[3] Mauro Barni, Kassem Kallas, and Benedetta Tondi. A new backdoor attack in cnns by training set corruption without label poisoning. In 2019 IEEE International Conference on Image Processing (ICIP), pages 101–105. IEEE, 2019. 3
[4] Léon Bottou and Olivier Bousquet. The tradeoffs of large scale learning. In John C. Platt, Daphne Koller, Yoram Singer, and Sam T. Roweis, editors, Advances in Neural Information Processing Systems 20, Proceedings of the Twenty-First Annual Conference on Neural Information Processing Systems, Vancouver, British Columbia, Canada, December 3-6, 2007, pages 161–168. Curran Associates, Inc., 2007. 5
[5] Weixin Chen, Baoyuan Wu, and Haoqian Wang. Effective backdoor defense by exploiting sensitivity of poisoned samples. Advances in Neural Information Processing Systems, 35:9727–9737, 2022. 1, 3
[6] Xinyun Chen, Chang Liu, Bo Li, Kimberly Lu, and Dawn Song. Targeted backdoor attacks on deep learning systems using data poisoning. arXiv e-prints, pages arXiv–1712, 2017. 1, 3, 5, 7, 8
[7] Ziyi Cheng, Baoyuan Wu, Zhenya Zhang, and Jianjun Zhao. Tat: Targeted backdoor attacks against visual object tracking. Pattern Recognition, 142:109629, 2023. 1
[8] Kuofeng Gao, Jiawang Bai, Baoyuan Wu, Mengxi Ya, and Shu-Tao Xia. Imperceptible and robust backdoor attack in 3d point cloud. arXiv preprint arXiv:2208.08052, 2022. 1
[9] Tianyu Gu, Kang Liu, Brendan Dolan-Gavitt, and Siddharth Garg. Badnets: Evaluating backdooring attacks on deep neural networks. IEEE Access, 7:47230–47244, 2019. 1, 3, 5, 7, 8, 9
[10] Junfeng Guo, Ang Li, and Cong Liu. AEVA: black-box backdoor detection using adversarial extreme value analysis. In The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022. OpenReview.net, 2022. 3, 5
[11] Puneet Gupta and Esa Rahtu. Ciidefence: Defeating adversarial attacks by fusing class-specific image inpainting and image denoising. In 2019 IEEE/CVF International Conference on Computer Vision, ICCV 2019, Seoul, Korea (South), October 27 - November 2, 2019, pages 6707–6716. IEEE, 2019. 3
[12] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Identity mappings in deep residual networks. In Computer Vision–ECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 11–14, 2016, Proceedings, Part IV 14, pages 630–645. Springer, 2016. 5
[13] Kunzhe Huang, Yiming Li, Baoyuan Wu, Zhan Qin, and Kui Ren. Backdoor defense via decoupling the training process. In The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022. OpenReview.net, 2022. 1, 3
[14] Andrew Ilyas, Logan Engstrom, Anish Athalye, and Jessy Lin. Black-box adversarial attacks with limited queries and information. In International conference on machine learning, pages 2137–2146. PMLR, 2018. 1
[15] Alex Krizhevsky et al. Learning multiple layers of features from tiny images. 2009. 5
[16] Alexey Kurakin, Ian J Goodfellow, and Samy Bengio. Adversarial examples in the physical world. In Artificial intelligence safety and security, pages 99–112. Chapman and Hall/CRC, 2018. 1
[17] Ya Le and Xuan Yang. Tiny imagenet visual recognition challenge. CS 231N, 7(7):3, 2015. 5
[18] Yige Li, Xixiang Lyu, Nodens Koren, Lingjuan Lyu, Bo Li, and Xingjun Ma. Neural attention distillation: Erasing backdoor triggers from deep neural networks. In 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net, 2021. 3, 6, 7, 9
[19] Yuezun Li, Yiming Li, Baoyuan Wu, Longkang Li, Ran He, and Siwei Lyu. Invisible backdoor attack with sample-specific triggers. In 2021 IEEE/CVF International Conference on Computer Vision, ICCV 2021, Montreal, QC, Canada, October 10-17, 2021, pages 16443–16452. IEEE, 2021. 1, 3, 5, 7, 8, 9
10

[20] Fangzhou Liao, Ming Liang, Yinpeng Dong, Tianyu Pang, Xiaolin Hu, and Jun Zhu. Defense against adversarial attacks using high-level representation guided denoiser. In 2018 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2018, Salt Lake City, UT, USA, June 18-22, 2018, pages 1778–1787. IEEE Computer Society, 2018. 3
[21] Kang Liu, Brendan Dolan-Gavitt, and Siddharth Garg. Fine-pruning: Defending against backdooring attacks on deep neural networks. In Research in Attacks, Intrusions, and Defenses: 21st International Symposium, RAID 2018, Heraklion, Crete, Greece, September 10-12, 2018, Proceedings 21, pages 273–294. Springer, 2018. 2, 3, 6, 7, 9
[22] Yingqi Liu, Shiqing Ma, Yousra Aafer, Wen-Chuan Lee, Juan Zhai, Weihang Wang, and Xiangyu Zhang. Trojaning attack on neural networks. In 25th Annual Network and Distributed System Security Symposium, NDSS 2018, San Diego, California, USA, February 18-221, 2018. The Internet Society, 2018. 1, 5, 7, 8
[23] Zhiyun Lu, Wei Han, Yu Zhang, and Liangliang Cao. Exploring targeted universal adversarial perturbations to end-to-end ASR models. In Hynek Hermansky, Honza Cernocký, Lukás Burget, Lori Lamel, Odette Scharenborg, and Petr Motlícek, editors, Interspeech 2021, 22nd Annual Conference of the International Speech Communication Association, Brno, Czechia, 30 August - 3 September 2021, pages 3460–3464. ISCA, 2021. 5
[24] Wanlun Ma, Derui Wang, Ruoxi Sun, Minhui Xue, Sheng Wen, and Yang Xiang. The" beatrix”resurrections: Robust backdoor detection via gram matrices. ArXiv preprint, abs/2209.11715, 2022. 3, 5
[25] Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu. Towards deep learning models resistant to adversarial attacks. In 6th International Conference on Learning Representations, ICLR 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings. OpenReview.net, 2018. 3, 5, 6
[26] Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi, and Pascal Frossard. Deepfool: a simple and accurate method to fool deep neural networks. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 2574–2582, 2016. 1
[27] Tuan Anh Nguyen and Anh Tran. Input-aware dynamic backdoor attack. In Hugo Larochelle, Marc’Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin, editors, Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, 2020. 5, 7, 8
[28] Tuan Anh Nguyen and Anh Tuan Tran. Wanet - imperceptible warping-based backdoor attack. In 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net, 2021. 1, 3, 5, 7, 8
[29] Ali Shafahi, W. Ronny Huang, Mahyar Najibi, Octavian Suciu, Christoph Studer, Tudor Dumitras, and Tom Goldstein. Poison frogs! targeted clean-label poisoning attacks on neural networks. In Samy Bengio, Hanna M. Wallach, Hugo Larochelle, Kristen Grauman, Nicolò Cesa-Bianchi, and Roman Garnett, editors, Advances in Neural Information Processing Systems 31: Annual Conference on Neural Information Processing Systems 2018, NeurIPS 2018, December 3-8, 2018, Montréal, Canada, pages 6106–6116, 2018. 3
[30] Rui Shao, Pramuditha Perera, Pong C Yuen, and Vishal M Patel. Open-set adversarial defense with clean-adversarial mutual learning. International Journal of Computer Vision, 130(4):1070–1087, 2022. 3
[31] Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition. In Yoshua Bengio and Yann LeCun, editors, 3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings, 2015. 5
[32] Johannes Stallkamp, Marc Schlipsing, Jan Salmen, and Christian Igel. The german traffic sign recognition benchmark: a multi-class classification competition. In The 2011 international joint conference on neural networks, pages 1453–1460. IEEE, 2011. 5
[33] Bolun Wang, Yuanshun Yao, Shawn Shan, Huiying Li, Bimal Viswanath, Haitao Zheng, and Ben Y Zhao. Neural cleanse: Identifying and mitigating backdoor attacks in neural networks. In 2019 IEEE Symposium on Security and Privacy (SP), pages 707–723. IEEE, 2019. 3, 6, 7, 9
[34] Ruotong Wang, Hongrui Chen, Zihao Zhu, Li Liu, Yong Zhang, Yanbo Fan, and Baoyuan Wu. Robust backdoor attack with visible, semantic, sample-specific, and compatible triggers. arXiv preprint arXiv:2306.00816, 2023. 1
11

[35] Yisen Wang, Difan Zou, Jinfeng Yi, James Bailey, Xingjun Ma, and Quanquan Gu. Improving adversarial robustness requires revisiting misclassified examples. In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net, 2020. 3, 5
[36] Wikipedia contributors. Polarizer — Wikipedia, the free encyclopedia, 2023. https://en. wikipedia.org/wiki/Polarizer; accessed 10-May-2023. 2
[37] Baoyuan Wu, Hongrui Chen, Mingda Zhang, Zihao Zhu, Shaokui Wei, Danni Yuan, and Chao Shen. Backdoorbench: A comprehensive benchmark of backdoor learning. In Thirty-sixth Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2022. 5, 6
[38] Dongxian Wu and Yisen Wang. Adversarial neuron pruning purifies backdoored deep models. In Marc’Aurelio Ranzato, Alina Beygelzimer, Yann N. Dauphin, Percy Liang, and Jennifer Wortman Vaughan, editors, Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems 2021, NeurIPS 2021, December 6-14, 2021, virtual, pages 16913–16925, 2021. 2, 3, 6, 7, 9
[39] Dongxian Wu, Shu-Tao Xia, and Yisen Wang. Adversarial weight perturbation helps robust generalization. In Hugo Larochelle, Marc’Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin, editors, Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, 2020. 3
[40] Cihang Xie, Yuxin Wu, Laurens van der Maaten, Alan L. Yuille, and Kaiming He. Feature denoising for improving adversarial robustness. In IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2019, Long Beach, CA, USA, June 16-20, 2019, pages 501–509. Computer Vision Foundation / IEEE, 2019. 3
[41] Hao Xiong, Cui Kong, Xiaoxue Yang, and Ying Wu. Optical polarizer based on the mechanical effect of light. Optics Letters, 41(18):4316–4319, 2016. 2
[42] Yi Zeng, Si Chen, Won Park, Zhuoqing Mao, Ming Jin, and Ruoxi Jia. Adversarial unlearning of backdoors via implicit hypergradient. In The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022. OpenReview.net, 2022. 2, 3, 6, 7, 9
[43] Yi Zeng, Won Park, Z. Morley Mao, and Ruoxi Jia. Rethinking the backdoor attacks’ triggers: A frequency perspective. In 2021 IEEE/CVF International Conference on Computer Vision, ICCV 2021, Montreal, QC, Canada, October 10-17, 2021, pages 16453–16461. IEEE, 2021. 3, 5, 7, 8, 9
[44] Runkai Zheng, Rongjun Tang, Jianze Li, and Li Liu. Data-free backdoor removal based on channel lipschitzness. In Computer Vision–ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23–27, 2022, Proceedings, Part V, pages 175–191. Springer, 2022. 3
[45] Runkai Zheng, Rongjun Tang, Jianze Li, and Li Liu. Pre-activation distributions expose backdoor neurons. Advances in Neural Information Processing Systems, 35:18667–18680, 2022. 3, 6, 7, 9
[46] Mingli Zhu, Shaokui Wei, Li Shen, Yanbo Fan, and Baoyuan Wu. Enhancing fine-tuning based backdoor defense with sharpness-aware minimization. ArXiv preprint, abs/2304.11823, 2023. 6
12

